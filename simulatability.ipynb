{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some tests to decide on an experimental design for the simulatability experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import scipy.stats as stats \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hase et al. 2020.\n",
    "\n",
    "This is a within-subject* design with 4 phases: (1) Predictions only, (2) Pre-learn test, (3) Teaching: Predictions + Explanations, (4) Eval.\n",
    "\n",
    "Phase 1 and 3 share a set of documents as do 2 and 4.\n",
    "\n",
    "Result: Report **average change** in user accuracy per explanation method (phase 2 vs. 4), CI and p values of mean\n",
    "\n",
    "Additional parameters in Hase et al.:\n",
    "- Balance data \"by model correctness\" so random guessing can't succeed: *\"we ensure that true positives, false positives,\n",
    "true negatives, and false negatives are equally represented in the inputs. [...] We confirm user understanding of the data\n",
    "balancing in our screening test\"*\n",
    "- Forced choice, to not \"favor overly niche explanations\" (like in Ribeiro et al.)\n",
    "- Separate teach and test phases\n",
    "- Pre prediction phase to obtain a baseline\n",
    "- **All users see the same examples**\n",
    "\n",
    "\n",
    "*: One explanation method per user, some users repeat the experiment with a new dataset!?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_experiment = [\"user_id\", \"document_id\", \"user_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess(detector_label,p):\n",
    "    return detector_label if bool(np.random.choice([0,1],p=[1-p, p])) else not detector_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_study(df_user_study, df_phase_2, df_phase_4):\n",
    "    # \"Reproducing\" information from Table 1/2 in Hase et al.:\n",
    "    user_metrics_pre = df_phase_2.groupby([\"user_id\"]).apply(lambda x : user_metrics(x,df_user_study))\n",
    "\n",
    "    user_metrics_eval = df_phase_4.groupby([\"user_id\"]).apply(lambda x : user_metrics(x,df_user_study)) # TODO\n",
    "    df_change = user_metrics_eval - user_metrics_pre\n",
    "    df_change = df_change.rename(columns={\"User Accuracy\": \"Change in User Accuracy\"})\n",
    "\n",
    "    user_acc_col = df_change[\"Change in User Accuracy\"] # for convenience\n",
    "\n",
    "    # use student t for low number of samples\n",
    "    lower, upper = stats.t.interval(\n",
    "    confidence=0.95, \n",
    "    df=len(user_acc_col)-1, # degrees of freedom = # samples - 1 for mean\n",
    "              loc=user_acc_col.mean(), \n",
    "              scale=stats.sem(user_acc_col)\n",
    "              ) \n",
    "    \n",
    "    p_val = ttest_ind(user_metrics_eval[\"User Accuracy\"],user_metrics_pre[\"User Accuracy\"]).pvalue\n",
    "\n",
    "    k_alpha = krippendorff.alpha(reliability_data=df_phase_4.groupby([\"user_id\"]).apply(lambda df : user_responses.astype(int).to_list()).to_list())\n",
    "\n",
    "\n",
    "\n",
    "    lower_b, upper_b = stats.bootstrap((user_acc_col,), np.mean, confidence_level=0.95,).confidence_interval\n",
    "    \n",
    "    # print results\n",
    "  #  print(\"Mean change in acc\",user_acc_col.mean())\n",
    "   # print(\"CI for mean change: [{},{}]\".format(lower,upper))\n",
    "\n",
    "##    print(\"CI by bootstrap: [{},{}]\".format(lower_b, upper_b))\n",
    "\n",
    "    \n",
    "  #  print(\"p=%.10f\" % p_val, \"significant (< 0.05)\" if p_val < 0.05 else \"NOT significant (> 0.05)\")\n",
    " #   print(\"Krippendorff between users: {}\".format(k_alpha))\n",
    "    return p_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# Create your connection.\n",
    "connection = sqlite3.connect(\"../survey/db.db\")\n",
    "\n",
    "user_df = pd.read_sql_query(\"SELECT * FROM users\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_learn = 16\n",
    "n_eval = 16\n",
    "n_users = 10\n",
    "\n",
    "mu_got_it_right_pre=0.5\n",
    "sigma_got_it_right_pre=0.05\n",
    "mu_gain = 0.2\n",
    "sigma_gain = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:3002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentNr = 1\n",
    "label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_study = pd.read_pickle(\"./dataset_user_study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "user_dist_without = lambda : np.clip(np.random.normal(mu_got_it_right_pre, sigma_got_it_right_pre, 1)[0], 0,1)\n",
    "user_dist_gain = lambda : np.clip(np.random.normal(mu_gain, sigma_gain, 1)[0], -1,1)\n",
    "for idx, user in user_df.iterrows():\n",
    "    res = requests.get(url+\"/auth/\"+ user[\"access_token\"])\n",
    "    auth_token = json.loads(res.text)\n",
    "    headers = {'Content-Type': 'application/json','Authorization': \"Bearer \"+auth_token, \"Content-Type\": \"application/json\",}\n",
    "\n",
    "    # go to phase 2\n",
    "    requests.get(url+\"/completeCurrentPhase\", headers=headers)\n",
    "    requests.get(url+\"/completeCurrentPhase\", headers=headers)\n",
    "    requests.get(url+\"/completeCurrentPhase\", headers=headers)\n",
    "    for doc_nr, row in df_user_study.iterrows():\n",
    "        p_without = user_dist_without()\n",
    "        requests.post(url+\"/submitPhase2\", json={\"ID\": doc_nr, \"label\": guess(row[\"f(b)\"], p_without)}, headers=headers)\n",
    "    requests.get(url+\"/completeCurrentPhase\", headers=headers)\n",
    "    requests.get(url+\"/completeCurrentPhase\", headers=headers)\n",
    "    for doc_nr, row in df_user_study.iterrows():\n",
    "        p_with = np.clip(p_without + user_dist_gain(), 0,1)\n",
    "        requests.post(url+\"/submitPhase4\", json={\"ID\": doc_nr, \"label\": guess(row[\"f(b)\"], p_with)}, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_metrics(df_user_responses, detector_predictions):\n",
    "    detector_predictions = detector_predictions.astype(bool)\n",
    "    user_responses = df_user_responses.loc[df_user_responses.groupby(\"document_nr\")[\"timestamp\"].idxmax()].set_index(\"document_nr\")[\"label\"].astype(bool) # only keep most recent response\n",
    "    # display(user_responses)\n",
    "    # display(detector_predictions)\n",
    "    TP = ((detector_predictions) & (user_responses)).sum()\n",
    "    FP = ((~detector_predictions) & (user_responses)).sum()\n",
    "\n",
    "    TN = ((~detector_predictions) & (~user_responses)).sum()\n",
    "    FN = ((detector_predictions) & (~user_responses)).sum()\n",
    "\n",
    "    acc = (TP+TN) / (TP+FP+TN+FN)\n",
    "    # print(\"acc\", acc)\n",
    "    # print(\"TP\", TP)\n",
    "    # print(\"FP\", FP)\n",
    "    # print(\"TN\", TN)\n",
    "    # print(\"FN\", FN)\n",
    "\n",
    "\n",
    "    assert sum([TP,FP,TN,FN]) == len(detector_predictions), \"Check that input is bool\"\n",
    "    assert (acc ==(user_responses == detector_predictions).sum() / len(detector_predictions)), \"Check that input is bool: acc\"\n",
    "\n",
    "    return pd.DataFrame([(TP,TN,FP,FN, acc)], columns=[\"TP\",\"TN\",\"FP\",\"FN\", \"User Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = user_df.set_index(\"ID\").rename_axis(\"user_id\")[[\"explainer\", \"detector\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase_2 = pd.read_sql_query(\"SELECT * FROM responses_phase_2\", connection)\n",
    "df_phase_4 = pd.read_sql_query(\"SELECT * FROM responses_phase_4\", connection)\n",
    "\n",
    "metrics_phase_4 = df_phase_4.groupby([\"user_id\"]).apply(lambda df_user_responses : user_metrics(df_user_responses,df_user_study[\"f(b)\"].rename_axis(\"document_nr\")))\n",
    "metrics_phase_2 = df_phase_2.groupby([\"user_id\"]).apply(lambda df_user_responses : user_metrics(df_user_responses,df_user_study[\"f(b)\"].rename_axis(\"document_nr\")))\n",
    "\n",
    "\n",
    "difference = metrics_phase_4 - metrics_phase_2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detector\n",
       "DetectorDetectGPT    0.203704\n",
       "DetectorGuo          0.209877\n",
       "DetectorRadford      0.185185\n",
       "Name: User Accuracy, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.join(u).groupby([\"detector\"])[\"User Accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "explainer\n",
       "Anchor_Explainer    0.265432\n",
       "LIME_Explainer      0.179012\n",
       "SHAP_Explainer      0.154321\n",
       "Name: User Accuracy, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.join(u).groupby([\"explainer\"])[\"User Accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detector           explainer       \n",
       "DetectorDetectGPT  Anchor_Explainer    0.314815\n",
       "                   LIME_Explainer      0.222222\n",
       "                   SHAP_Explainer      0.074074\n",
       "DetectorGuo        Anchor_Explainer    0.259259\n",
       "                   LIME_Explainer      0.166667\n",
       "                   SHAP_Explainer      0.203704\n",
       "DetectorRadford    Anchor_Explainer    0.222222\n",
       "                   LIME_Explainer      0.148148\n",
       "                   SHAP_Explainer      0.185185\n",
       "Name: User Accuracy, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.join(u).groupby([\"detector\", \"explainer\"])[\"User Accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.001177719019692292\n",
      "9\n",
      "0.0005498607824233039\n",
      "9\n",
      "0.002406518577369974\n"
     ]
    }
   ],
   "source": [
    "for name, group_2 in metrics_phase_2.join(u).groupby([\"detector\"]):\n",
    "    detector = name[0]\n",
    "    group_4 =  metrics_phase_4.join(u)[metrics_phase_4.join(u)[\"detector\"] == detector]\n",
    "    print(len(group_2))\n",
    "    print(ttest_ind(group_2[\"User Accuracy\"],group_4[\"User Accuracy\"]).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.39455232925049e-05\n",
      "0.002275787115565749\n",
      "0.004757523483762692\n"
     ]
    }
   ],
   "source": [
    "for name, group_2 in metrics_phase_2.join(u).groupby([\"explainer\"]):\n",
    "    explainer = name[0]\n",
    "    group_4 =  metrics_phase_4.join(u)[metrics_phase_4.join(u)[\"explainer\"] == explainer]\n",
    "\n",
    "    print(ttest_ind(group_2[\"User Accuracy\"],group_4[\"User Accuracy\"]).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # \"Reproducing\" information from Table 1/2 in Hase et al.:\n",
    "\n",
    "\n",
    "# user_metrics_eval = df_phase_4.groupby([\"user_id\"]).apply(lambda x : user_metrics(x,df_user_study)) # TODO\n",
    "# df_change = user_metrics_eval - user_metrics_pre\n",
    "# df_change = df_change.rename(columns={\"User Accuracy\": \"Change in User Accuracy\"})\n",
    "\n",
    "# user_acc_col = df_change[\"Change in User Accuracy\"] # for convenience\n",
    "\n",
    "# # use student t for low number of samples\n",
    "# lower, upper = stats.t.interval(\n",
    "# confidence=0.95, \n",
    "# df=len(user_acc_col)-1, # degrees of freedom = # samples - 1 for mean\n",
    "#             loc=user_acc_col.mean(), \n",
    "#             scale=stats.sem(user_acc_col)\n",
    "#             ) \n",
    "\n",
    "# p_val = ttest_ind(user_metrics_eval[\"User Accuracy\"],user_metrics_pre[\"User Accuracy\"]).pvalue\n",
    "\n",
    "# k_alpha = krippendorff.alpha(reliability_data=df_phase_4.groupby([\"user_id\"]).apply(lambda df : user_responses.astype(int).to_list()).to_list())\n",
    "\n",
    "\n",
    "\n",
    "# lower_b, upper_b = stats.bootstrap((user_acc_col,), np.mean, confidence_level=0.95,).confidence_interval\n",
    "\n",
    "# # print results\n",
    "# #  print(\"Mean change in acc\",user_acc_col.mean())\n",
    "# # print(\"CI for mean change: [{},{}]\".format(lower,upper))\n",
    "\n",
    "# ##    print(\"CI by bootstrap: [{},{}]\".format(lower_b, upper_b))\n",
    "\n",
    "\n",
    "# #  print(\"p=%.10f\" % p_val, \"significant (< 0.05)\" if p_val < 0.05 else \"NOT significant (> 0.05)\")\n",
    "# #   print(\"Krippendorff between users: {}\".format(k_alpha))\n",
    "# return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_hase(\n",
    "\n",
    "#         n_learn = 16,\n",
    "#         n_eval = 16,\n",
    "#         n_users = 10,\n",
    "\n",
    "#         mu_got_it_right_pre=0.5,\n",
    "#         sigma_got_it_right_pre=0.05,\n",
    "\n",
    "#         mu_gain = 0.1,\n",
    "#         sigma_gain = 0.1,\n",
    "\n",
    "\n",
    "# ):\n",
    "#     users = []\n",
    "#     user_dist_without = lambda : np.clip(np.random.normal(mu_got_it_right_pre, sigma_got_it_right_pre, 1)[0], 0,1)\n",
    "#     user_dist_gain = lambda : np.clip(np.random.normal(mu_gain, sigma_gain, 1)[0], -1,1)\n",
    "#     for i in range(1, n_users+1):\n",
    "#         p_without = user_dist_without()\n",
    "#         p_with = np.clip(p_without + user_dist_gain(), 0,1)\n",
    "#         users.append((\"u_%s\" % i, p_without ,p_with))\n",
    "#     documents_learn_1_2 = [\"l_%s\" % i for i in range(1,n_learn+1)]\n",
    "#     documents_pre_eval = [\"e_%s\" % i for i in range(1,n_eval+1)]\n",
    "\n",
    "#     df_detector_output = mock_detector_responses(documents_pre_eval)\n",
    "\n",
    "#     responses_pre, responses_eval = mock_user_responses(df_detector_output, documents_pre_eval, users)\n",
    "#     df_pre =pd.DataFrame(responses_pre, columns=columns_experiment)\n",
    "#     df_eval =pd.DataFrame(responses_eval, columns=columns_experiment)\n",
    "\n",
    "#  #   print(\"# responses pre\", len(responses_pre))\n",
    "#   #  print(\"# responses pre per method\", len(responses_pre)/3)\n",
    "#    # print(\"Each user saw {} instances. \".format(2*n_learn + 2*n_eval) )\n",
    "#    # print(\"Used {} unique documents. A set of {} in phase 1 and 3; and a set of {} in phase 2 and 4.\".format(n_learn + n_eval,n_learn, n_eval))\n",
    "\n",
    "# #    print(\"Results based on {} unique eval documents.\".format(n_eval))\n",
    "\n",
    "# #    print(\"Results based on {} datapoints.\".format(len(responses_eval)))\n",
    "#     p_value = evaluate_user_study(df_detector_output, df_pre, df_eval)\n",
    "#     return p_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
