{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some tests to decide on an experimental design for the simulatability experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import scipy.stats as stats \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hase et al. 2020.\n",
    "\n",
    "This is a within-subject* design with 4 phases: (1) Predictions only, (2) Pre-learn test, (3) Teaching: Predictions + Explanations, (4) Eval.\n",
    "\n",
    "Phase 1 and 3 share a set of documents as do 2 and 4.\n",
    "\n",
    "Result: Report **average change** in user accuracy per explanation method (phase 2 vs. 4), CI and p values of mean\n",
    "\n",
    "Additional parameters in Hase et al.:\n",
    "- Balance data \"by model correctness\" so random guessing can't succeed: *\"we ensure that true positives, false positives,\n",
    "true negatives, and false negatives are equally represented in the inputs. [...] We confirm user understanding of the data\n",
    "balancing in our screening test\"*\n",
    "- Forced choice, to not \"favor overly niche explanations\" (like in Ribeiro et al.)\n",
    "- Separate teach and test phases\n",
    "- Pre prediction phase to obtain a baseline\n",
    "- **All users see the same examples**\n",
    "\n",
    "\n",
    "*: One explanation method per user, some users repeat the experiment with a new dataset!?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_experiment = [\"user_id\", \"document_id\", \"user_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_post(detector_label):\n",
    "    return detector_label if bool(np.random.choice([0,1],p=[0.45,0.55])) else ~detector_label\n",
    "def guess_pre(detector_label):\n",
    "    return detector_label if bool(np.random.choice([0,1],p=[0.5,0.5])) else ~detector_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_metrics(user_df, df_detector_output):\n",
    "    df = user_df.join(df_detector_output, on=\"document_id\")\n",
    "\n",
    "   # display(df)\n",
    "    TP = ((df[\"detector_label\"]) & (df[\"user_label\"])).sum()\n",
    "    FP = ((~df[\"detector_label\"]) & (df[\"user_label\"])).sum()\n",
    "\n",
    "    TN = ((~df[\"detector_label\"]) & (~df[\"user_label\"])).sum()\n",
    "    FN = ((df[\"detector_label\"]) & (~df[\"user_label\"])).sum()\n",
    "\n",
    "    acc = (TP+TN) / (TP+FP+TN+FN)\n",
    "    # print(\"acc\", acc)\n",
    "    # print(\"TP\", TP)\n",
    "    # print(\"FP\", FP)\n",
    "    # print(\"TN\", TN)\n",
    "    # print(\"FN\", FN)\n",
    "\n",
    "\n",
    "    assert sum([TP,FP,TN,FN]) == len(df), \"Check that input is bool\"\n",
    "    assert (acc ==(df[\"user_label\"] == df[\"detector_label\"]).sum() / len(df)), \"Check that input is bool: acc\"\n",
    "\n",
    "    return pd.DataFrame([(TP,TN,FP,FN, acc)], columns=[\"TP\",\"TN\",\"FP\",\"FN\", \"User Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_study(df_detector_output, df_pre, df_eval):\n",
    "    # \"Reproducing\" information from Table 1/2 in Hase et al.:\n",
    "    user_metrics_pre = df_pre.groupby([\"user_id\"]).apply(lambda x : user_metrics(x,df_detector_output))\n",
    "\n",
    "    user_metrics_eval = df_eval.groupby([\"user_id\"]).apply(lambda x : user_metrics(x,df_detector_output)) # TODO\n",
    "    df_change = user_metrics_eval - user_metrics_pre\n",
    "    df_change = df_change.rename(columns={\"User Accuracy\": \"Change in User Accuracy\"})\n",
    "\n",
    "    user_acc_col = df_change[\"Change in User Accuracy\"] # for convenience\n",
    "\n",
    "    # use student t for low number of samples\n",
    "    lower, upper = stats.t.interval(\n",
    "    confidence=0.95, \n",
    "    df=len(user_acc_col)-1, # degrees of freedom = # samples - 1 for mean\n",
    "              loc=user_acc_col.mean(), \n",
    "              scale=stats.sem(user_acc_col)\n",
    "              ) \n",
    "    \n",
    "    p_val = ttest_ind(user_metrics_eval[\"User Accuracy\"],user_metrics_pre[\"User Accuracy\"]).pvalue\n",
    "\n",
    "#    k_alpha = krippendorff.alpha(reliability_data=df_eval.groupby([\"user_id\"]).apply(lambda df : df[\"user_label\"].astype(int).to_list()).to_list())\n",
    "\n",
    "\n",
    "\n",
    " #   lower_b, upper_b = stats.bootstrap((user_acc_col,), np.mean, confidence_level=0.95,).confidence_interval\n",
    "    \n",
    "    # print results\n",
    "  #  print(\"Mean change in acc\",user_acc_col.mean())\n",
    "   # print(\"CI for mean change: [{},{}]\".format(lower,upper))\n",
    "\n",
    "    #print(\"CI by bootstrap: [{},{}]\".format(lower_b, upper_b))\n",
    "\n",
    "    \n",
    "    #print(\"p=%.10f\" % p_val, \"significant (< 0.05)\" if p_val < 0.05 else \"NOT significant (> 0.05)\")\n",
    "    #print(\"Krippendorff between users: {}\".format(k_alpha))\n",
    "    return p_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_user_responses(df_detector_output, documents_pre_eval, users):\n",
    "    responses_pre = []\n",
    "    responses_eval = []\n",
    "    # Phase 1: \"16 examples from the validation set with labels and model predictions but no explanations\"\n",
    "    # Phase 2: \"Then they must predict the model output for either 16 or 32 new inputs, with the number chosen based on user time constraints.\"\n",
    "    for user_id in users:\n",
    "        responses_pre += [(user_id, document_id, guess_pre(df_detector_output.loc[document_id][\"detector_label\"])) for document_id in documents_pre_eval]\n",
    "    # Phase 3: \" Next, they return to the same learning examples, now with explanations included.\"\n",
    "    # Phase 4: \" Finally, they predict model behavior again on the same instances from the first prediction round\"\n",
    "    for user_id in users:\n",
    "        responses_eval += [(user_id, document_id,guess_post(df_detector_output.loc[document_id][\"detector_label\"])) for document_id in documents_pre_eval]\n",
    "    return responses_pre, responses_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mock_detector_responses(documents_pre_eval):\n",
    "    df_detector_output = pd.DataFrame([(document_id, bool(random.randint(0,1)), bool(random.randint(0,1))) for document_id in documents_pre_eval], columns=[\"document_id\", \"detector_label\",\"gold_label\"])\n",
    "    df_detector_output= df_detector_output.set_index(\"document_id\")\n",
    "    return df_detector_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_hase(\n",
    "\n",
    "        n_learn = 16,\n",
    "        n_eval = 16,\n",
    "        n_users = 12,\n",
    "\n",
    "\n",
    "):\n",
    "    users = [\"u_%s\" % i for i in range(1,n_users+1)]\n",
    "    documents_learn_1_2 = [\"l_%s\" % i for i in range(1,n_learn+1)]\n",
    "    documents_pre_eval = [\"e_%s\" % i for i in range(1,n_eval+1)]\n",
    "\n",
    "    df_detector_output = mock_detector_responses(documents_pre_eval)\n",
    "\n",
    "    responses_pre, responses_eval = mock_user_responses(df_detector_output, documents_pre_eval, users)\n",
    "    df_pre =pd.DataFrame(responses_pre, columns=columns_experiment)\n",
    "    df_eval =pd.DataFrame(responses_eval, columns=columns_experiment)\n",
    "\n",
    "\n",
    "\n",
    "   # print(\"Each user saw {} instances. \".format(2*n_learn + 2*n_eval) )\n",
    "   # print(\"Used {} unique documents. A set of {} in phase 1 and 3; and a set of {} in phase 2 and 4.\".format(n_learn + n_eval,n_learn, n_eval))\n",
    "\n",
    "#    print(\"Results based on {} unique eval documents.\".format(n_eval))\n",
    "\n",
    "#    print(\"Results based on {} datapoints.\".format(len(responses_eval)))\n",
    "    p_value = evaluate_user_study(df_detector_output, df_pre, df_eval)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16897220348520578"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_hase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loris\\thesis\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "p_values = []\n",
    "n_users_ = list(range(2,50))\n",
    "for n_users in n_users_:\n",
    "    p_values.append(simulate_hase(\n",
    "        n_learn = 16,\n",
    "        n_eval = 16,\n",
    "        n_users = n_users,\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_users_, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
