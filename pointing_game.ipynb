{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "N_DEBUG = 163\n",
    "\n",
    "OUTPUT_DIR = \"./pointing_game_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorGuo, DetectorRadford]#,DetectorDetectGPT]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "explainer_classes = [Anchor_Explainer, LIME_Explainer, SHAP_Explainer ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pointing_game_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x202027a20d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test # always load the full dataset! (np.random.shuffle(tokenized_sentences)). slice the actual hybrid_documents if debugging!\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    305.000000\n",
       "mean       5.708197\n",
       "std        2.109681\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        7.000000\n",
       "max       16.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series((len(list(nlp(d).sents)) for d in documents)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the hybrid documents for the assert in the loop\n",
    "ref_assert_hybrid_documents, _, _ = pointing_game_util.hybrid(documents.to_list(), gold_labels.to_list(), word_tokenizer=LIME_Explainer(DetectorRadford()).tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    271.000000\n",
       "mean       6.409594\n",
       "std        1.163544\n",
       "min        3.000000\n",
       "25%        6.000000\n",
       "50%        6.000000\n",
       "75%        7.000000\n",
       "max       15.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series((len(list(nlp(d).sents)) for d in ref_assert_hybrid_documents)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DetectorGuo\n",
      "Initialized Anchor_Explainer\n",
      "Indexing hybrid documents for Anchor_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:04<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized LIME_Explainer\n",
      "Indexing hybrid documents for LIME_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanations: 100%|██████████| 163/163 [00:01<00:00, 89.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SHAP_Explainer\n",
      "Indexing hybrid documents for SHAP_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanations: 100%|██████████| 163/163 [00:01<00:00, 118.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DetectorRadford\n",
      "Initialized Anchor_Explainer\n",
      "Indexing hybrid documents for Anchor_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:06<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized LIME_Explainer\n",
      "Indexing hybrid documents for LIME_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanations: 100%|██████████| 163/163 [00:02<00:00, 58.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SHAP_Explainer\n",
      "Indexing hybrid documents for SHAP_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanations: 100%|██████████| 163/163 [00:01<00:00, 114.63it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    print(\"Initialized \" + detector.__class__.__name__)\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(\"Initialized \" + explainer.__class__.__name__)\n",
    "\n",
    "        print(\"Indexing hybrid documents for \" + explainer.__class__.__name__)\n",
    "        hybrid_documents, tokenized_hybrid_documents, GT = pointing_game_util.hybrid(documents.to_list(), gold_labels.to_list(), word_tokenizer=explainer.tokenize)\n",
    "        assert (all([a==b for a,b in zip(ref_assert_hybrid_documents,hybrid_documents)])), \"(full) Hybrid documents don't match\" # tokenized_hybrid_documents differ by design to make the calculation of the pointing game accuracy easier\n",
    "\n",
    "        if DEBUG:\n",
    "            hybrid_documents = hybrid_documents[0:N_DEBUG]\n",
    "            tokenized_hybrid_documents = tokenized_hybrid_documents[0:N_DEBUG]\n",
    "            GT = GT[0:N_DEBUG]\n",
    "        \n",
    "        # write csv (for debug purposes)\n",
    "        pd.DataFrame(zip(hybrid_documents, tokenized_hybrid_documents, GT), columns=[\"Hybrid Document\", \"Tokenized Hybrid Document\", \"GT\"]).to_csv(os.path.join(OUTPUT_DIR, detector.__class__.__name__+ \"-\"+explainer.__class__.__name__+\".csv\"),index=False)\n",
    "        print(\"Predicting hybrid documents\")\n",
    "        predictions_hybrid = detector.predict_label(hybrid_documents)\n",
    "\n",
    "        print(\"Obtaining explanations on hybrid documents and calculating pointing game accuracy\")\n",
    "\n",
    "        \n",
    "       # pointing_game_acc = pointing_game_util.get_pointing_game_acc(hybrid_documents, explainer, predictions_hybrid, GT)\n",
    "        pointing_game_scores = pointing_game_util.get_pointing_game_scores(hybrid_documents, explainer, predictions_hybrid, GT)\n",
    "      #  print(\"Pointing game accuracy for {} | {}: {}\".format(explainer.__class__.__name__, detector.__class__.__name__, pointing_game_acc))\n",
    "        results.extend([(explainer.__class__.__name__, detector.__class__.__name__, pointing_game_score) for pointing_game_score in pointing_game_scores])\n",
    "    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some formatting functions\n",
    "def get_p_asterisks(group):\n",
    "    val =  group.mean()\n",
    "   # print(group.name)\n",
    "    _, p = ttest_1samp(group, popmean=0.5)\n",
    "    if p <= 0.001:\n",
    "        return \"{:.2f}\\\\textsuperscript{{***}}\".format(val)\n",
    "    if p <= 0.01:\n",
    "        return \"{:.2f}\\\\textsuperscript{{**}}\".format(val)\n",
    "    if p <= 0.05:\n",
    "        return \"{:.2f}\\\\textsuperscript{{*}}\".format(val)\n",
    "    if p > 0.05:\n",
    "        return \"{:.2f}\\\\textsuperscript{{ns}}\".format(val)\n",
    "\n",
    "def highlight_max(col):\n",
    "    vals = col.str.extract(r\"(-*\\d*\\.\\d*)\").astype(float).values.flatten()\n",
    "    max_val = vals.max()\n",
    "    return [\"font-weight: bold;\" if c == max_val else \"\" for c in vals ]\n",
    "def df_to_latex(styled_df, caption=\"TODO\", label=\"TODO\"):\n",
    "    return styled_df.to_latex(environment=\"table\", convert_css=True, clines=\"all;data\", hrules=True, caption=caption, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_dff(dff, groupby):\n",
    "    dff[\"Explainer\"] = dff[\"Explainer\"].str.replace(\"_Explainer\", \"\")\n",
    "    p_results = dff.groupby(groupby).agg(\n",
    "    {\n",
    "          \"Pointing Game Scores\": get_p_asterisks,\n",
    "        }\n",
    "    )\n",
    "    p_results = p_results.style.apply(highlight_max, subset=p_results.columns)\n",
    "    return p_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th>Pointing Game Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>978 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Explainer         Detector  Pointing Game Scores\n",
       "0    Anchor_Explainer      DetectorGuo                   0.0\n",
       "1    Anchor_Explainer      DetectorGuo                   1.0\n",
       "2    Anchor_Explainer      DetectorGuo                   1.0\n",
       "3    Anchor_Explainer      DetectorGuo                   0.1\n",
       "4    Anchor_Explainer      DetectorGuo                   0.7\n",
       "..                ...              ...                   ...\n",
       "973    SHAP_Explainer  DetectorRadford                   0.0\n",
       "974    SHAP_Explainer  DetectorRadford                   0.0\n",
       "975    SHAP_Explainer  DetectorRadford                   0.0\n",
       "976    SHAP_Explainer  DetectorRadford                   1.0\n",
       "977    SHAP_Explainer  DetectorRadford                   1.0\n",
       "\n",
       "[978 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Pointing Game Scores\"])\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_43e87_row2_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_43e87\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_43e87_level0_col0\" class=\"col_heading level0 col0\" >Pointing Game Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Explainer</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_43e87_level0_row0\" class=\"row_heading level0 row0\" >Anchor</th>\n",
       "      <td id=\"T_43e87_row0_col0\" class=\"data row0 col0\" >0.58\\textsuperscript{***}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_43e87_level0_row1\" class=\"row_heading level0 row1\" >LIME</th>\n",
       "      <td id=\"T_43e87_row1_col0\" class=\"data row1 col0\" >0.50\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_43e87_level0_row2\" class=\"row_heading level0 row2\" >SHAP</th>\n",
       "      <td id=\"T_43e87_row2_col0\" class=\"data row2 col0\" >0.73\\textsuperscript{***}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20211933210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_results_aggregate_level = style_dff(dff, groupby=[\"Explainer\"])\n",
    "display(p_results_aggregate_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_531bf_row4_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_531bf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_531bf_level0_col0\" class=\"col_heading level0 col0\" >Pointing Game Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Explainer</th>\n",
       "      <th class=\"index_name level1\" >Detector</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"2\">Anchor</th>\n",
       "      <th id=\"T_531bf_level1_row0\" class=\"row_heading level1 row0\" >DetectorGuo</th>\n",
       "      <td id=\"T_531bf_row0_col0\" class=\"data row0 col0\" >0.68\\textsuperscript{***}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level1_row1\" class=\"row_heading level1 row1\" >DetectorRadford</th>\n",
       "      <td id=\"T_531bf_row1_col0\" class=\"data row1 col0\" >0.49\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level0_row2\" class=\"row_heading level0 row2\" rowspan=\"2\">LIME</th>\n",
       "      <th id=\"T_531bf_level1_row2\" class=\"row_heading level1 row2\" >DetectorGuo</th>\n",
       "      <td id=\"T_531bf_row2_col0\" class=\"data row2 col0\" >0.62\\textsuperscript{**}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level1_row3\" class=\"row_heading level1 row3\" >DetectorRadford</th>\n",
       "      <td id=\"T_531bf_row3_col0\" class=\"data row3 col0\" >0.39\\textsuperscript{**}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"2\">SHAP</th>\n",
       "      <th id=\"T_531bf_level1_row4\" class=\"row_heading level1 row4\" >DetectorGuo</th>\n",
       "      <td id=\"T_531bf_row4_col0\" class=\"data row4 col0\" >0.82\\textsuperscript{***}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_531bf_level1_row5\" class=\"row_heading level1 row5\" >DetectorRadford</th>\n",
       "      <td id=\"T_531bf_row5_col0\" class=\"data row5 col0\" >0.64\\textsuperscript{***}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20210808490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_results = style_dff(dff, groupby=[\"Explainer\", \"Detector\"])\n",
    "display(p_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Scores per detector and explainer}\n",
      "\\label{pointing-game-explainer-detector}\n",
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      " &  & Pointing Game Scores \\\\\n",
      "Explainer & Detector &  \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Anchor} & DetectorGuo & 0.68\\textsuperscript{***} \\\\\n",
      "\\cline{2-3}\n",
      " & DetectorRadford & 0.49\\textsuperscript{ns} \\\\\n",
      "\\cline{1-3} \\cline{2-3}\n",
      "\\multirow[c]{2}{*}{LIME} & DetectorGuo & 0.62\\textsuperscript{**} \\\\\n",
      "\\cline{2-3}\n",
      " & DetectorRadford & 0.39\\textsuperscript{**} \\\\\n",
      "\\cline{1-3} \\cline{2-3}\n",
      "\\multirow[c]{2}{*}{SHAP} & DetectorGuo & \\bfseries 0.82\\textsuperscript{***} \\\\\n",
      "\\cline{2-3}\n",
      " & DetectorRadford & 0.64\\textsuperscript{***} \\\\\n",
      "\\cline{1-3} \\cline{2-3}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Scores per explainer}\n",
      "\\label{pointing-game-explainer}\n",
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      " & Pointing Game Scores \\\\\n",
      "Explainer &  \\\\\n",
      "\\midrule\n",
      "Anchor & 0.58\\textsuperscript{***} \\\\\n",
      "\\cline{1-2}\n",
      "LIME & 0.50\\textsuperscript{ns} \\\\\n",
      "\\cline{1-2}\n",
      "SHAP & \\bfseries 0.73\\textsuperscript{***} \\\\\n",
      "\\cline{1-2}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_to_latex(p_results, label=\"pointing-game-explainer-detector\", caption=\"Scores per detector and explainer\"))\n",
    "print(df_to_latex(p_results_aggregate_level, label=\"pointing-game-explainer\", caption=\"Scores per explainer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for hybrid_document in hybrid_documents:\n",
    "\n",
    "#     explainer = LIME_Explainer(detector)\n",
    "#     explainer.get_explanation_cached(hybrid_document).show_in_notebook()\n",
    "\n",
    "#     explainer = SHAP_Explainer(detector)\n",
    "#     shap.text_plot(explainer.get_explanation_cached(hybrid_document))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
