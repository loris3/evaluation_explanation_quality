{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "N_DEBUG = 10\n",
    "\n",
    "OUTPUT_DIR = \"./pointing_game_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorGuo,DetectorRadford]#,DetectorDetectGPT]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "explainer_classes = [Anchor_Explainer, LIME_Explainer, SHAP_Explainer ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pointing_game_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x151a47e78d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test # always load the full dataset! (np.random.shuffle(tokenized_sentences)). slice the actual hybrid_documents if debugging!\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series((len(list(nlp(d).sents)) for d in documents)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the hybrid documents for the assert in the loop\n",
    "ref_assert_hybrid_documents, _, _ = pointing_game_util.hybrid(documents.to_list(), gold_labels.to_list(), word_tokenizer=LIME_Explainer(DetectorRadford()).tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series((len(list(nlp(d).sents)) for d in ref_assert_hybrid_documents)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DetectorGuo\n",
      "Initialized Anchor_Explainer\n",
      "Indexing hybrid documents for Anchor_Explainer\n",
      "Predicting hybrid documents\n",
      "Generating explanations on hybrid documents and calculating pointing game accuracy\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Anchor_Explainer' object has no attribute 'get_fi_scores_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m predictions_hybrid \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mpredict_label(hybrid_documents)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating explanations on hybrid documents and calculating pointing game accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m pointing_game_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpointing_game_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pointing_game_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhybrid_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_hybrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m pointing_game_scores \u001b[38;5;241m=\u001b[39m pointing_game_util\u001b[38;5;241m.\u001b[39mget_pointing_game_scores(hybrid_documents, explainer, predictions_hybrid, GT)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPointing game accuracy for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(explainer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, detector\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, pointing_game_acc))\n",
      "File \u001b[1;32mc:\\Users\\loris\\thesis\\pointing_game_util.py:33\u001b[0m, in \u001b[0;36mget_pointing_game_acc\u001b[1;34m(hybrid_documents, explainer, predictions_hybrid, GT)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pointing_game_acc\u001b[39m(hybrid_documents, explainer, predictions_hybrid, GT):\n\u001b[1;32m---> 33\u001b[0m     fi_scores \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fi_scores_batch\u001b[49m(hybrid_documents)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# for each <explanation on a hybrid document> get the index of the top word by FI FOR THE PREDICTED CLASS.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# in the notation of Poerner et al.:\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#                   ---------------------------\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#                   ---------rmax(X,phi)-------------------------------\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#                                   ---f(X)---           \u001b[39;00m\n\u001b[0;32m     40\u001b[0m     indices_top_word \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(explanation[prediction], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m explanation, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fi_scores, predictions_hybrid)] \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Anchor_Explainer' object has no attribute 'get_fi_scores_batch'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    print(\"Initialized \" + detector.__class__.__name__)\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(\"Initialized \" + explainer.__class__.__name__)\n",
    "\n",
    "        print(\"Indexing hybrid documents for \" + explainer.__class__.__name__)\n",
    "        hybrid_documents, tokenized_hybrid_documents, GT = pointing_game_util.hybrid(documents.to_list(), gold_labels.to_list(), word_tokenizer=explainer.tokenize)\n",
    "        assert (all([a==b for a,b in zip(ref_assert_hybrid_documents,hybrid_documents)])), \"(full) Hybrid documents don't match\" # tokenized_hybrid_documents differ by design to make the calculation of the pointing game accuracy easier\n",
    "\n",
    "        if DEBUG:\n",
    "            hybrid_documents = hybrid_documents[0:N_DEBUG]\n",
    "            tokenized_hybrid_documents = tokenized_hybrid_documents[0:N_DEBUG]\n",
    "            GT = GT[0:N_DEBUG]\n",
    "        \n",
    "        # write csv (for debug purposes)\n",
    "        pd.DataFrame(zip(hybrid_documents, tokenized_hybrid_documents, GT), columns=[\"Hybrid Document\", \"Tokenized Hybrid Document\", \"GT\"]).to_csv(os.path.join(OUTPUT_DIR, detector.__class__.__name__+ \"-\"+explainer.__class__.__name__+\".csv\"),index=False)\n",
    "        print(\"Predicting hybrid documents\")\n",
    "        predictions_hybrid = detector.predict_label(hybrid_documents)\n",
    "\n",
    "        print(\"Generating explanations on hybrid documents and calculating pointing game accuracy\")\n",
    "\n",
    "        \n",
    "       # pointing_game_acc = pointing_game_util.get_pointing_game_acc(hybrid_documents, explainer, predictions_hybrid, GT)\n",
    "        pointing_game_scores = pointing_game_util.get_pointing_game_scores(hybrid_documents, explainer, predictions_hybrid, GT)\n",
    "      #  print(\"Pointing game accuracy for {} | {}: {}\".format(explainer.__class__.__name__, detector.__class__.__name__, pointing_game_acc))\n",
    "        results.extend([(explainer.__class__.__name__, detector.__class__.__name__, pointing_game_score) for pointing_game_score in pointing_game_scores])\n",
    "    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some formatting functions\n",
    "def get_p_asterisks(group):\n",
    "    val =  group.mean()\n",
    "   # print(group.name)\n",
    "    _, p = ttest_1samp(group, popmean=0.5)\n",
    "    if p <= 0.001:\n",
    "        return \"{:.2f}\\\\textsuperscript{{***}}\".format(val)\n",
    "    if p <= 0.01:\n",
    "        return \"{:.2f}\\\\textsuperscript{{**}}\".format(val)\n",
    "    if p <= 0.05:\n",
    "        return \"{:.2f}\\\\textsuperscript{{*}}\".format(val)\n",
    "    if p > 0.05:\n",
    "        return \"{:.2f}\\\\textsuperscript{{ns}}\".format(val)\n",
    "\n",
    "def highlight_max(col):\n",
    "    vals = col.str.extract(r\"(-*\\d*\\.\\d*)\").astype(float).values.flatten()\n",
    "    max_val = vals.max()\n",
    "    return [\"font-weight: bold;\" if c == max_val else \"\" for c in vals ]\n",
    "def df_to_latex(styled_df, caption=\"TODO\", label=\"TODO\"):\n",
    "    return styled_df.to_latex(environment=\"table\", convert_css=True, clines=\"all;data\", hrules=True, caption=caption, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_dff(dff, groupby):\n",
    "    p_results = dff.groupby(groupby).agg(\n",
    "    {\n",
    "          \"Pointing Game Scores\": get_p_asterisks,\n",
    "        }\n",
    "    )\n",
    "    p_results = p_results.style.apply(highlight_max, subset=p_results.columns)\n",
    "    return p_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Pointing Game Scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_94672_row1_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_94672\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_94672_level0_col0\" class=\"col_heading level0 col0\" >Pointing Game Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Explainer</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_94672_level0_row0\" class=\"row_heading level0 row0\" >LIME_Explainer</th>\n",
       "      <td id=\"T_94672_row0_col0\" class=\"data row0 col0\" >0.50\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94672_level0_row1\" class=\"row_heading level0 row1\" >SHAP_Explainer</th>\n",
       "      <td id=\"T_94672_row1_col0\" class=\"data row1 col0\" >0.70\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23cdab46fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_results_aggregate_level = style_dff(dff, groupby=[\"Explainer\"])\n",
    "display(p_results_aggregate_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b0b0b_row2_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b0b0b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b0b0b_level0_col0\" class=\"col_heading level0 col0\" >Pointing Game Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Explainer</th>\n",
       "      <th class=\"index_name level1\" >Detector</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b0b0b_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"2\">LIME_Explainer</th>\n",
       "      <th id=\"T_b0b0b_level1_row0\" class=\"row_heading level1 row0\" >DetectorGuo</th>\n",
       "      <td id=\"T_b0b0b_row0_col0\" class=\"data row0 col0\" >0.60\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0b0b_level1_row1\" class=\"row_heading level1 row1\" >DetectorRadford</th>\n",
       "      <td id=\"T_b0b0b_row1_col0\" class=\"data row1 col0\" >0.40\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0b0b_level0_row2\" class=\"row_heading level0 row2\" rowspan=\"2\">SHAP_Explainer</th>\n",
       "      <th id=\"T_b0b0b_level1_row2\" class=\"row_heading level1 row2\" >DetectorGuo</th>\n",
       "      <td id=\"T_b0b0b_row2_col0\" class=\"data row2 col0\" >0.80\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0b0b_level1_row3\" class=\"row_heading level1 row3\" >DetectorRadford</th>\n",
       "      <td id=\"T_b0b0b_row3_col0\" class=\"data row3 col0\" >0.60\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23cce2d0090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_results = style_dff(dff, groupby=[\"Explainer\", \"Detector\"])\n",
    "display(p_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Scores per detector and explainer}\n",
      "\\label{pointing-game-explainer-detector}\n",
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      " &  & Pointing Game Scores \\\\\n",
      "Explainer & Detector &  \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{LIME_Explainer} & DetectorGuo & 0.60\\textsuperscript{ns} \\\\\n",
      "\\cline{2-3}\n",
      " & DetectorRadford & 0.40\\textsuperscript{ns} \\\\\n",
      "\\cline{1-3} \\cline{2-3}\n",
      "\\multirow[c]{2}{*}{SHAP_Explainer} & DetectorGuo & \\bfseries 0.80\\textsuperscript{ns} \\\\\n",
      "\\cline{2-3}\n",
      " & DetectorRadford & 0.60\\textsuperscript{ns} \\\\\n",
      "\\cline{1-3} \\cline{2-3}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Scores per explainer}\n",
      "\\label{pointing-game-explainer}\n",
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      " & Pointing Game Scores \\\\\n",
      "Explainer &  \\\\\n",
      "\\midrule\n",
      "LIME_Explainer & 0.50\\textsuperscript{ns} \\\\\n",
      "\\cline{1-2}\n",
      "SHAP_Explainer & \\bfseries 0.70\\textsuperscript{ns} \\\\\n",
      "\\cline{1-2}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_to_latex(p_results, label=\"pointing-game-explainer-detector\", caption=\"Scores per detector and explainer\"))\n",
    "print(df_to_latex(p_results_aggregate_level, label=\"pointing-game-explainer\", caption=\"Scores per explainer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for hybrid_document in hybrid_documents:\n",
    "\n",
    "#     explainer = LIME_Explainer(detector)\n",
    "#     explainer.get_explanation_cached(hybrid_document).show_in_notebook()\n",
    "\n",
    "#     explainer = SHAP_Explainer(detector)\n",
    "#     shap.text_plot(explainer.get_explanation_cached(hybrid_document))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
