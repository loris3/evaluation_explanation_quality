{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_SUBMODULE_DIR = \"./survey\" # this notebook will create a folder \"explanations\" there\n",
    "SAVE_PATH = \"./results/user_study/selection.csv\" # created by document_selection_user_study.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the user-study csv from `SAVE_PATH` to the UI. The UI uses a JSON file for each document and explainer-detector pair. The explanations are stored as HTML files. For LIME and SHAP, javascript is bundled, for Anchor, it must be imported due to bundle size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detector_radford import DetectorRadford\n",
    "from detector_detectgpt import DetectorDetectGPT\n",
    "from detector_dummy import DetectorDummy\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_classes = [LIME_Explainer, SHAP_Explainer, Anchor_Explainer]\n",
    "detector_classes = [ DetectorGuo, DetectorRadford, DetectorDetectGPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Detector\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR)): \n",
    "    raise FileNotFoundError(\"Make sure you cloned the survey submodule\")\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the explanations are cached, this will still take some time as `detector.predict_proba` must be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export for user-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for detector_name, group in df.groupby(\"Detector\"):\n",
    "    if detector_name == \"DetectorDetectGPT\":\n",
    "        detector = DetectorDetectGPT()\n",
    "    if detector_name == \"DetectorRadford\":\n",
    "        detector = DetectorRadford()\n",
    "    if detector_name == \"DetectorGuo\":\n",
    "        detector = DetectorGuo()\n",
    "\n",
    "    documents_group = list(group.reset_index().iterrows()) # reset_index: count from 0 in each group\n",
    "    \n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        for index, row in tqdm(documents_group, \"Exporting \" + detector.__class__.__name__ + \" \" + explainer.__class__.__name__):\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            # Phase 1 + 3: \n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(a)+\".html\")\n",
    "            with open(path_explanation_html, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                text_file.write(explainer.get_HTML(a, bundle=False))\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(a)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"ground_truth\" : row[\"GT a\"],\n",
    "                    \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 1+3\"]])[0]),\n",
    "                    \"detector_p_machine\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][0]),\n",
    "                    \"detector_p_human\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][1]),\n",
    "                    \"document\": row[\"Documents Phases 1+3\"],\n",
    "                    \"explanation_filename\": explainer.get_hash(a),               \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n",
    "            # Phase 2+4: Just string\n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(b)+\".html\")\n",
    "            #  display(HTML(explainer.get_HTML(b)))\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(b)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"document\": row[\"Documents Phases 2+4\"]                \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export as HTML for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One file for the selecting combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"<html><body>\"\n",
    "out += \"<h1>Pairs as generated in the notebook</h1>\"\n",
    "\n",
    "for (detector_name, explainer_name), group in df.groupby([\"Detector\", \"Explainer\"]):\n",
    "    out += \"<h2>{}</h2>\".format(detector_name)\n",
    "    if detector_name == \"DetectorDetectGPT\":\n",
    "        detector = DetectorDetectGPT()\n",
    "    if detector_name == \"DetectorRadford\":\n",
    "        detector = DetectorRadford()\n",
    "    if detector_name == \"DetectorGuo\":\n",
    "        detector = DetectorGuo()\n",
    "\n",
    "    if explainer_name == \"LIME_Explainer\":\n",
    "        explainer = LIME_Explainer(detector)\n",
    "    if explainer_name == \"SHAP_Explainer\":\n",
    "        explainer = SHAP_Explainer(detector)\n",
    "    if explainer_name == \"Anchor_Explainer\":\n",
    "        explainer = Anchor_Explainer(detector)\n",
    "    documents_group = list(group.reset_index().iterrows()) # reset_index: count from 0 in each group\n",
    "    \n",
    "\n",
    "    for index, row in tqdm(documents_group, \"Exporting \" + detector.__class__.__name__ + \" \" + explainer.__class__.__name__):\n",
    "        a = row[\"Documents Phases 1+3\"]\n",
    "        b = row[\"Documents Phases 2+4\"]\n",
    "        template = \"\"\"\n",
    "        Document: {document_nr}<br/>\n",
    "        Detector: {detector}<br/>\n",
    "        Explainer: {explainer}<br/>\n",
    "        Ground Truth: {ground_truth}<br/>\n",
    "        f(x): {detector_label}\n",
    "        \"\"\"\n",
    "        # Phase 1 + 3: \n",
    "        out += \"<h3>Pair {}</h3>\".format(index)\n",
    "\n",
    "        explanation_data = { \n",
    "            \"document_nr\": row[\"idx a\"],\n",
    "            \"detector\" : detector.__class__.__name__,\n",
    "            \"explainer\" : explainer.__class__.__name__,\n",
    "            \"ground_truth\" : row[\"GT a\"],\n",
    "            \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 1+3\"]])[0]),\n",
    "        }\n",
    "        out+= template.format(**explanation_data)\n",
    "        out += explainer.get_HTML(a, bundle=True)\n",
    "       \n",
    "\n",
    "        explanation_data = { \n",
    "            \"document_nr\": row[\"idx b\"],\n",
    "            \"detector\" : detector.__class__.__name__,\n",
    "            \"explainer\" : explainer.__class__.__name__,\n",
    "            \"ground_truth\" : row[\"GT b\"],\n",
    "            \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 2+4\"]])[0]),           \n",
    "        }\n",
    "        out+= template.format(**explanation_data)\n",
    "        out += explainer.get_HTML(b, bundle=True)\n",
    "        out +=\"<hr/>\"\n",
    "out +=\"</body></html>\"\n",
    "with open(\"rendered_datasets_user_study/dataset.html\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And one per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for detector_name, group in df.groupby(\"Detector\"):\n",
    "   \n",
    "    if detector_name == \"DetectorDetectGPT\":\n",
    "        detector = DetectorDetectGPT()\n",
    "    if detector_name == \"DetectorRadford\":\n",
    "        detector = DetectorRadford()\n",
    "    if detector_name == \"DetectorGuo\":\n",
    "        detector = DetectorGuo()\n",
    "\n",
    "\n",
    "    documents_group = list(group.reset_index().iterrows()) # reset_index: count from 0 in each group\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        out = \"<html><body>\"\n",
    "        out += \"<h1>Pairs as displayed to this group</h1>\"\n",
    "        out += \"<p>Users only see the first explanation of each pair.</p>\"\n",
    "        for index, row in tqdm(documents_group, \"Exporting \" + detector.__class__.__name__ + \" \" + explainer.__class__.__name__):\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            template = \"\"\"\n",
    "            Document: {document_nr}<br/>\n",
    "            Displayed Explainer: {explainer}<br/>\n",
    "            Selecting Explainer: {explainer_s}<br/>\n",
    "            Ground Truth: {ground_truth}<br/>\n",
    "            f(x): {detector_label}\n",
    "            \"\"\"\n",
    "            # Phase 1 + 3: \n",
    "            out += \"<h3>Pair {}</h3>\".format(index)\n",
    "\n",
    "            explanation_data = { \n",
    "                \"document_nr\": row[\"idx a\"],\n",
    "                \"explainer\" : explainer.__class__.__name__,\n",
    "                \"explainer_s\" : row[\"Explainer\"],\n",
    "                \"ground_truth\" : row[\"GT a\"],\n",
    "                \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 1+3\"]])[0]),\n",
    "            }\n",
    "            out+= template.format(**explanation_data)\n",
    "            out += explainer.get_HTML(a, bundle=True)\n",
    "        \n",
    "\n",
    "            explanation_data = { \n",
    "                \"document_nr\": row[\"idx b\"],\n",
    "                \"explainer\" : explainer.__class__.__name__,\n",
    "                \"explainer_s\" : row[\"Explainer\"],\n",
    "                \"ground_truth\" : row[\"GT b\"],\n",
    "                \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 2+4\"]])[0]),           \n",
    "            }\n",
    "            out+= template.format(**explanation_data)\n",
    "            out += explainer.get_HTML(b, bundle=True)\n",
    "            out +=\"<hr/>\"\n",
    "        out +=\"</body></html>\"\n",
    "        with open(\"rendered_datasets_user_study/groups/{}-{}.html\".format(detector_name, explainer.__class__.__name__), \"w\", encoding=\"UTF-8\") as text_file:\n",
    "            text_file.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Explainer(DetectorDummy(human_watermark=\"example\", machine_watermark=\"an\")).get_HTML(\"This is an example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
