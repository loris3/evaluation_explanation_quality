{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_SUBMODULE_DIR = \"../survey\" # this notebook will create a folder \"explanations\" there\n",
    "SAVE_PATH = \"./dataset_user_study_18.csv\" # created by document_selection_user_study.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the user-study csv from `SAVE_PATH` to the UI. The UI uses a JSON file for each document and explainer-detector pair. The explanations are stored as HTML files. For LIME and SHAP, javascript is bundled, for Anchor, it must be imported due to bundle size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n"
     ]
    }
   ],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_dummy import DetectorDummy\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_classes = [LIME_Explainer, SHAP_Explainer, Anchor_Explainer]\n",
    "detector_classes = [ DetectorGuo, DetectorRadford, DetectorDetectGPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Documents Phases 1+3</th>\n",
       "      <th>Documents Phases 2+4</th>\n",
       "      <th>f(a)</th>\n",
       "      <th>f(b)</th>\n",
       "      <th>GT a</th>\n",
       "      <th>GT b</th>\n",
       "      <th>idx a</th>\n",
       "      <th>idx b</th>\n",
       "      <th>Spacy Similarity</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "      <th>Cosine Similarity tfidf</th>\n",
       "      <th>hash a</th>\n",
       "      <th>hash b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DetectorDetectGPT</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unnamed: 0  Explainer  Documents Phases 1+3  \\\n",
       "Detector                                                         \n",
       "DetectorDetectGPT          18         18                    18   \n",
       "DetectorGuo                18         18                    18   \n",
       "DetectorRadford            18         18                    18   \n",
       "\n",
       "                   Documents Phases 2+4  f(a)  f(b)  GT a  GT b  idx a  idx b  \\\n",
       "Detector                                                                        \n",
       "DetectorDetectGPT                    18    18    18    18    18     18     18   \n",
       "DetectorGuo                          18    18    18    18    18     18     18   \n",
       "DetectorRadford                      18    18    18    18    18     18     18   \n",
       "\n",
       "                   Spacy Similarity  Jaccard Similarity  \\\n",
       "Detector                                                  \n",
       "DetectorDetectGPT                18                  18   \n",
       "DetectorGuo                      18                  18   \n",
       "DetectorRadford                  18                  18   \n",
       "\n",
       "                   Cosine Similarity tfidf  hash a  hash b  \n",
       "Detector                                                    \n",
       "DetectorDetectGPT                       18      18      18  \n",
       "DetectorGuo                             18      18      18  \n",
       "DetectorRadford                         18      18      18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"Detector\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR)): \n",
    "    raise FileNotFoundError(\"Make sure you cloned the survey submodule\")\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the explanations are cached, this will still take some time as `detector.predict_proba` must be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.21s)\n",
      "DONE (0.09s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting DetectorDetectGPT LIME_Explainer: 100%|██████████| 18/18 [00:52<00:00,  2.92s/it]\n",
      "Exporting DetectorDetectGPT SHAP_Explainer: 100%|██████████| 18/18 [00:54<00:00,  3.01s/it]\n",
      "Exporting DetectorDetectGPT Anchor_Explainer: 100%|██████████| 18/18 [00:54<00:00,  3.04s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Exporting DetectorGuo LIME_Explainer: 100%|██████████| 18/18 [00:01<00:00, 10.24it/s]\n",
      "Exporting DetectorGuo SHAP_Explainer: 100%|██████████| 18/18 [00:01<00:00,  9.76it/s]\n",
      "Exporting DetectorGuo Anchor_Explainer: 100%|██████████| 18/18 [00:02<00:00,  6.32it/s]\n",
      "Exporting DetectorRadford LIME_Explainer: 100%|██████████| 18/18 [00:01<00:00,  9.39it/s]\n",
      "Exporting DetectorRadford SHAP_Explainer: 100%|██████████| 18/18 [00:01<00:00, 10.10it/s]\n",
      "Exporting DetectorRadford Anchor_Explainer: 100%|██████████| 18/18 [00:02<00:00,  6.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for detector_name, group in df.groupby(\"Detector\"):\n",
    "    if detector_name == \"DetectorDetectGPT\":\n",
    "        detector = DetectorDetectGPT()\n",
    "    if detector_name == \"DetectorRadford\":\n",
    "        detector = DetectorRadford()\n",
    "    if detector_name == \"DetectorGuo\":\n",
    "        detector = DetectorGuo()\n",
    "\n",
    "    documents_group = list(group.reset_index().iterrows()) # reset_index: count from 0 in each group\n",
    "    \n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        for index, row in tqdm(documents_group, \"Exporting \" + detector.__class__.__name__ + \" \" + explainer.__class__.__name__):\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            # Phase 1 + 3: \n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(a)+\".html\")\n",
    "            with open(path_explanation_html, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                text_file.write(explainer.get_HTML(a, bundle=False))\n",
    "            # print(row)\n",
    "            # display(HTML(explainer.get_HTML(a)))\n",
    "            # display(HTML(explainer.get_HTML(b)))\n",
    "            # print(\"----------------\")\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(a)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"ground_truth\" : row[\"GT a\"],\n",
    "                    \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 1+3\"]])[0]),\n",
    "                    \"detector_p_machine\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][0]),\n",
    "                    \"detector_p_human\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][1]),\n",
    "                    \"document\": row[\"Documents Phases 1+3\"],\n",
    "                    \"explanation_filename\": explainer.get_hash(a),               \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n",
    "            # Phase 2+4: Just string\n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(b)+\".html\")\n",
    "            #  display(HTML(explainer.get_HTML(b)))\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(b)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"document\": row[\"Documents Phases 2+4\"]                \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
