{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_SUBMODULE_DIR = \"./survey\" # this notebook will create a folder \"explanations\" there\n",
    "SAVE_PATH = \"./dataset_user_study.csv\" # created by document_selection_user_study.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the user-study csv from `SAVE_PATH` to the UI. The UI uses a JSON file for each document and explainer-detector pair. The explanations are stored as HTML files. For LIME and SHAP, javascript is bundled, for Anchor, it must be imported due to bundle size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_dummy import DetectorDummy\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_classes = [LIME_Explainer, SHAP_Explainer, Anchor_Explainer]\n",
    "detector_classes = [ DetectorGuo, DetectorRadford, DetectorDetectGPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataset_user_study_new.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Detector\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR)): \n",
    "    raise FileNotFoundError(\"Make sure you cloned the survey submodule\")\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\"))\n",
    "if not os.path.exists(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\")): \n",
    "    os.makedirs(os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the explanations are cached, this will still take some time as `detector.predict_proba` must be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for detector_name, group in df.groupby(\"Detector\"):\n",
    "    if detector_name == \"DetectorDetectGPT\":\n",
    "        detector = DetectorDetectGPT()\n",
    "    if detector_name == \"DetectorRadford\":\n",
    "        detector = DetectorRadford()\n",
    "    if detector_name == \"DetectorGuo\":\n",
    "        detector = DetectorGuo()\n",
    "\n",
    "    documents_group = list(group.reset_index().iterrows()) # reset_index: count from 0 in each group\n",
    "    \n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        for index, row in tqdm(documents_group, \"Exporting \" + detector.__class__.__name__ + \" \" + explainer.__class__.__name__):\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            # Phase 1 + 3: \n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(a)+\".html\")\n",
    "            with open(path_explanation_html, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                text_file.write(explainer.get_HTML(a, bundle=False))\n",
    "            # print(row)\n",
    "            # display(HTML(explainer.get_HTML(a)))\n",
    "            # display(HTML(explainer.get_HTML(b)))\n",
    "            # print(\"----------------\")\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(a)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"ground_truth\" : row[\"GT a\"],\n",
    "                    \"detector_label\" : int(detector.predict_label([row[\"Documents Phases 1+3\"]])[0]),\n",
    "                    \"detector_p_machine\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][0]),\n",
    "                    \"detector_p_human\": float(detector.predict_proba([row[\"Documents Phases 1+3\"]])[0][1]),\n",
    "                    \"document\": row[\"Documents Phases 1+3\"],\n",
    "                    \"explanation_filename\": explainer.get_hash(a),               \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n",
    "            # Phase 2+4: Just string\n",
    "            path_explanation_html = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"html\", explainer.get_hash(b)+\".html\")\n",
    "            #  display(HTML(explainer.get_HTML(b)))\n",
    "            path_explanation_json = os.path.join(SURVEY_SUBMODULE_DIR, \"explanations\", \"data\", explainer.get_hash(b)+\".json\")\n",
    "            with open(path_explanation_json, \"w\", encoding=\"UTF-8\") as text_file:\n",
    "                explanation_data = { # do not include explanation\n",
    "                    \"document_nr\": index,\n",
    "                    \"detector\" : detector.__class__.__name__,\n",
    "                    \"explainer\" : explainer.__class__.__name__,\n",
    "                    \"document\": row[\"Documents Phases 2+4\"]                \n",
    "                }\n",
    "                text_file.write(json.dumps(explanation_data))\n",
    "        # break    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
