{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./results/benchmark/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorGuo, DetectorRadford, DetectorDetectGPT]\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "train = pd.read_pickle(\"./dataset_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CACHE_DIR): \n",
    "    os.makedirs(CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.concat([test[\"answer\"], train[\"answer\"]])\n",
    "gold_labels = pd.concat([(test[\"author\"] == \"human_answers\") ,  train[\"author\"] == \"human_answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "columns = [\"Detector\", \"Acc\", \"F1\", \"ROC AUC\", \"TN\", \"FP\", \"FN\", \"TP\", \"ms/evaluation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(label, detector):\n",
    "    if os.path.isfile(os.path.join(CACHE_DIR, label)):\n",
    "        return\n",
    "    start = time.time_ns()\n",
    "    predictions = detector.predict_label(documents) # seed is set in detectors by default\n",
    "    end = time.time_ns()\n",
    "    with open(os.path.join(CACHE_DIR, label), 'wb') as f:\n",
    "        pickle.dump((predictions, ((end - start) / len(documents))// 1000000), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "\n",
    "# detectGPT_default = DetectorDetectGPT()\n",
    "# detectGPT_default.n_perturbations = 100\n",
    "\n",
    "# detectGPT_default.base_model_name = \"gpt2-xl\"\n",
    "# base_model, base_tokenizer = detectGPT_default.load_base_model_and_tokenizer(detectGPT_default.base_model_name)\n",
    "# detectGPT_default.base_model = base_model\n",
    "# detectGPT_default.base_tokenizer = base_tokenizer\n",
    "\n",
    "# mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(detectGPT_default.mask_filling_model_name, cache_dir=detectGPT_default.cache_dir)\n",
    "# detectGPT_default.mask_model = mask_model\n",
    "\n",
    "# mask_tokenizer = transformers.AutoTokenizer.from_pretrained(detectGPT_default.mask_filling_model_name, model_max_length=mask_model.config.n_positions, cache_dir=detectGPT_default.cache_dir)\n",
    "# detectGPT_default.mask_tokenizer = mask_tokenizer\n",
    "\n",
    "# detectGPT_default.load_base_model()\n",
    "# detectGPT_default.load_mask_model()\n",
    "\n",
    "# run(DetectorDetectGPT.__name__+\" @100 GPT-2\", detectGPT_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detectGPT_pythia_100 = DetectorDetectGPT()\n",
    "# detectGPT_pythia_100.n_perturbations = 100\n",
    "\n",
    "# run(DetectorDetectGPT.__name__ +\" @100\", detectGPT_pythia_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for detector_class in detector_classes:\n",
    "#     run(detector_class.__name__, detector_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = []\n",
    "results_full = []\n",
    "for label in os.listdir(CACHE_DIR): \n",
    "    with open(os.path.join(CACHE_DIR, label) , 'rb') as f:\n",
    "        predictions, time = pickle.load(f)\n",
    "        results_test.append((label,\n",
    "                    accuracy_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    f1_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    roc_auc_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    *confusion_matrix(gold_labels[0:len(test)], predictions[0:len(test)]).ravel(), # TN, FP, FN, TP\n",
    "                    time\n",
    "                    ))\n",
    "        results_full.append((label,\n",
    "                    accuracy_score(gold_labels, predictions),\n",
    "                    f1_score(gold_labels, predictions),\n",
    "                    roc_auc_score(gold_labels, predictions),\n",
    "                    *confusion_matrix(gold_labels, predictions).ravel(), # TN, FP, FN, TP\n",
    "                    time\n",
    "                    ))\n",
    "df_test = pd.DataFrame(results_test, columns=columns).set_index(\"Detector\")\n",
    "df_full = pd.DataFrame(results_full, columns=columns).set_index(\"Detector\")\n",
    "\n",
    "display(df_test)\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"figures/benchmark_test.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(df_test.style.format(precision=3).to_latex(environment=\"table\", \n",
    "                                        convert_css=True, \n",
    "                                        clines=\"all;data\", \n",
    "                                        hrules=True, \n",
    "                                        caption=\"Performance on the dataset explanations where generated for (balanced, n={})\".format(len(test)), \n",
    "                                        label=\"table-benchmark_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"figures/benchmark_full.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(df_full.style.to_latex(environment=\"table\", \n",
    "                                        convert_css=True, \n",
    "                                        clines=\"all;data\", \n",
    "                                        hrules=True, \n",
    "                                        caption=\"Performance on the full dataset (balanced, n={})\".format(len(train)+len(test)), \n",
    "                                        label=\"table-benchmark_full\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
