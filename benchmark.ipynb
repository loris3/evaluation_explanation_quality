{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./benchmark_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorGuo, DetectorRadford, DetectorDetectGPT]\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "train = pd.read_pickle(\"./dataset_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CACHE_DIR): \n",
    "    os.makedirs(CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.concat([test[\"answer\"], train[\"answer\"]])\n",
    "gold_labels = pd.concat([(test[\"author\"] == \"human_answers\") ,  train[\"author\"] == \"human_answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = []\n",
    "results = []\n",
    "columns = [\"Detector\", \"Acc\", \"F1\", \"ROC AUC\", \"TN\", \"FP\", \"FN\", \"TP\", \"ms/evaluation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detector = DetectorGuo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(label, detector):\n",
    "    if os.path.isfile(os.path.join(CACHE_DIR, label)):\n",
    "        return\n",
    "    start = time.time_ns()\n",
    "    predictions = detector.predict_label(documents) # seed is set in detectors by default\n",
    "    end = time.time_ns()\n",
    "    with open(os.path.join(CACHE_DIR, label), 'wb') as f:\n",
    "        pickle.dump((predictions, ((end - start) / len(documents))// 1000000), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ms/evaluation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.990164</td>\n",
       "      <td>0.990033</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Acc        F1   ROC AUC   TN  FP  FN   TP  ms/evaluation\n",
       "Detector                                                               \n",
       "test      0.990164  0.990033  0.990132  153   0   3  149           18.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ms/evaluation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.996063</td>\n",
       "      <td>0.996047</td>\n",
       "      <td>0.996063</td>\n",
       "      <td>508</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>504</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Acc        F1   ROC AUC   TN  FP  FN   TP  ms/evaluation\n",
       "Detector                                                               \n",
       "test      0.996063  0.996047  0.996063  508   0   4  504           18.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test = []\n",
    "results_full = []\n",
    "for label in os.listdir(CACHE_DIR): \n",
    "    with open(os.path.join(CACHE_DIR, label) , 'rb') as f:\n",
    "        predictions, time = pickle.load(f)\n",
    "        results_test.append((label,\n",
    "                    accuracy_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    f1_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    roc_auc_score(gold_labels[0:len(test)], predictions[0:len(test)]),\n",
    "                    *confusion_matrix(gold_labels[0:len(test)], predictions[0:len(test)]).ravel(), # TN, FP, FN, TP\n",
    "                    time\n",
    "                    ))\n",
    "        results_full.append((label,\n",
    "                    accuracy_score(gold_labels, predictions),\n",
    "                    f1_score(gold_labels, predictions),\n",
    "                    roc_auc_score(gold_labels, predictions),\n",
    "                    *confusion_matrix(gold_labels, predictions).ravel(), # TN, FP, FN, TP\n",
    "                    time\n",
    "                    ))\n",
    "df_test = pd.DataFrame(results_test, columns=columns).set_index(\"Detector\")\n",
    "df_full = pd.DataFrame(results_full, columns=columns).set_index(\"Detector\")\n",
    "\n",
    "display(df_test)\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "detectGPT_default = DetectorDetectGPT()\n",
    "detectGPT_default.n_perturbations = 100\n",
    "\n",
    "detectGPT_default.base_model_name = \"gpt2-xl\"\n",
    "detectGPT_default.mask_filling_model_name = \"t5-3b\"\n",
    "base_model, base_tokenizer = detectGPT_default.load_base_model_and_tokenizer(detectGPT_default.base_model_name)\n",
    "detectGPT_default.base_model = base_model\n",
    "detectGPT_default.base_tokenizer = base_tokenizer\n",
    "\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(detectGPT_default.mask_filling_model_name, cache_dir=detectGPT_default.cache_dir)\n",
    "detectGPT_default.mask_model = mask_model\n",
    "\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(detectGPT_default.mask_filling_model_name, model_max_length=mask_model.config.n_positions, cache_dir=detectGPT_default.cache_dir)\n",
    "detectGPT_default.mask_tokenizer = mask_tokenizer\n",
    "\n",
    "detectGPT_default.load_base_model()\n",
    "detectGPT_default.load_mask_model()\n",
    "\n",
    "run(DetectorDetectGPT.__name__+\" @100 GPT-2\", detectGPT_default)\n",
    "pd.DataFrame(results, columns=columns).set_index(\"Detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detectGPT_pythia_100 = DetectorDetectGPT()\n",
    "detectGPT_pythia_100.n_perturbations = 100\n",
    "detectors = [(DetectorDetectGPT.__name__ +\" @100\", detectGPT_pythia_100)]\n",
    "run()\n",
    "pd.DataFrame(results, columns=columns).set_index(\"Detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detectors = [(detector_class.__name__, detector_class()) for detector_class in detector_classes]\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=columns).set_index(\"Detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ms/evaluation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DetectorDetectGPT @100</th>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.703947</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>62</td>\n",
       "      <td>6087.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.990164</td>\n",
       "      <td>0.990033</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.921311</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.921354</td>\n",
       "      <td>139</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>142</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorDetectGPT</th>\n",
       "      <td>0.744262</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.743486</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>77</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Acc        F1   ROC AUC   TN  FP  FN   TP  \\\n",
       "Detector                                                                 \n",
       "DetectorDetectGPT @100  0.704918  0.579439  0.703947  153   0  90   62   \n",
       "DetectorGuo             0.990164  0.990033  0.990132  153   0   3  149   \n",
       "DetectorRadford         0.921311  0.922078  0.921354  139  14  10  142   \n",
       "DetectorDetectGPT       0.744262  0.663793  0.743486  150   3  75   77   \n",
       "\n",
       "                        ms/evaluation  \n",
       "Detector                               \n",
       "DetectorDetectGPT @100         6087.0  \n",
       "DetectorGuo                      18.0  \n",
       "DetectorRadford                  19.0  \n",
       "DetectorDetectGPT               833.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"figures/benchmark.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(df.style.to_latex(environment=\"table\", \n",
    "                                        convert_css=True, \n",
    "                                        clines=\"all;data\", \n",
    "                                        hrules=True, \n",
    "                                        caption=\"Performance on the dataset explanations where generated for (balanced, n={})\".format(len(documents)), \n",
    "                                        label=\"table-benchmark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't435regfsdxvc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mt435regfsdxvc\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't435regfsdxvc' is not defined"
     ]
    }
   ],
   "source": [
    "t435regfsdxvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*3*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DetectorRadford().predict_label([documents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for detector_class in detector_classes:\n",
    "#     detector = detector_class()\n",
    "#     start = time.time()\n",
    "#     y = detector.predict_label(documents)\n",
    "#     end = time.time()\n",
    "\n",
    "#     acc = sum(y == gold_labels)/ len(documents)\n",
    "#     results.append((detector.__class__.__name__, acc, end - start))\n",
    "#     print(results[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector = DetectorDetectGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector.get_pad_token_id_masker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = detector.predict_proba(documents, deterministic=True)\n",
    "# yy = np.array(y).argmax(axis=1)\n",
    "# acc = sum(yy == gold_labels)/ len(documents)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = detector.predict_proba(documents, deterministic=True)\n",
    "# yy = np.array(y).argmax(axis=1)\n",
    "# acc = sum(yy == gold_labels)/ len(documents)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document, gt in zip(documents,gold_labels):\n",
    "#     explainer = SHAP_Explainer(detector)\n",
    "#     print(gt)\n",
    "#     display(HTML(explainer.get_vanilla_visualization_HTML(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in tqdm(documents):\n",
    "#     detector.predict_proba([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer DetectorGuo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [1:49:26<00:00,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME_Explainer DetectorGuo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [3:39:30<00:00, 18.52s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [1:52:49<00:00,  9.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [4:06:20<00:00, 20.79s/it]  \n"
     ]
    }
   ],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__, detector.__class__.__name__)\n",
    "        docs = [document for document in documents if not explainer.is_cached(document)]\n",
    "        #docs = docs[0:len(docs)//2]\n",
    "        for document in tqdm(docs):\n",
    "            explainer.get_explanation_cached(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector = DetectorDetectGPT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (4294918467.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    345wter78/2\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "345wter78/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detector = DetectorRadford()\n",
    "# explainer = LIME_Explainer(detector)\n",
    "\n",
    "# explainer.get_fi_scores_batch(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = LIME_Explainer(detector)\n",
    "\n",
    "# explainer.get_fi_scores_batch(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector=DetectorDetectGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = Anchor_Explainer(detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"This is a sentence, machine human.\"\n",
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words, positions, true_label, sample_fn = explainer.explainer.get_sample_fn(sent, detector.predict_label, onepass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sample_fn([], 100)\n",
    "# # [\"\".join(a) for a in sentences.reshape(sentences.shape[0], -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sample_fn([], 100)\n",
    "# # [\"\".join(a) for a in sentences.reshape(sentences.shape[0], -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5ztrhdgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'An example is a specific instance or illustration that represents a general idea or concept. It serves to clarify or demonstrate a point, making complex ideas more understandable. For instance, a red apple can be an example of a fruit, showcasing the broader category of fruits through a specific case.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Institutional investors are organizations that invest on behalf of their clients, such as pension funds, mutual funds, endowments, and insurance companies. These investors typically have a large amount of capital to invest and may be seeking to diversify their portfolios across a variety of asset classes. Institutional investors may also be seeking to generate returns or achieve specific investment goals on behalf of their clients. They may conduct extensive research and analysis to identify investment opportunities and make informed decisions about where to allocate their capital. Institutional investors play a significant role in financial markets and often have a significant impact on the prices of securities and other assets.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "text = documents.iloc[i]\n",
    "print(gold_labels.iloc[i])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\" \".join([t if random.random() < 0.95 else detector.get_pad_token() for t in text.split(\" \")]) for _ in range(0,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.predict_proba([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.predict_label([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ChatGPT prompt: \"What is an example? Answer in 50 words\"\n",
    "# exp = explainer.get_explanation_cached('An example is a specific instance or illustration that represents a general idea or concept. It serves to clarify or demonstrate a point, making complex ideas more understandable. For instance, a red apple can be an example of a fruit, showcasing the broader category of fruits through a specific case.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_explanation.AnchorExplanation('text', exp, explainer.explainer.as_html).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance(explainer, Anchor_Explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document, gt  in tqdm(zip(documents, gold_labels)):\n",
    "\n",
    "    exp = explainer.get_explanation_cached(document)\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        computation_times.append((exp[\"computation_time\"], gt, len(document.split(\" \"))))\n",
    "   # anchor_explanation.AnchorExplanation('text', exp, explainer.explainer.as_html).show_in_notebook()\n",
    "    df = pd.DataFrame(computation_times, columns=[\"time\", \"gt\",  \"len\"])\n",
    "    display(df)\n",
    "    print(df[\"time\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(computation_times, columns=[\"time\", \"gt\", \"prediction\", \"len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
