{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: Always set this flag to `True` before git commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBFUSCATE_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "import lime\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<p><b>This is a {kind_of_document} document.</b></p>\n",
    "<p>The detector {correctly_or_wrongly} predicted that this document was... </p>\n",
    "<p>&emsp; ... machine generated with {p_machine} % confidence.</p>\n",
    "<p>&emsp; ... human written with {p_human} % confidence.</p> \n",
    "<div style=\"float:left;\">{highlighted_text}</div>\n",
    "\"\"\"\n",
    "#<div style=\"float:left; height:30em;\">{barplot_machine}{barplot_human}</div>\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "def print_template(document, gold_label, detector, explainer):\n",
    "    p_machine, p_human = detector.predict_proba([document])[0]\n",
    "   # machine, human = explainer.get_barplots_HTML(document)\n",
    "    display(HTML(template.format(\n",
    "    p_machine=int(p_machine*100) if not OBFUSCATE_RESULTS else \"<redacted>\", \n",
    "    p_human=int(p_human*100) if not OBFUSCATE_RESULTS else \"<redacted>\",\n",
    "  #  barplot_machine=machine,\n",
    "  #  barplot_human=human,\n",
    "    kind_of_document= ((\"machine generated\" if gold_label == False else \"human written\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    correctly_or_wrongly= ((\"correctly\" if detector.predict_label([document])[0] == gold_label else \"wrongly\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    highlighted_text=explainer.get_highlighted_text_HTML((document if not OBFUSCATE_RESULTS else \"<redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted>\")),\n",
    "    )))\n",
    "def print_shared_features(features, fi_scores):\n",
    "    for feature, (fi_score_a, fi_score_b) in zip(features, fi_scores):\n",
    "        print(feature)\n",
    "        print(\"\\t\\ta: {} \\t b: {}\".format(fi_score_a, fi_score_b))\n",
    "        print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer, skip_n=0):\n",
    "\n",
    "    for (a,b)in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            print(\"idx_a: <redacted> idx_b <redacted>\")\n",
    "        else:\n",
    "            print(\"idx_a: {} idx_b {}\".format(document_ids[a],document_ids[b]))\n",
    "\n",
    "        print_template(documents[a], gold_labels[a], detector, explainer)\n",
    "        print_template(documents[b], gold_labels[b], detector, explainer)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Feature Importance Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix of explanations for all documents in \"data\"\n",
    "# This function was once SubmodularPick.__init__() in LIME. It was planned to use its output for a search strategy for similar explanations. \n",
    "# Only the code for creating W from the paper (rows are explanations, cols are BOW features) remains\n",
    "def get_explanation_matrix_W(data, explainer, quiet=False):\n",
    "    # Get (cached) explanations \n",
    "    explanations_and_documents = [(d, explainer.get_fi_scores(d, fill=True)[0]) for d in tqdm(data, desc=\"Loading all explanations\",disable=quiet) ] # [0]: only irt to label machine, fill: return all words, even those with 0 fi\n",
    "\n",
    "    get_feature_name_signed = lambda feature,value : feature + (\"_+\" if value >=0 else \"_-\") # appends \"_+\" or \"_-\" to each feature name, e.g. \"example\" -> \"example_+\" if fi(example) > 0\n",
    "    # Ribeiro et al.: Find all the explanation model features used. Defines the dimension d'\n",
    "    # i.e. determine columns of W: each word (BOW) gets (up to) two columns, one for positive FI scores, one for negative FI scores\n",
    "    features_dict = {}\n",
    "    feature_iter = 0\n",
    "    for d, exp in tqdm(explanations_and_documents, desc=\"Building global dict of features\", disable=quiet):\n",
    "     #   print(\"exp\",exp)\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            feature_name = get_feature_name_signed(feature,value) # get_feature_name_signed: see above\n",
    "            if feature_name not in features_dict.keys():\n",
    "                features_dict[feature_name] = (feature_iter)\n",
    "                feature_iter += 1\n",
    "    d_prime = len(features_dict.keys())\n",
    "\n",
    "    # Ribeiro et al.: Create the n x d' dimensional 'explanation matrix', W\n",
    "    W = np.zeros((len(explanations_and_documents), d_prime))\n",
    "\n",
    "    # fill W, look up cols in dict that was just created\n",
    "    # W: one row per explanation, one col per feature in feature_dict\n",
    "    for i, (d, exp) in enumerate(tqdm(explanations_and_documents,  desc=\"Building W\",disable=quiet)):\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            # get_feature_name_signed: see above\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            W[i, features_dict[get_feature_name_signed(feature,value)]] += value\n",
    "    return W, features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples: (pair of documents whose explanations are similar, the features that overlap, fi scores of said features)\n",
    "# this maximizes similarity between documents (greedy, each document is only part of one tuple)\n",
    "# another function should select n tuples to maximize coverage in explanation space akin to SP-LIME\n",
    "sum_two_max = None\n",
    "def get_pairs(documents, W, detector, features_dict, n_pairs=None):\n",
    "    if n_pairs is None:\n",
    "        n_pairs = len(documents)//2\n",
    "    idx_pairs = [] # tuples of indices of similar documents a,b in \"data\"\n",
    "    features = [] # list of features those documents covered\n",
    "    fi_scores_pairs = []\n",
    "\n",
    "    W_ = np.copy(W)\n",
    "\n",
    "    document_indices = np.arange(0, W_.shape[0])\n",
    "   # print(document_indices.shape, W_.shape)\n",
    "    for _ in tqdm(range(0,n_pairs), desc=\"Obtaining pairs\"):\n",
    "        sim = cosine_similarity(W_) # calculate cosine similarity between all explanations\n",
    "        sim = np.triu(sim,k=1)  # remove redundant information for argmax()\n",
    "\n",
    "        idx_max = np.unravel_index(sim.argmax(), sim.shape) # get most similar pair, result is (idx_a, idx_b)\n",
    "       # print(idx_max)\n",
    "        features_non_zero_in_both = np.intersect1d(W_[idx_max[0]].nonzero(),W_[idx_max[1]].nonzero()) # get features that have non-zero fi in both explanations\n",
    "                                                                                                      # is used later for selecting a set of tuples with high coverage (as in SP-LIME)\n",
    "        non_zero_features = [] # list with features that will be returned\n",
    "        non_zero_fi_scores_tuples = [] # list of tuples with fi scores in a and b that will be returned\n",
    "    \n",
    "        # look up feature_idxs in features_dict and append them to the output\n",
    "        for iii in features_non_zero_in_both:\n",
    "           key = list(features_dict.keys())[list(features_dict.values()).index(iii)]\n",
    "           non_zero_features.append(key)\n",
    "           non_zero_fi_scores_tuples.append((W_[idx_max[0],features_dict[key]], W_[idx_max[1],features_dict[key]]))\n",
    "        \n",
    "        # Only add pair to output list if valid: at least one common feature is not zero AND f(a) == f(b) (i.e., the explanation is arguing for the same detector verdict)\n",
    "        if len(non_zero_features) > 0:\n",
    "            a,b = detector.predict_label([documents[document_indices[idx_max[0]]], documents[document_indices[idx_max[1]]]])\n",
    "            if a == b:\n",
    "                idx_pairs.append(document_indices[list(idx_max)])\n",
    "                fi_scores_pairs.append(non_zero_fi_scores_tuples)\n",
    "                features.append(non_zero_features)\n",
    "        # delete pair from W_:\n",
    "        W_ = np.delete(W_, idx_max, axis=0) \n",
    "        document_indices = np.delete(document_indices, list(idx_max))\n",
    "\n",
    "    return idx_pairs, features, fi_scores_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "# this is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# implementing a greedy algorithm here: \n",
    "#   \"In order to achieve a maximal cover for p facilities under a given service distance, \n",
    "#   the algorithm starts with an empty solution set and then adds to this set one at a \n",
    "#   time the best facility sites. The GA algorithm picks for the first facility that \n",
    "#   site which covers the most of the total population. For the second facility, GA \n",
    "#   picks the site that covers the most of the population not covered by the first \n",
    "#   facility. Then, for the third facility, GA picks the site that covers the most of the \n",
    "#   population not covered by the first and second facilities. This process is continued until either p facilities have been selected or all the population is covered. \n",
    "#   Details of the algorithm are given in Church.\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n",
    "def get_site_with_max_coverage(sites, previous_selections, W):\n",
    "    best_site = None\n",
    "    best_coverage = 0\n",
    "    for site in sites:\n",
    "        candidate = set(np.array(previous_selections).flatten()).union(site) # extend the previous selection by \"site\", this addresses \"For the second facility, GA picks the site that covers the most of the population not covered by the first \"\n",
    "        cov = coverage(candidate, W) # compute new coverage\n",
    "        if cov >= best_coverage:\n",
    "            best_coverage = cov\n",
    "            best_site = site\n",
    "    return best_site, best_coverage\n",
    "\n",
    "def get_p_tuples_with_high_coverage(indices, W, p=10):\n",
    "  sites = list(indices)\n",
    "  # \"the algorithm start with emty solution set\"\n",
    "  result = list()\n",
    "  # \"and then adds to this set one at a time the best facility sites\"\n",
    "  while True:\n",
    "      # \"The GA algorithm picks for the first facility that \n",
    "      # site which covers the most of the total population\"\n",
    "      best_site, best_coverage = get_site_with_max_coverage(sites, result, W)\n",
    "      result.append(best_site)\n",
    "      # \"This process is continued until either p facilities have been selected or all the population is covered.\"\n",
    "      if len(result) == p or best_coverage == W.shape[1]:\n",
    "          break\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two pairs, one for f(x) = machine and one for f(x) = human\n",
    "# checks texts_already_selected and chooses next best pair (for each class) if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair)\n",
    "def obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    W, features_dict = get_explanation_matrix_W(documents, explainer)\n",
    "    indices, _, _ = get_pairs(documents, W, detector, features_dict)\n",
    "    # want a dataset that is balanced irt to the two base classes:\n",
    "    # increase number of pairs returned by greedy algorithm until the p tuples include examples for both classes:\n",
    "    k = 4\n",
    "\n",
    "    pair_human = None\n",
    "    pair_machine = None\n",
    "\n",
    "    predictions= None\n",
    "    while True:\n",
    "        # obtain k pairs with high coverage\n",
    "        pairs = get_p_tuples_with_high_coverage(indices, W, p=k)\n",
    "\n",
    "        # get f(a) as one example per class is returned\n",
    "        predictions = [detector.predict_label([documents[a]])[0] for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "        # return example with highest coverage for each class\n",
    "        # if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair), the one with the next-highest coverage (for that prediction) is returned \n",
    "        for idx_pair, prediction in enumerate(predictions):\n",
    "            a,b = pairs[idx_pair]\n",
    "            # check if a or b are in texts_already_selected\n",
    "            if (documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected):\n",
    "                if prediction == 0 and pair_machine is None: # only keep first\n",
    "                    pair_machine = pairs[idx_pair] \n",
    "                if prediction == 1 and pair_human is None: # only keep first\n",
    "                    pair_human = pairs[idx_pair] \n",
    "            if pair_human is not None and pair_machine is not None:\n",
    "                return [pair_machine, pair_human] \n",
    "        k+=1 # loop until both pair_machine and pair_human not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Rule-Based Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document_a, document_b):\n",
    "    # case sensitive, on spacy tokens\n",
    "    a = list(chain(*[[token.text for token in sent] for sent in nlp(document_a).sents]))\n",
    "    b = list(chain(*[[token.text for token in sent] for sent in nlp(document_b).sents]))\n",
    "    intersection = float(len(list(set(a).intersection(b))))\n",
    "    union = float((len(set(a)) + len(set(b)))) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the order of occurence in a list of words, e.g.:\n",
    "# [\"example\", \"test\", \"example\", \"one\"] -> ['example_0', 'test_0', 'example_1', 'one_0']\n",
    "def encode_count(list_of_words):\n",
    "    d = defaultdict(lambda : 0)\n",
    "    encoded = []\n",
    "    for word in list_of_words:\n",
    "        encoded.append(word + \"_\" + str(d[word]))\n",
    "        d[word] +=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonary Anchors returns can define multiple anchors:\n",
    "# {this, is, an, example} : 0.9\n",
    "# {this, is, an}: 0.8\n",
    "# {this, is, }: 0.75\n",
    "# {this}: 0.4\n",
    "# extract all of them, only keep those with p >= 0.75 (threshold set when searching)\n",
    "def get_anchors_at_each_k(documents, explainer):\n",
    "    anchors = []\n",
    "    p = []\n",
    "    ids = []\n",
    "    for i,_ in tqdm(enumerate(documents), desc=\"Loading all explanations\"):#enumerate(documents):\n",
    "        exp = explainer.get_explanation_cached(documents[i])\n",
    "        exp[\"names\"] = encode_count(exp[\"names\"]) # Anchors is not BOW. But the algorithm is written with python set()s\n",
    "        while len(exp[\"mean\"]) >=1:#and exp[\"mean\"][-1] >= 0.75:\n",
    "            anchors.append(set(exp[\"names\"])) \n",
    "            p.append(exp[\"mean\"][-1])\n",
    "            ids.append(i)\n",
    "\n",
    "            exp[\"mean\"].pop()\n",
    "            exp[\"names\"].pop()\n",
    "    return anchors, p, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for pairs of anchors\n",
    "# returns 2 pairs of documents, one pair for f(x) = machine, one for f(x) = human, both sampled randomly\n",
    "# checks for and skips documents in \"texts_already_selected\" (i.e. it was already selected for an other explainer-detector pair)\n",
    "\n",
    "def obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "\n",
    "    anchors, p, ids = get_anchors_at_each_k(documents, explainer)\n",
    "                        # DetectGPT + Anchors is to expensive to run experiments on \n",
    "    # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "    duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "    # get the ids and p for each duplicate_anchor in  duplicate_anchors\n",
    "    # \"candidates\" is a list of lists with ids (and all other details) of each duplicate_anchor\n",
    "    candidates = [[(anchor, p, document_id) for anchor, p, document_id in zip(anchors, p, ids) if anchor == duplicate_anchor] for duplicate_anchor in duplicate_anchors ]\n",
    "    # now check for each paring of the documents in each sublist of \"candidates\":\n",
    "    #   is f(a) == f(b)?, if not: discard\n",
    "    # then pick pair with highest jaccard_score on the original documents in each \"candidate\"\n",
    "    pairs = []\n",
    "\n",
    "    predictions_cache = {}\n",
    "    def cached_predict(idx):\n",
    "        if idx not in predictions_cache:\n",
    "            predictions_cache[idx] = detector.predict_label([documents[idx]])[0]\n",
    "        return predictions_cache[idx]\n",
    "    for candidate in tqdm(candidates, desc=\"Assessing candidates\",position=1):\n",
    "        anchor_s, p, ids  = zip(*candidate)\n",
    "        c = list(combinations(ids, 2))\n",
    "        c = [(a,b) for a,b in c if cached_predict(a) == cached_predict(b)]\n",
    "        if len(c) == 0:\n",
    "            continue\n",
    "        jaccard_scores = [(a,b, jaccard_similarity(documents[a], documents[b])) for a,b in tqdm(c, desc=\"Calculating Jaccard Similarity (of documents not Anchors)\",position=0)]\n",
    "        a,b, score = max(jaccard_scores, key=lambda x: x[2])\n",
    "        pairs.append((a,b))\n",
    "\n",
    "    # sample twice: once for f(x) == human and once for f(x) == machine. f(a) == f(b) is tested earlier\n",
    "\n",
    "    predictions = [cached_predict(a) for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "    predictions_ = np.array(predictions)\n",
    "    pairs_ = np.array(pairs)\n",
    "\n",
    "    machine = pairs_[predictions_ == False]\n",
    "    human = pairs_[predictions_ == True]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    result = []\n",
    "    # select 2 pairs from pairs_: one for each class\n",
    "    # need to check if a document from the pair is in texts_already_selected\n",
    "    while True:       \n",
    "        # one explainer (DetectGPT) has no explanations for f(x) = human:\n",
    "        if not(True in predictions):\n",
    "            print(\"Warning: No examples for f(x) = human. Returning an additional example for machine\")\n",
    "            result =  machine[np.random.choice(machine.shape[0], 2, replace=False)]\n",
    "        elif not (False in predictions):\n",
    "            print(\"Warning: No examples for f(x) = machine. Returning an additional example for human\")\n",
    "            result = human[np.random.choice(human.shape[0], 2, replace=False)]\n",
    "        else:\n",
    "            result =  [machine[np.random.randint(0, machine.shape[0]),:], human[np.random.randint(0, human.shape[0]),:]] # returns a random pair for machine and a random pair for human\n",
    "\n",
    "        \n",
    "        # check for duplicates in texts_already_selected, re-sample if the pairs are duplicates.\n",
    "        if all([(documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected) for a,b in result]):\n",
    "            return result\n",
    "        else:\n",
    "            print(\"Loop: Avoiding duplicates\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        return obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)\n",
    "    else:\n",
    "        return obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test \n",
    "\n",
    "documents = list(test[\"answer\"])\n",
    "gold_labels = list(test[\"author\"] == \"human_answers\") # convention: 0: machine, 1: human, see detector.py\n",
    "document_ids = list(range(0,len(documents))) # note that the search algorithms don't use these ids. They are only used for printing and the exclude_list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Document Selection\n",
    "Some documents are excluded from the user-study for the reasons specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude_list = {\n",
    "    (195, 60,108, 228,143): \"Names forum/service explicitly\",\n",
    "    (288,117, 188, 110, 159, 97, 105, 115,266, 158): \"Author introduces themselves by name\",\n",
    "    (16,): \"References earlier post by other user\",\n",
    "    (190,294,): \"Names forum user who asked the question\",\n",
    "    (27,103,): \"NSFW\",\n",
    "    \n",
    "    \n",
    "}\n",
    "exclude_list = [x for xs in [ list(key) for key in exclude_list.keys()] for x in xs]\n",
    "exclude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply exclude_list\n",
    "documents = [d for i,d in zip(document_ids, documents) if i not in exclude_list]\n",
    "gold_labels = [gl for i,gl in zip(document_ids, gold_labels) if i not in exclude_list]\n",
    "document_ids = [i for i in document_ids if i not in exclude_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: If you plan to participate in the user study, set `OBFUSCATE_RESULTS` to `True` before proceeding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Detector\", \"Explainer\", \"Documents Phases 1+3\", \"Documents Phases 2+4\", \"f(a)\", \"f(b)\", \"GT a\", \"GT b\", \"idx a\", \"idx b\", \"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\",\"hash a\", \"hash b\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_selection(pairs, explainer, detector):\n",
    "    for a,b in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            continue\n",
    "        \n",
    "        tfidf_= tfidf.transform([documents[a], documents[b]])   \n",
    "        selection.append((detector.__class__.__name__,\n",
    "                        explainer.__class__.__name__,\n",
    "                        documents[a], documents[b],\n",
    "                        *detector.predict_label([documents[a], documents[b]]),\n",
    "                        gold_labels[a],\n",
    "                        gold_labels[b],\n",
    "                        document_ids[a],\n",
    "                        document_ids[b],\n",
    "                        nlp(documents[a]).similarity(nlp(documents[b])),\n",
    "                        jaccard_similarity(documents[a], documents[b]),\n",
    "                        (tfidf_ * tfidf_.T).toarray()[0,1],\n",
    "                        explainer.get_hash(documents[a]),\n",
    "                        explainer.get_hash(documents[b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "for detector_class in [DetectorDetectGPT,DetectorRadford,DetectorGuo]:\n",
    "    detector = detector_class()\n",
    "    display(HTML(\"<h1>{}</h1>\".format(detector.__class__.__name__)))\n",
    "    for explainer_class in [Anchor_Explainer, LIME_Explainer,SHAP_Explainer]:\n",
    "        explainer = explainer_class(detector)\n",
    "        display(HTML(\"<h2>{}</h2>\".format(explainer.__class__.__name__)))\n",
    "        \n",
    "        texts_already_selected = []\n",
    "        if len(selection) > 0:\n",
    "            texts_already_selected = list(zip(*selection))[2] + list(zip(*selection))[3]\n",
    "        pairs = obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected=texts_already_selected)\n",
    "        print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer)\n",
    "        update_selection(pairs, explainer, detector)\n",
    "        # break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(selection, columns=columns)\n",
    "if not OBFUSCATE_RESULTS:\n",
    "    display(df)\n",
    "else:\n",
    "    display(pd.DataFrame([], columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row[\"GT a\"])\n",
    "    print(row[\"idx a\"])\n",
    "    print(row[\"Documents Phases 1+3\"])\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row[\"GT b\"])\n",
    "    print(row[\"idx b\"])\n",
    "    print(row[\"Documents Phases 2+4\"])\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any(df[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].stack().reset_index(drop=\"True\").duplicated()), \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df.groupby([\"Detector\", \"Explainer\"]).count()[\"Documents Phases 1+3\"] == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OBFUSCATE_RESULTS:\n",
    "    df.to_pickle(\"./dataset_user_study.pkl\") # file in .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove from .gitignore after user study\n",
    "# TODO change format to something else after user study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "detector = DetectorRadford()\n",
    "explainer = SHAP_Explainer(detector)\n",
    "selection =[]\n",
    "\n",
    "candidates = list(range(0,len(documents)))\n",
    "random.shuffle(candidates)\n",
    "candidates = candidates[0:2*len(df)]\n",
    "assert len(candidates) == 2*len(df)\n",
    "\n",
    "while len(candidates) >= 2:\n",
    " \n",
    "    pair = (candidates[0], candidates[1])\n",
    "    \n",
    "    candidates = candidates[2:]\n",
    "    \n",
    "    #print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer)\n",
    "    update_selection([pair], explainer, detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = pd.DataFrame(selection, columns=columns)\n",
    "df_random.groupby(\"Explainer\")[[\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]].mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Explainer\")[[\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]].mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_ind\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Jaccard Similarity (a,b)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random[\"Jaccard Similarity (a,b)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(df[\"Jaccard Similarity (a,b)\"], df_random[\"Jaccard Similarity (a,b)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]:\n",
    "    print(ttest_rel(df[metric], df_random[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = documents[document_ids[27]]\n",
    "b = documents[document_ids[281]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation BOW Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim_in_W(dff):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class()\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in dff.iterrows():\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = cosine_similarity(W) \n",
    "                sim = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Cosine Similarity a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_random = get_cos_sim_in_W(df_random)\n",
    "df_fi_similarity = get_cos_sim_in_W(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity[\"Cosine Similarity a,b in W\"], df_fi_similarity_random[\"Cosine Similarity a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eucledian_distance_in_W(dff):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class()\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in dff.iterrows():\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = euclidean_distances(W) \n",
    "                sim = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Eucledian Distance a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random = get_eucledian_distance_in_W(df_random)\n",
    "df_fi_similarity_eucledian = get_eucledian_distance_in_W(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity_eucledian[\"Eucledian Distance a,b in W\"], df_fi_similarity_eucledian_random[\"Eucledian Distance a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]:\n",
    "    print(metric)\n",
    "    print(\"     \", ttest_rel(df[metric], df_random[metric]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
