{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: Always set this flag to `True` before git commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBFUSCATE_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "import lime\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<p><b>This is a {kind_of_document} document.</b></p>\n",
    "<p>The detector {correctly_or_wrongly} predicted that this document was... </p>\n",
    "<p>&emsp; ... machine generated with {p_machine} % confidence.</p>\n",
    "<p>&emsp; ... human written with {p_human} % confidence.</p> \n",
    "<div style=\"float:left;\">{highlighted_text}</div>\n",
    "\"\"\"\n",
    "#<div style=\"float:left; height:30em;\">{barplot_machine}{barplot_human}</div>\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "def print_template(document, gold_label, detector, explainer):\n",
    "    p_machine, p_human = detector.predict_proba([document])[0]\n",
    "   # machine, human = explainer.get_barplots_HTML(document)\n",
    "    display(HTML(template.format(\n",
    "    p_machine=int(p_machine*100) if not OBFUSCATE_RESULTS else \"<redacted>\", \n",
    "    p_human=int(p_human*100) if not OBFUSCATE_RESULTS else \"<redacted>\",\n",
    "  #  barplot_machine=machine,\n",
    "  #  barplot_human=human,\n",
    "    kind_of_document= ((\"machine generated\" if gold_label == False else \"human written\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    correctly_or_wrongly= ((\"correctly\" if detector.predict_label([document])[0] == gold_label else \"wrongly\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    highlighted_text=explainer.get_highlighted_text_HTML((document if not OBFUSCATE_RESULTS else \"<redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted>\")),\n",
    "    )))\n",
    "def print_shared_features(features, fi_scores):\n",
    "    for feature, (fi_score_a, fi_score_b) in zip(features, fi_scores):\n",
    "        print(feature)\n",
    "        print(\"\\t\\ta: {} \\t b: {}\".format(fi_score_a, fi_score_b))\n",
    "        print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer, skip_n=0):\n",
    "\n",
    "    for (a,b)in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            print(\"idx_a: <redacted> idx_b <redacted>\")\n",
    "        else:\n",
    "            print(\"idx_a: {} idx_b {}\".format(document_ids[a],document_ids[b]))\n",
    "\n",
    "        print_template(documents[a], gold_labels[a], detector, explainer)\n",
    "        print_template(documents[b], gold_labels[b], detector, explainer)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Feature Importance Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix of explanations for all documents in \"data\"\n",
    "# This function was once SubmodularPick.__init__() in LIME. It was planned to use its output for a search strategy for similar explanations. \n",
    "# Only the code for creating W from the paper (rows are explanations, cols are BOW features) remains\n",
    "def get_explanation_matrix_W(data, explainer, quiet=False):\n",
    "    # Get (cached) explanations \n",
    "    explanations_and_documents = [(d, explainer.get_fi_scores(d, fill=True)[0]) for d in tqdm(data, desc=\"Loading all explanations\",disable=quiet) ] # [0]: only irt to label machine, fill: return all words, even those with 0 fi\n",
    "\n",
    "    get_feature_name_signed = lambda feature,value : feature + (\"_+\" if value >=0 else \"_-\") # appends \"_+\" or \"_-\" to each feature name, e.g. \"example\" -> \"example_+\" if fi(example) > 0\n",
    "    # Ribeiro et al.: Find all the explanation model features used. Defines the dimension d'\n",
    "    # i.e. determine columns of W: each word (BOW) gets (up to) two columns, one for positive FI scores, one for negative FI scores\n",
    "    features_dict = {}\n",
    "    feature_iter = 0\n",
    "    for d, exp in tqdm(explanations_and_documents, desc=\"Building global dict of features\", disable=quiet):\n",
    "     #   print(\"exp\",exp)\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            feature_name = get_feature_name_signed(feature,value) # get_feature_name_signed: see above\n",
    "            if feature_name not in features_dict.keys():\n",
    "                features_dict[feature_name] = (feature_iter)\n",
    "                feature_iter += 1\n",
    "    d_prime = len(features_dict.keys())\n",
    "\n",
    "    # Ribeiro et al.: Create the n x d' dimensional 'explanation matrix', W\n",
    "    W = np.zeros((len(explanations_and_documents), d_prime))\n",
    "\n",
    "    # fill W, look up cols in dict that was just created\n",
    "    # W: one row per explanation, one col per feature in feature_dict\n",
    "    for i, (d, exp) in enumerate(tqdm(explanations_and_documents,  desc=\"Building W\",disable=quiet)):\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            # get_feature_name_signed: see above\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            W[i, features_dict[get_feature_name_signed(feature,value)]] += value\n",
    "    return W, features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples: (pair of documents whose explanations are similar, the features that overlap, fi scores of said features)\n",
    "# this maximizes similarity between documents (greedy, each document is only part of one tuple)\n",
    "# another function should select n tuples to maximize coverage in explanation space akin to SP-LIME\n",
    "sum_two_max = None\n",
    "def get_pairs(documents, W, detector, features_dict, n_pairs=None):\n",
    "    if n_pairs is None:\n",
    "        n_pairs = len(documents)//2\n",
    "    idx_pairs = [] # tuples of indices of similar documents a,b in \"data\"\n",
    "    features = [] # list of features those documents covered\n",
    "    fi_scores_pairs = []\n",
    "\n",
    "    W_ = np.copy(W)\n",
    "\n",
    "    document_indices = np.arange(0, W_.shape[0])\n",
    "   # print(document_indices.shape, W_.shape)\n",
    "    for _ in tqdm(range(0,n_pairs), desc=\"Obtaining pairs\"):\n",
    "        sim = cosine_similarity(W_) # calculate cosine similarity between all explanations\n",
    "        sim = np.triu(sim,k=1)  # remove redundant information for argmax()\n",
    "\n",
    "        idx_max = np.unravel_index(sim.argmax(), sim.shape) # get most similar pair, result is (idx_a, idx_b)\n",
    "       # print(idx_max)\n",
    "        features_non_zero_in_both = np.intersect1d(W_[idx_max[0]].nonzero(),W_[idx_max[1]].nonzero()) # get features that have non-zero fi in both explanations\n",
    "                                                                                                      # is used later for selecting a set of tuples with high coverage (as in SP-LIME)\n",
    "        non_zero_features = [] # list with features that will be returned\n",
    "        non_zero_fi_scores_tuples = [] # list of tuples with fi scores in a and b that will be returned\n",
    "    \n",
    "        # look up feature_idxs in features_dict and append them to the output\n",
    "        for iii in features_non_zero_in_both:\n",
    "           key = list(features_dict.keys())[list(features_dict.values()).index(iii)]\n",
    "           non_zero_features.append(key)\n",
    "           non_zero_fi_scores_tuples.append((W_[idx_max[0],features_dict[key]], W_[idx_max[1],features_dict[key]]))\n",
    "        \n",
    "        # Only add pair to output list if valid: at least one common feature is not zero AND f(a) == f(b) (i.e., the explanation is arguing for the same detector verdict)\n",
    "        if len(non_zero_features) > 0:\n",
    "            a,b = detector.predict_label([documents[document_indices[idx_max[0]]], documents[document_indices[idx_max[1]]]])\n",
    "            if a == b:\n",
    "                idx_pairs.append(document_indices[list(idx_max)])\n",
    "                fi_scores_pairs.append(non_zero_fi_scores_tuples)\n",
    "                features.append(non_zero_features)\n",
    "        # delete pair from W_:\n",
    "        W_ = np.delete(W_, idx_max, axis=0) \n",
    "        document_indices = np.delete(document_indices, list(idx_max))\n",
    "\n",
    "    return idx_pairs, features, fi_scores_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "\n",
    "# this is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# can brute force here as only the 10 top pairs by similarity are used for each class, \n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two pairs, one for f(x) = machine and one for f(x) = human\n",
    "# checks texts_already_selected and chooses next best pair (for each class) if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair)\n",
    "def obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    W, features_dict = get_explanation_matrix_W(documents, explainer)\n",
    "    similar_pairs, _, _ = get_pairs(documents, W, detector, features_dict)\n",
    "    # want a dataset that is balanced irt to the two base classes:\n",
    "    # two pairs will be returned, one with f(x) == machine, and one with f(x) == human\n",
    "    top_10_pairs_human = []\n",
    "    top_10_pairs_machine = []\n",
    "    for pair in similar_pairs:\n",
    "        if detector.predict_label([documents[pair[0]]])[0]:\n",
    "            top_10_pairs_human.append(pair)\n",
    "        else:\n",
    "            top_10_pairs_machine.append(pair)\n",
    "        if len(top_10_pairs_human) >= 10 and len(top_10_pairs_machine) >= 10:\n",
    "            top_10_pairs_human = top_10_pairs_human[0:10]\n",
    "            top_10_pairs_machine = top_10_pairs_machine[0:10]\n",
    "            break\n",
    "\n",
    "            \n",
    "    combinations_ = ((a,b) for a in top_10_pairs_machine for b in top_10_pairs_human)\n",
    "    # return two pairs maximizing coverage, one with f(x) == machine, and one with f(x) == human \n",
    "    coverage_ = [((a,b),coverage([a,b],W)) for a,b in combinations_]\n",
    "    pair_a = None\n",
    "    pair_b = None\n",
    "    while True:\n",
    "        (pair_a, pair_b), c = max(coverage_, key=lambda item : item[1])\n",
    "        result = [pair_a, pair_b]\n",
    "        if all([(documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected) for a,b in result]):\n",
    "            return [pair_a, pair_b]\n",
    "        print(\"Duplicate loop\")\n",
    "        coverage_.remove(((pair_a, pair_b), c))\n",
    "\n",
    "    \n",
    "        # break\n",
    "        # # get f(a) as one example per class is returned\n",
    "        # predictions = [detector.predict_label([documents[a]])[0] for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "        # # return example with highest coverage for each class\n",
    "        # # if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair), the one with the next-highest coverage (for that prediction) is returned \n",
    "        # for idx_pair, prediction in enumerate(predictions):\n",
    "        #     a,b = pairs[idx_pair]\n",
    "        #     # check if a or b are in texts_already_selected\n",
    "        #     if (documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected):\n",
    "        #         if prediction == 0 and pair_machine is None: # only keep first\n",
    "        #             pair_machine = pairs[idx_pair] \n",
    "        #         if prediction == 1 and pair_human is None: # only keep first\n",
    "        #             pair_human = pairs[idx_pair] \n",
    "        #     if pair_human is not None and pair_machine is not None:\n",
    "        #         return [pair_machine, pair_human] \n",
    "        # k+=1 # loop until both pair_machine and pair_human not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Rule-Based Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document_a, document_b):\n",
    "    # case sensitive, on spacy tokens\n",
    "    a = list(chain(*[[token.text for token in sent] for sent in nlp(document_a).sents]))\n",
    "    b = list(chain(*[[token.text for token in sent] for sent in nlp(document_b).sents]))\n",
    "    intersection = float(len(list(set(a).intersection(b))))\n",
    "    union = float((len(set(a)) + len(set(b)))) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the order of occurence in a list of words, e.g.:\n",
    "# [\"example\", \"test\", \"example\", \"one\"] -> ['example_0', 'test_0', 'example_1', 'one_0']\n",
    "def encode_count(list_of_words):\n",
    "    d = defaultdict(lambda : 0)\n",
    "    encoded = []\n",
    "    for word in list_of_words:\n",
    "        encoded.append(word + \"_\" + str(d[word]))\n",
    "        d[word] +=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonary Anchors returns can define multiple anchors:\n",
    "# {this, is, an, example} : 0.9\n",
    "# {this, is, an}: 0.8\n",
    "# {this, is, }: 0.75\n",
    "# {this}: 0.4\n",
    "# extract all of them, only keep those with p >= 0.75 (threshold set when searching)\n",
    "def get_anchors_at_each_k(documents, explainer):\n",
    "    anchors = []\n",
    "    p = []\n",
    "    ids = []\n",
    "    for i,_ in tqdm(enumerate(documents), desc=\"Loading all explanations\"):#enumerate(documents):\n",
    "        exp = explainer.get_explanation_cached(documents[i])\n",
    "        exp[\"names\"] = encode_count(exp[\"names\"]) # Anchors is not BOW. But the algorithm is written with python set()s\n",
    "        while len(exp[\"mean\"]) >=1:#and exp[\"mean\"][-1] >= 0.75:\n",
    "            anchors.append(set(exp[\"names\"])) \n",
    "            p.append(exp[\"mean\"][-1])\n",
    "            ids.append(i)\n",
    "\n",
    "            exp[\"mean\"].pop()\n",
    "            exp[\"names\"].pop()\n",
    "    return anchors, p, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for pairs of anchors\n",
    "# returns 2 pairs of documents, one pair for f(x) = machine, one for f(x) = human, both sampled randomly\n",
    "# checks for and skips documents in \"texts_already_selected\" (i.e. it was already selected for an other explainer-detector pair)\n",
    "\n",
    "def obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "\n",
    "    anchors, p, ids = get_anchors_at_each_k(documents, explainer)\n",
    "                        # DetectGPT + Anchors is to expensive to run experiments on \n",
    "    # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "    duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "    # get the ids and p for each duplicate_anchor in  duplicate_anchors\n",
    "    # \"candidates\" is a list of lists with ids (and all other details) of each duplicate_anchor\n",
    "    candidates = [[(anchor, p, document_id) for anchor, p, document_id in zip(anchors, p, ids) if anchor == duplicate_anchor] for duplicate_anchor in duplicate_anchors ]\n",
    "    # now check for each paring of the documents in each sublist of \"candidates\":\n",
    "    #   is f(a) == f(b)?, if not: discard\n",
    "    # then pick pair with highest jaccard_score on the original documents in each \"candidate\"\n",
    "    pairs = []\n",
    "\n",
    "    predictions_cache = {}\n",
    "    def cached_predict(idx):\n",
    "        if idx not in predictions_cache:\n",
    "            predictions_cache[idx] = detector.predict_label([documents[idx]])[0]\n",
    "        return predictions_cache[idx]\n",
    "    for candidate in tqdm(candidates, desc=\"Assessing candidates\",position=1):\n",
    "        anchor_s, p, ids  = zip(*candidate)\n",
    "        c = list(combinations(ids, 2))\n",
    "        c = [(a,b) for a,b in c if cached_predict(a) == cached_predict(b)]\n",
    "        if len(c) == 0:\n",
    "            continue\n",
    "        jaccard_scores = [(a,b, jaccard_similarity(documents[a], documents[b])) for a,b in tqdm(c, desc=\"Calculating Jaccard Similarity (of documents not Anchors)\",position=0)]\n",
    "        a,b, score = max(jaccard_scores, key=lambda x: x[2])\n",
    "        pairs.append((a,b))\n",
    "\n",
    "    # sample twice: once for f(x) == human and once for f(x) == machine. f(a) == f(b) is tested earlier\n",
    "\n",
    "    predictions = [cached_predict(a) for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "    predictions_ = np.array(predictions)\n",
    "    pairs_ = np.array(pairs)\n",
    "\n",
    "    machine = pairs_[predictions_ == False]\n",
    "    human = pairs_[predictions_ == True]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    result = []\n",
    "    # select 2 pairs from pairs_: one for each class\n",
    "    # need to check if a document from the pair is in texts_already_selected\n",
    "    while True:       \n",
    "        # one explainer (DetectGPT) has no explanations for f(x) = human:\n",
    "        if not(True in predictions):\n",
    "            print(\"Warning: No examples for f(x) = human. Returning an additional example for machine\")\n",
    "            result =  machine[np.random.choice(machine.shape[0], 2, replace=False)]\n",
    "        elif not (False in predictions):\n",
    "            print(\"Warning: No examples for f(x) = machine. Returning an additional example for human\")\n",
    "            result = human[np.random.choice(human.shape[0], 2, replace=False)]\n",
    "        else:\n",
    "            result =  [machine[np.random.randint(0, machine.shape[0]),:], human[np.random.randint(0, human.shape[0]),:]] # returns a random pair for machine and a random pair for human\n",
    "\n",
    "        \n",
    "        # check for duplicates in texts_already_selected, re-sample if the pairs are duplicates.\n",
    "        if all([(documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected) for a,b in result]):\n",
    "            return result\n",
    "        else:\n",
    "            print(\"Loop: Avoiding duplicates\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        return obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)\n",
    "    else:\n",
    "        return obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test \n",
    "\n",
    "documents = list(test[\"answer\"])\n",
    "gold_labels = list(test[\"author\"] == \"human_answers\") # convention: 0: machine, 1: human, see detector.py\n",
    "document_ids = list(range(0,len(documents))) # note that the search algorithms don't use these ids. They are only used for printing and the exclude_list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Document Selection\n",
    "Some documents are excluded from the user-study for the reasons specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195,\n",
       " 60,\n",
       " 108,\n",
       " 228,\n",
       " 143,\n",
       " 288,\n",
       " 117,\n",
       " 188,\n",
       " 110,\n",
       " 159,\n",
       " 97,\n",
       " 105,\n",
       " 115,\n",
       " 266,\n",
       " 158,\n",
       " 16,\n",
       " 190,\n",
       " 294,\n",
       " 27,\n",
       " 103]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exclude_list = {\n",
    "    (195, 60,108, 228,143): \"Names forum/service explicitly\",\n",
    "    (288,117, 188, 110, 159, 97, 105, 115,266, 158): \"Author introduces themselves by name\",\n",
    "    (16,): \"References earlier post by other user\",\n",
    "    (190,294,): \"Names forum user who asked the question\",\n",
    "    (27,103,): \"NSFW\",\n",
    "    \n",
    "    \n",
    "}\n",
    "exclude_list = [x for xs in [ list(key) for key in exclude_list.keys()] for x in xs]\n",
    "exclude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply exclude_list\n",
    "documents = [d for i,d in zip(document_ids, documents) if i not in exclude_list]\n",
    "gold_labels = [gl for i,gl in zip(document_ids, gold_labels) if i not in exclude_list]\n",
    "document_ids = [i for i in document_ids if i not in exclude_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: If you plan to participate in the user study, set `OBFUSCATE_RESULTS` to `True` before proceeding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Detector\", \"Explainer\", \"Documents Phases 1+3\", \"Documents Phases 2+4\", \"f(a)\", \"f(b)\", \"GT a\", \"GT b\", \"idx a\", \"idx b\", \"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\",\"hash a\", \"hash b\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_selection(selection, pairs, explainer, detector):\n",
    "    for a,b in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            continue\n",
    "        \n",
    "        tfidf_= tfidf.transform([documents[a], documents[b]])   \n",
    "        selection.append((detector.__class__.__name__,\n",
    "                        explainer.__class__.__name__,\n",
    "                        documents[a], documents[b],\n",
    "                        *detector.predict_label([documents[a], documents[b]]),\n",
    "                        gold_labels[a],\n",
    "                        gold_labels[b],\n",
    "                        document_ids[a],\n",
    "                        document_ids[b],\n",
    "                        nlp(documents[a]).similarity(nlp(documents[b])),\n",
    "                        jaccard_similarity(documents[a], documents[b]),\n",
    "                        (tfidf_ * tfidf_.T).toarray()[0,1],\n",
    "                        explainer.get_hash(documents[a]),\n",
    "                        explainer.get_hash(documents[b])))\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>DetectorRadford</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>LIME_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 285/285 [00:00<00:00, 807.37it/s]\n",
      "Building global dict of features: 100%|██████████| 285/285 [00:01<00:00, 152.18it/s]\n",
      "Building W: 100%|██████████| 285/285 [00:01<00:00, 150.07it/s]\n",
      "Obtaining pairs: 100%|██████████| 142/142 [00:08<00:00, 16.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>SHAP_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 285/285 [00:00<00:00, 3238.67it/s]\n",
      "Building global dict of features: 100%|██████████| 285/285 [00:19<00:00, 14.79it/s]\n",
      "Building W: 100%|██████████| 285/285 [00:20<00:00, 13.89it/s]\n",
      "Obtaining pairs: 100%|██████████| 142/142 [00:08<00:00, 16.07it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>DetectorGuo</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>LIME_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 285/285 [00:00<00:00, 960.99it/s]\n",
      "Building global dict of features: 100%|██████████| 285/285 [00:01<00:00, 153.00it/s]\n",
      "Building W: 100%|██████████| 285/285 [00:01<00:00, 155.68it/s]\n",
      "Obtaining pairs: 100%|██████████| 142/142 [00:07<00:00, 18.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>SHAP_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 285/285 [00:00<00:00, 3064.49it/s]\n",
      "Building global dict of features: 100%|██████████| 285/285 [00:19<00:00, 14.32it/s]\n",
      "Building W: 100%|██████████| 285/285 [00:18<00:00, 15.05it/s]\n",
      "Obtaining pairs: 100%|██████████| 142/142 [00:08<00:00, 17.34it/s]\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "for detector_class in [DetectorRadford,DetectorGuo]:\n",
    "    detector = detector_class()\n",
    "    display(HTML(\"<h1>{}</h1>\".format(detector.__class__.__name__)))\n",
    "    for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "        explainer = explainer_class(detector)\n",
    "        display(HTML(\"<h2>{}</h2>\".format(explainer.__class__.__name__)))\n",
    "        \n",
    "        texts_already_selected = []\n",
    "        if len(selection) > 0:\n",
    "            texts_already_selected = list(zip(*selection))[2] + list(zip(*selection))[3]\n",
    "        pairs = obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected=texts_already_selected)\n",
    "       # print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer)\n",
    "        selection = update_selection(selection, pairs, explainer, detector)\n",
    "        # break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(selection, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not OBFUSCATE_RESULTS:\n",
    "#     df.to_pickle(\"./dataset_user_study.pkl\") # file in .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(\"./dataset_user_study.pkl\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in df.iterrows():\n",
    "#     print(row[\"GT a\"])\n",
    "#     print(row[\"idx a\"])\n",
    "#     print(row[\"Documents Phases 1+3\"])\n",
    "#     print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in df.iterrows():\n",
    "#     print(row[\"GT b\"])\n",
    "#     print(row[\"idx b\"])\n",
    "#     print(row[\"Documents Phases 2+4\"])\n",
    "#     print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any(df[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].stack().reset_index(drop=\"True\").duplicated()), \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df.groupby([\"Detector\", \"Explainer\"]).count()[\"Documents Phases 1+3\"] == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove from .gitignore after user study\n",
    "# TODO change format to something else after user study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cache = {}\n",
    "def prediction_cached(detector, document):\n",
    "    id = (detector.__class__.__name__,document)\n",
    "    if id not in prediction_cache:\n",
    "        prediction_cache[id] = detector.predict_label([document])[0]\n",
    "    return prediction_cache[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.08s)\n",
      "DONE (0.10s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detector_detectgpt = DetectorDetectGPT()\n",
    "detector_radford = DetectorRadford()\n",
    "detector_guo = DetectorGuo()\n",
    "def get_random_df(df, seed=42):\n",
    "    selection = []\n",
    "    random.seed(seed)\n",
    "    for idx, row in tqdm(list(df.iterrows())):\n",
    "        detector = None\n",
    "        explainer = None\n",
    "        if row[\"Detector\"] == \"DetectorDetectGPT\":\n",
    "            detector = detector_detectgpt\n",
    "        if row[\"Detector\"] == \"DetectorRadford\":\n",
    "            detector = detector_radford\n",
    "        if row[\"Detector\"] == \"DetectorGuo\":\n",
    "            detector = detector_guo\n",
    "\n",
    "        if row[\"Explainer\"]  == \"Anchor_Explainer\":\n",
    "            explainer = Anchor_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"LIME_Explainer\":\n",
    "            explainer = LIME_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"SHAP_Explainer\":\n",
    "            explainer = SHAP_Explainer(detector)\n",
    "        #               all documents not in exclude_list                                                                            without replacement\n",
    "        candidates = [i for i in range(0,len(documents)) if i not in exclude_list and (len(selection) == 0 or document_ids[i] not in list(zip(*selection))[8] + list(zip(*selection))[9])]\n",
    "        random.shuffle(candidates)\n",
    "        pairs = None\n",
    "        while True:\n",
    "            pairs = [(candidates[0], candidates[1])]\n",
    "            if prediction_cached(detector, documents[candidates[0]]) == row[\"f(a)\"] and prediction_cached(detector, documents[candidates[0]]) == prediction_cached(detector, documents[candidates[1]]):\n",
    "                break\n",
    "            candidates = candidates[2:]\n",
    "        \n",
    "\n",
    "        selection = update_selection(selection, pairs, explainer, detector)\n",
    "    return pd.DataFrame(selection, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs_random = [get_random_df(df, seed=i) for i in range(0,3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          Detector       Explainer  \\\n",
       " 0  DetectorRadford  LIME_Explainer   \n",
       " 1  DetectorRadford  LIME_Explainer   \n",
       " 2  DetectorRadford  SHAP_Explainer   \n",
       " 3  DetectorRadford  SHAP_Explainer   \n",
       " 4      DetectorGuo  LIME_Explainer   \n",
       " 5      DetectorGuo  LIME_Explainer   \n",
       " 6      DetectorGuo  SHAP_Explainer   \n",
       " 7      DetectorGuo  SHAP_Explainer   \n",
       " \n",
       "                                 Documents Phases 1+3  \\\n",
       " 0  Banks generally do not intentionally allow tra...   \n",
       " 1  The Department of Computer Science and Technol...   \n",
       " 2  If your deductions are higher than your income...   \n",
       " 3  First of all you do not \"co-sign a car\". I ass...   \n",
       " 4  The sale of agricultural land may be subject t...   \n",
       " 5  Peter A. Wegner (August 20, 1932 – July 27, 20...   \n",
       " 6  It is possible that your son is experiencing a...   \n",
       " 7  You'll likely see several more scary market ev...   \n",
       " \n",
       "                                 Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       " 0  Trailing 12-month total returns, or TTM return...     0     0  False   \n",
       " 1  Investing is not the same as illegal drugs.  O...     1     1   True   \n",
       " 2  If you are experiencing abdominal pain and blo...     0     0  False   \n",
       " 3  The S&P 500 is a stock market index that consi...     1     1   True   \n",
       " 4  It is possible that the pain and other symptom...     0     0  False   \n",
       " 5  Shane Legg CBE is a machine learning research ...     1     1   True   \n",
       " 6  It's important to understand that as the prima...     0     0  False   \n",
       " 7  they apply it to my next payment That's what m...     1     1   True   \n",
       " \n",
       "     GT b  idx a  idx b  \\\n",
       " 0  False    150    130   \n",
       " 1   True     65    259   \n",
       " 2  False     77    177   \n",
       " 3  False    279     42   \n",
       " 4  False    184    296   \n",
       " 5   True    290     14   \n",
       " 6  False    170    209   \n",
       " 7   True    173     50   \n",
       " \n",
       "    Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       " 0                                           0.898404                            \n",
       " 1                                           0.795111                            \n",
       " 2                                           0.945031                            \n",
       " 3                                           0.776085                            \n",
       " 4                                           0.880504                            \n",
       " 5                                           0.934458                            \n",
       " 6                                           0.952442                            \n",
       " 7                                           0.908464                            \n",
       " \n",
       "    Jaccard Similarity (a,b)  Cosine Similarity tfidf Vectors  \\\n",
       " 0                  0.134228                         0.091690   \n",
       " 1                  0.054688                         0.036678   \n",
       " 2                  0.191781                         0.134893   \n",
       " 3                  0.074766                         0.063316   \n",
       " 4                  0.155738                         0.075695   \n",
       " 5                  0.115108                         0.066670   \n",
       " 6                  0.183099                         0.142818   \n",
       " 7                  0.144000                         0.090759   \n",
       " \n",
       "                                               hash a  \\\n",
       " 0  4f5f2665ced7f83dce8b05529784287dfcccb1f87eae10...   \n",
       " 1  774e49fc1258125bc75fbb86be17caa15e26490312398e...   \n",
       " 2  4a28a8f0d8c6fbee4a7aad31697d315f2e478277646094...   \n",
       " 3  b86b448f6ea0ae0a8b732af24747141d278749a200de40...   \n",
       " 4  696a852f59f1d09d3570692f5324c14edc03cc772d0670...   \n",
       " 5  99dc8f913352180d170d3321c3dcc1e8dfaae3745c5102...   \n",
       " 6  53bb29915fb86c3bd323ddf3342af288e3110b3d8c470c...   \n",
       " 7  7bd59b306ee4f86d66ea0c02c7c6ad2f0c42df557f3731...   \n",
       " \n",
       "                                               hash b  \n",
       " 0  e398f19e5fbb9d1db41882037d2b1f98d0bf09b76e0e01...  \n",
       " 1  9cdf429ec5aad74552cc521f4f38cc25c3dc0a3d8ef9c5...  \n",
       " 2  642deaeddd2290674c7e87199e3d35b5a1720219827bef...  \n",
       " 3  dfc6927f5922f84107546353a48ae801cbe3e9b60a719f...  \n",
       " 4  b3ec68c74de53fc0c6fe8729beb8ae1840ff888e169d55...  \n",
       " 5  de00c4691105a7252f08b1cb03d8b83fd565cc9db08ace...  \n",
       " 6  03a0219be4bcd67fcb5b3b7148dedd456635d4b891af33...  \n",
       " 7  f3372df44a7f627dbf381c0e0732206e97b01b859f5172...  ,\n",
       "           Detector       Explainer  \\\n",
       " 0  DetectorRadford  LIME_Explainer   \n",
       " 1  DetectorRadford  LIME_Explainer   \n",
       " 2  DetectorRadford  SHAP_Explainer   \n",
       " 3  DetectorRadford  SHAP_Explainer   \n",
       " 4      DetectorGuo  LIME_Explainer   \n",
       " 5      DetectorGuo  LIME_Explainer   \n",
       " 6      DetectorGuo  SHAP_Explainer   \n",
       " 7      DetectorGuo  SHAP_Explainer   \n",
       " \n",
       "                                 Documents Phases 1+3  \\\n",
       " 0  Subchapter S Corporations are a special type o...   \n",
       " 1  Pain in back (spine) in presence of bladder ca...   \n",
       " 2  There are often promotions and bonuses offered...   \n",
       " 3  Monitoring all three is good practice. That wa...   \n",
       " 4  It is generally not possible for an adult to b...   \n",
       " 5  You only have to hold the shares at the openin...   \n",
       " 6  It is not appropriate for me to recommend spec...   \n",
       " 7  Insider trading is when you buy or sell an inv...   \n",
       " \n",
       "                                 Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       " 0  Yes, Volvo is a public company. It is listed o...     0     0   True   \n",
       " 1  It is important to seek medical attention for ...     1     1   True   \n",
       " 2  Treatment for a spiral fracture of the ring fi...     0     0  False   \n",
       " 3  Hi,     Thanks for asking.iI understand your c...     1     1   True   \n",
       " 4  Egocentric vision or first-person vision is a ...     0     0  False   \n",
       " 5  The CBOE states, in an investor's guide to Int...     1     1   True   \n",
       " 6  Peter Wegner is a computer scientist and profe...     0     0  False   \n",
       " 7  You can receive all the Money in your Bank. By...     1     1   True   \n",
       " \n",
       "     GT b  idx a  idx b  \\\n",
       " 0  False    178     15   \n",
       " 1  False     18    122   \n",
       " 2  False    183    252   \n",
       " 3   True    218     83   \n",
       " 4   True     59     88   \n",
       " 5   True    109    181   \n",
       " 6  False     81    153   \n",
       " 7   True     78    100   \n",
       " \n",
       "    Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       " 0                                           0.883848                            \n",
       " 1                                           0.931114                            \n",
       " 2                                           0.964032                            \n",
       " 3                                           0.841697                            \n",
       " 4                                           0.921507                            \n",
       " 5                                           0.688017                            \n",
       " 6                                           0.799386                            \n",
       " 7                                           0.948850                            \n",
       " \n",
       "    Jaccard Similarity (a,b)  Cosine Similarity tfidf Vectors  \\\n",
       " 0                  0.093220                         0.041715   \n",
       " 1                  0.087838                         0.094713   \n",
       " 2                  0.150000                         0.129325   \n",
       " 3                  0.055556                         0.021198   \n",
       " 4                  0.099338                         0.037878   \n",
       " 5                  0.054545                         0.038886   \n",
       " 6                  0.109244                         0.086443   \n",
       " 7                  0.116279                         0.082267   \n",
       " \n",
       "                                               hash a  \\\n",
       " 0  21f73b34c6c07b710312c2799e81f257fe4e8db76be9ae...   \n",
       " 1  b6093f1c278b941337792d607c7b170f1dbb71f2e718d3...   \n",
       " 2  bbd23fc1b225444ab4f9f37f7684f7174a36889a1d6b71...   \n",
       " 3  65efd036bb4ad704d5309f97b6c4655f418105c910c4f9...   \n",
       " 4  bf5cc4428ba8a20b7f40625ce7893a43aa9b3652f73d1c...   \n",
       " 5  d2acb3efcbd10c6b0656218ca86c3820e9c746e04749e4...   \n",
       " 6  8cd7237c9d90b8c46bfae0755792dad7c387f352c70f1d...   \n",
       " 7  55b3d2de9c67ba5abebb810ca26c7e0176ee6511cb0cdb...   \n",
       " \n",
       "                                               hash b  \n",
       " 0  6bec45b04093791ca905d5a2b6a7fa6a905a64f965b546...  \n",
       " 1  08a7b4e29d115abac42856432e00f2d1948de7c9c99a72...  \n",
       " 2  3838aecf67942145b93b96e42021f369c969c3ec9b719c...  \n",
       " 3  30105cb1b8df3ae073e13b0e6fa3e7fb0590744a9f6796...  \n",
       " 4  016246bfffb3278dceda1274873d0ef9431a15c37b9fe7...  \n",
       " 5  55e2989bbf88651a51be21f1efae5e0c939c6238f049b5...  \n",
       " 6  1ef829cbe6540a96630abe7a67b0d0397a44b2321cbdb9...  \n",
       " 7  24d5ad95c1e2717ba84af4c43ce69d1a5f8e291f22a73f...  ,\n",
       "           Detector       Explainer  \\\n",
       " 0  DetectorRadford  LIME_Explainer   \n",
       " 1  DetectorRadford  LIME_Explainer   \n",
       " 2  DetectorRadford  SHAP_Explainer   \n",
       " 3  DetectorRadford  SHAP_Explainer   \n",
       " 4      DetectorGuo  LIME_Explainer   \n",
       " 5      DetectorGuo  LIME_Explainer   \n",
       " 6      DetectorGuo  SHAP_Explainer   \n",
       " 7      DetectorGuo  SHAP_Explainer   \n",
       " \n",
       "                                 Documents Phases 1+3  \\\n",
       " 0  Motion estimation is the process of estimating...   \n",
       " 1  Swelling after dental surgery is normal and ca...   \n",
       " 2  Yes, it is possible for a food allergy to caus...   \n",
       " 3  Is that indicator can only be used for short-t...   \n",
       " 4  It is possible that the pain and other symptom...   \n",
       " 5  yes u can. But though EF is moderate but u hav...   \n",
       " 6  Option contracts are generally not subject to ...   \n",
       " 7  Welcome at HCM    I have gone through your que...   \n",
       " \n",
       "                                 Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       " 0  There are a number of reasons why changing the...     0     0  False   \n",
       " 1  Shane Legg CBE is a machine learning research ...     1     1  False   \n",
       " 2  Social intelligence is the ability to understa...     0     0  False   \n",
       " 3  I'm a bit out of my element here, but my guess...     1     1   True   \n",
       " 4  During earnings reports or other scheduled eve...     0     0  False   \n",
       " 5  There are quite a few things here; Edit: If yo...     1     1   True   \n",
       " 6  It is important to get the lump on your grands...     0     0  False   \n",
       " 7  As your is a very specific case, please get an...     1     1   True   \n",
       " \n",
       "     GT b  idx a  idx b  \\\n",
       " 0  False    135     79   \n",
       " 1   True    152     14   \n",
       " 2  False     36    149   \n",
       " 3   True    225    124   \n",
       " 4  False    296    134   \n",
       " 5   True    179    137   \n",
       " 6  False    299    216   \n",
       " 7   True    280     82   \n",
       " \n",
       "    Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       " 0                                           0.963377                            \n",
       " 1                                           0.778099                            \n",
       " 2                                           0.943467                            \n",
       " 3                                           0.940925                            \n",
       " 4                                           0.900370                            \n",
       " 5                                           0.885625                            \n",
       " 6                                           0.925271                            \n",
       " 7                                           0.885688                            \n",
       " \n",
       "    Jaccard Similarity (a,b)  Cosine Similarity tfidf Vectors  \\\n",
       " 0                  0.166667                         0.151216   \n",
       " 1                  0.086420                         0.044864   \n",
       " 2                  0.140127                         0.099643   \n",
       " 3                  0.163265                         0.093020   \n",
       " 4                  0.118110                         0.084177   \n",
       " 5                  0.070796                         0.027896   \n",
       " 6                  0.108527                         0.077184   \n",
       " 7                  0.121019                         0.103524   \n",
       " \n",
       "                                               hash a  \\\n",
       " 0  4059437527202c28d6a6348b8f26545521872057e01148...   \n",
       " 1  66634da8b14ce1300defec76b82efaf85e07f0dcf555d1...   \n",
       " 2  5c5d94bdf780139b641b906600687b933abe4983bd2d5b...   \n",
       " 3  f36eea089d93178672b8dfd1ed059927a0018c71f337f3...   \n",
       " 4  b3ec68c74de53fc0c6fe8729beb8ae1840ff888e169d55...   \n",
       " 5  2500c0feb00f1f5e3696f3a171eedc7cee9eb2918a3b6e...   \n",
       " 6  5b452735e59499d5c471fefba9bbcc41bc42763a5d8553...   \n",
       " 7  ac714f35cb2e120b49e0ada6c58324044e7ade7ac733ba...   \n",
       " \n",
       "                                               hash b  \n",
       " 0  0ff35e39d50e3db24b3733674f84e9d8a6b836757b5d69...  \n",
       " 1  de00c4691105a7252f08b1cb03d8b83fd565cc9db08ace...  \n",
       " 2  b0e1911a0706b07b4d128f56bcd2fac5f517d7f9e4c4a4...  \n",
       " 3  283ea38cf17b09bb85b98776bcc506d1e964faa3d9b89c...  \n",
       " 4  dfcc98c581f4caab2398b9d443a8070c15dd3212f00dbb...  \n",
       " 5  524545933f9c0c2781b2f92a02fbc15869119024d25483...  \n",
       " 6  739372dc4bf92366bc991aab94a8f80ee06f90e30c51cd...  \n",
       " 7  6b25114906789fc86fa719f0394277f9289f080cbbbc7e...  ]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Jaccard Similarity (a,b)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Cosine Similarity tfidf Vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.042344</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>0.036806</td>\n",
       "      <td>0.094874</td>\n",
       "      <td>0.054820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834775</td>\n",
       "      <td>0.083865</td>\n",
       "      <td>0.076315</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>0.031411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950843</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.160636</td>\n",
       "      <td>0.027420</td>\n",
       "      <td>0.121287</td>\n",
       "      <td>0.018950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852902</td>\n",
       "      <td>0.082989</td>\n",
       "      <td>0.097862</td>\n",
       "      <td>0.057449</td>\n",
       "      <td>0.059178</td>\n",
       "      <td>0.036089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.124395</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>0.065917</td>\n",
       "      <td>0.024650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.836033</td>\n",
       "      <td>0.130490</td>\n",
       "      <td>0.080150</td>\n",
       "      <td>0.031346</td>\n",
       "      <td>0.044484</td>\n",
       "      <td>0.019984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.892366</td>\n",
       "      <td>0.081661</td>\n",
       "      <td>0.133623</td>\n",
       "      <td>0.042848</td>\n",
       "      <td>0.102148</td>\n",
       "      <td>0.035524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.914334</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.127099</td>\n",
       "      <td>0.014827</td>\n",
       "      <td>0.092183</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       "                                                                        mean   \n",
       "0                                           0.915210                           \n",
       "1                                           0.834775                           \n",
       "2                                           0.950843                           \n",
       "3                                           0.852902                           \n",
       "4                                           0.900794                           \n",
       "5                                           0.836033                           \n",
       "6                                           0.892366                           \n",
       "7                                           0.914334                           \n",
       "\n",
       "            Jaccard Similarity (a,b)            \\\n",
       "        std                     mean       std   \n",
       "0  0.042344                 0.131372  0.036806   \n",
       "1  0.083865                 0.076315  0.018743   \n",
       "2  0.011448                 0.160636  0.027420   \n",
       "3  0.082989                 0.097862  0.057449   \n",
       "4  0.020504                 0.124395  0.028720   \n",
       "5  0.130490                 0.080150  0.031346   \n",
       "6  0.081661                 0.133623  0.042848   \n",
       "7  0.031987                 0.127099  0.014827   \n",
       "\n",
       "  Cosine Similarity tfidf Vectors            \n",
       "                             mean       std  \n",
       "0                        0.094874  0.054820  \n",
       "1                        0.058752  0.031411  \n",
       "2                        0.121287  0.018950  \n",
       "3                        0.059178  0.036089  \n",
       "4                        0.065917  0.024650  \n",
       "5                        0.044484  0.019984  \n",
       "6                        0.102148  0.035524  \n",
       "7                        0.092183  0.010700  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random = pd.concat(dfs_random).groupby(level=0)[['Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)',\n",
    " 'Jaccard Similarity (a,b)',\n",
    " 'Cosine Similarity tfidf Vectors',]].agg([\"mean\", \"std\"])\n",
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_ind\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latex_utils import get_p_asterisks, highlight_max, df_to_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_relResult(statistic=0.6991642934389493, pvalue=0.5070086131708251)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_rel(df[\"Jaccard Similarity (a,b)\"], df_random[(\"Jaccard Similarity (a,b)\", \"mean\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_asterisks_2samp(row):\n",
    "  \n",
    "  #  print(row)\n",
    "   # print(group.name)\n",
    "    p = float(row[\"pvalue\"])\n",
    "    val = float(row[\"method - random\"])\n",
    "    if p <= 0.001:\n",
    "        val = \"{:.2f}\\\\textsuperscript{{***}}\".format(val)\n",
    "    if p <= 0.01:\n",
    "        val = \"{:.2f}\\\\textsuperscript{{**}}\".format(val)\n",
    "    if p <= 0.05:\n",
    "        val = \"{:.2f}\\\\textsuperscript{{*}}\".format(val)\n",
    "    if p > 0.05:\n",
    "        val = \"{:.2f}\\\\textsuperscript{{ns}}\".format(val)\n",
    "    row[\"method - random\"] = val\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df = df.set_index([\"Explainer\", \"Detector\",\"f(d) → f(m)\"])[export_cols].groupby(groupby).agg(\n",
    "# {\n",
    "#         \"a\", lambda group: get_p_asterisks_2samp(ttest_ind, a,b),\n",
    "\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# p_results[('n')] = p_results[('[Score 1] cos sim', 'count')]\n",
    "# p_results = p_results.drop([('[Score 1] cos sim', 'count')], axis=1)\n",
    "\n",
    "# p_results = p_results[[list(p_results.columns)[-1]] + list(p_results.columns)[:-1]]\n",
    "# p_results.columns = [a for a, _ in p_results.columns]\n",
    "\n",
    "# p_results = p_results.style.apply(highlight_max, subset=p_results.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Documents Phases 1+3</th>\n",
       "      <th>Documents Phases 2+4</th>\n",
       "      <th>f(a)</th>\n",
       "      <th>f(b)</th>\n",
       "      <th>GT a</th>\n",
       "      <th>GT b</th>\n",
       "      <th>idx a</th>\n",
       "      <th>idx b</th>\n",
       "      <th>Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)</th>\n",
       "      <th>Jaccard Similarity (a,b)</th>\n",
       "      <th>Cosine Similarity tfidf Vectors</th>\n",
       "      <th>hash a</th>\n",
       "      <th>hash b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Credit unions are not-for-profit financial coo...</td>\n",
       "      <td>Gordon Bell is a computer scientist and electr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>161</td>\n",
       "      <td>204</td>\n",
       "      <td>0.865433</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.035942</td>\n",
       "      <td>51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...</td>\n",
       "      <td>49afdd008b7667f8c575f34ad378723e39feb10666c2e9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Both are saying essentially the same thing.  T...</td>\n",
       "      <td>Sounds like you are reconciling more than once...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "      <td>273</td>\n",
       "      <td>0.734486</td>\n",
       "      <td>0.123188</td>\n",
       "      <td>0.068672</td>\n",
       "      <td>714b04dd8923e09ea3f370b93660441d792104140d13d3...</td>\n",
       "      <td>83d0fafaffdb717a9f6ec22b781b56516afc286a1eae78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>It is possible that your son is experiencing a...</td>\n",
       "      <td>There are many potential causes of fever, shiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>170</td>\n",
       "      <td>301</td>\n",
       "      <td>0.919242</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.089471</td>\n",
       "      <td>53bb29915fb86c3bd323ddf3342af288e3110b3d8c470c...</td>\n",
       "      <td>0e65e1bbbf8133d14e940529817a55398d439910f1376e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Human intelligence is the intellectual capabil...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>247</td>\n",
       "      <td>0.955458</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.065098</td>\n",
       "      <td>42b84c175e792cbace8c18218652b0e5fc172d0722913d...</td>\n",
       "      <td>936af4095ba223d41fa40efae657dfe1169abfac77176d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>A Tensor Processing Unit (TPU) is a custom acc...</td>\n",
       "      <td>It is difficult to determine the frequency wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>256</td>\n",
       "      <td>0.935912</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.070008</td>\n",
       "      <td>06146f733e03754c43c81770ce1e2b06934eb318f50d93...</td>\n",
       "      <td>ed76b4ba45aefde60af94db8dd4cb92a1c331f5ce954bf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Hello dearWarm  welcome to Healthcaremagic.com...</td>\n",
       "      <td>Thanks for your question on Healthcare Magic. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>129</td>\n",
       "      <td>194</td>\n",
       "      <td>0.871972</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>0.093720</td>\n",
       "      <td>e5e2abf2af1e944f04c8c9d000f7dbf4e66c4d234f83a9...</td>\n",
       "      <td>47e9700fa53a47fb40d1025a89aec77680c41248c82119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Automated decision-making refers to the use of...</td>\n",
       "      <td>Predictive analytics is a type of data analysi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>182</td>\n",
       "      <td>212</td>\n",
       "      <td>0.965771</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.215914</td>\n",
       "      <td>bab7c7ca0e1ba531721cb2418eb0b0fde40769c709323d...</td>\n",
       "      <td>150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>TL;DR: The date they were granted.  (Usually, ...</td>\n",
       "      <td>Reuters has a service you can subscribe to tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>71</td>\n",
       "      <td>249</td>\n",
       "      <td>0.953523</td>\n",
       "      <td>0.122093</td>\n",
       "      <td>0.073148</td>\n",
       "      <td>da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...</td>\n",
       "      <td>e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Detector       Explainer  \\\n",
       "0  DetectorRadford  LIME_Explainer   \n",
       "1  DetectorRadford  LIME_Explainer   \n",
       "2  DetectorRadford  SHAP_Explainer   \n",
       "3  DetectorRadford  SHAP_Explainer   \n",
       "4      DetectorGuo  LIME_Explainer   \n",
       "5      DetectorGuo  LIME_Explainer   \n",
       "6      DetectorGuo  SHAP_Explainer   \n",
       "7      DetectorGuo  SHAP_Explainer   \n",
       "\n",
       "                                Documents Phases 1+3  \\\n",
       "0  Credit unions are not-for-profit financial coo...   \n",
       "1  Both are saying essentially the same thing.  T...   \n",
       "2  It is possible that your son is experiencing a...   \n",
       "3  Predictive analytics encompasses a variety of ...   \n",
       "4  A Tensor Processing Unit (TPU) is a custom acc...   \n",
       "5  Hello dearWarm  welcome to Healthcaremagic.com...   \n",
       "6  Automated decision-making refers to the use of...   \n",
       "7  TL;DR: The date they were granted.  (Usually, ...   \n",
       "\n",
       "                                Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       "0  Gordon Bell is a computer scientist and electr...     0     0  False   \n",
       "1  Sounds like you are reconciling more than once...     1     1   True   \n",
       "2  There are many potential causes of fever, shiv...     0     0  False   \n",
       "3  Human intelligence is the intellectual capabil...     1     1   True   \n",
       "4  It is difficult to determine the frequency wit...     0     0  False   \n",
       "5  Thanks for your question on Healthcare Magic. ...     1     1   True   \n",
       "6  Predictive analytics is a type of data analysi...     0     0  False   \n",
       "7  Reuters has a service you can subscribe to tha...     1     1   True   \n",
       "\n",
       "    GT b  idx a  idx b  \\\n",
       "0  False    161    204   \n",
       "1   True     52    273   \n",
       "2  False    170    301   \n",
       "3   True      6    247   \n",
       "4  False     33    256   \n",
       "5   True    129    194   \n",
       "6  False    182    212   \n",
       "7   True     71    249   \n",
       "\n",
       "   Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       "0                                           0.865433                            \n",
       "1                                           0.734486                            \n",
       "2                                           0.919242                            \n",
       "3                                           0.955458                            \n",
       "4                                           0.935912                            \n",
       "5                                           0.871972                            \n",
       "6                                           0.965771                            \n",
       "7                                           0.953523                            \n",
       "\n",
       "   Jaccard Similarity (a,b)  Cosine Similarity tfidf Vectors  \\\n",
       "0                  0.100000                         0.035942   \n",
       "1                  0.123188                         0.068672   \n",
       "2                  0.160000                         0.089471   \n",
       "3                  0.108374                         0.065098   \n",
       "4                  0.108571                         0.070008   \n",
       "5                  0.092715                         0.093720   \n",
       "6                  0.166667                         0.215914   \n",
       "7                  0.122093                         0.073148   \n",
       "\n",
       "                                              hash a  \\\n",
       "0  51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...   \n",
       "1  714b04dd8923e09ea3f370b93660441d792104140d13d3...   \n",
       "2  53bb29915fb86c3bd323ddf3342af288e3110b3d8c470c...   \n",
       "3  42b84c175e792cbace8c18218652b0e5fc172d0722913d...   \n",
       "4  06146f733e03754c43c81770ce1e2b06934eb318f50d93...   \n",
       "5  e5e2abf2af1e944f04c8c9d000f7dbf4e66c4d234f83a9...   \n",
       "6  bab7c7ca0e1ba531721cb2418eb0b0fde40769c709323d...   \n",
       "7  da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...   \n",
       "\n",
       "                                              hash b  \n",
       "0  49afdd008b7667f8c575f34ad378723e39feb10666c2e9...  \n",
       "1  83d0fafaffdb717a9f6ec22b781b56516afc286a1eae78...  \n",
       "2  0e65e1bbbf8133d14e940529817a55398d439910f1376e...  \n",
       "3  936af4095ba223d41fa40efae657dfe1169abfac77176d...  \n",
       "4  ed76b4ba45aefde60af94db8dd4cb92a1c331f5ce954bf...  \n",
       "5  47e9700fa53a47fb40d1025a89aec77680c41248c82119...  \n",
       "6  150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...  \n",
       "7  e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean method</th>\n",
       "      <th>mean random</th>\n",
       "      <th>method - random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)</th>\n",
       "      <td>0.9002245630907298</td>\n",
       "      <td>0.8871572234713475</td>\n",
       "      <td>0.01\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard Similarity (a,b)</th>\n",
       "      <td>0.12270114253944292</td>\n",
       "      <td>0.11643161785407441</td>\n",
       "      <td>0.01\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine Similarity tfidf Vectors</th>\n",
       "      <td>0.08899658086140039</td>\n",
       "      <td>0.07985282680165172</td>\n",
       "      <td>0.01\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            mean method  \\\n",
       "Spacy Semantic Similarity: Cosine Similarity Av...   0.9002245630907298   \n",
       "Jaccard Similarity (a,b)                            0.12270114253944292   \n",
       "Cosine Similarity tfidf Vectors                     0.08899658086140039   \n",
       "\n",
       "                                                            mean random  \\\n",
       "Spacy Semantic Similarity: Cosine Similarity Av...   0.8871572234713475   \n",
       "Jaccard Similarity (a,b)                            0.11643161785407441   \n",
       "Cosine Similarity tfidf Vectors                     0.07985282680165172   \n",
       "\n",
       "                                                             method - random  \n",
       "Spacy Semantic Similarity: Cosine Similarity Av...  0.01\\textsuperscript{ns}  \n",
       "Jaccard Similarity (a,b)                            0.01\\textsuperscript{ns}  \n",
       "Cosine Similarity tfidf Vectors                     0.01\\textsuperscript{ns}  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics = [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]\n",
    "t = {}\n",
    "for metric in metrics:\n",
    "    tstatistic, pvalue = ttest_ind(df[metric], df_random[(metric, \"mean\")])\n",
    "    t[metric] = (tstatistic, pvalue, df[metric].mean(), df_random[(metric, \"mean\")].mean(), df[metric].mean() - df_random[(metric, \"mean\")].mean())\n",
    "pd.DataFrame(t, index=[\"tstatistic\", \"pvalue\",\"mean method\", \"mean random\", \"method - random\" ]).transpose().astype(\"str\").apply(get_p_asterisks_2samp, axis=1).drop([\"pvalue\",\"tstatistic\"], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_fi_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_fi_similarity\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_fi_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_W(df):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in df.iterrows():\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "\n",
    "                sim = cosine_similarity(W) \n",
    "                cosine_similarity_in_w = sim[0,1]\n",
    "\n",
    "                n_tokens_overlap_in_w = np.all(W != 0, axis = 0).sum()# / np.any(W != 0, axis = 0).sum()\n",
    "                \n",
    "                sim = euclidean_distances(W) \n",
    "                sim_eucledian = sim[0,1]\n",
    "                results.append((\n",
    "                    explainer.__class__.__name__,\n",
    "                    detector.__class__.__name__,\n",
    "                    sim_eucledian,\n",
    "                    cosine_similarity_in_w,\n",
    "                    n_tokens_overlap_in_w))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\n",
    "        \"Explainer\",\n",
    "        \"Detector\",\n",
    "        \"sim_eucledian\",\n",
    "        \"cosine_similarity_in_w\",\n",
    "        \"Overlap in W (tokens)\",\n",
    "        ])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Jaccard Similarity (a,b)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Cosine Similarity tfidf Vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.042344</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>0.036806</td>\n",
       "      <td>0.094874</td>\n",
       "      <td>0.054820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834775</td>\n",
       "      <td>0.083865</td>\n",
       "      <td>0.076315</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>0.031411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950843</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.160636</td>\n",
       "      <td>0.027420</td>\n",
       "      <td>0.121287</td>\n",
       "      <td>0.018950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852902</td>\n",
       "      <td>0.082989</td>\n",
       "      <td>0.097862</td>\n",
       "      <td>0.057449</td>\n",
       "      <td>0.059178</td>\n",
       "      <td>0.036089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.124395</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>0.065917</td>\n",
       "      <td>0.024650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.836033</td>\n",
       "      <td>0.130490</td>\n",
       "      <td>0.080150</td>\n",
       "      <td>0.031346</td>\n",
       "      <td>0.044484</td>\n",
       "      <td>0.019984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.892366</td>\n",
       "      <td>0.081661</td>\n",
       "      <td>0.133623</td>\n",
       "      <td>0.042848</td>\n",
       "      <td>0.102148</td>\n",
       "      <td>0.035524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.914334</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.127099</td>\n",
       "      <td>0.014827</td>\n",
       "      <td>0.092183</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       "                                                                        mean   \n",
       "0                                           0.915210                           \n",
       "1                                           0.834775                           \n",
       "2                                           0.950843                           \n",
       "3                                           0.852902                           \n",
       "4                                           0.900794                           \n",
       "5                                           0.836033                           \n",
       "6                                           0.892366                           \n",
       "7                                           0.914334                           \n",
       "\n",
       "            Jaccard Similarity (a,b)            \\\n",
       "        std                     mean       std   \n",
       "0  0.042344                 0.131372  0.036806   \n",
       "1  0.083865                 0.076315  0.018743   \n",
       "2  0.011448                 0.160636  0.027420   \n",
       "3  0.082989                 0.097862  0.057449   \n",
       "4  0.020504                 0.124395  0.028720   \n",
       "5  0.130490                 0.080150  0.031346   \n",
       "6  0.081661                 0.133623  0.042848   \n",
       "7  0.031987                 0.127099  0.014827   \n",
       "\n",
       "  Cosine Similarity tfidf Vectors            \n",
       "                             mean       std  \n",
       "0                        0.094874  0.054820  \n",
       "1                        0.058752  0.031411  \n",
       "2                        0.121287  0.018950  \n",
       "3                        0.059178  0.036089  \n",
       "4                        0.065917  0.024650  \n",
       "5                        0.044484  0.019984  \n",
       "6                        0.102148  0.035524  \n",
       "7                        0.092183  0.010700  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean method</th>\n",
       "      <th>mean random</th>\n",
       "      <th>method - random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sim_eucledian</th>\n",
       "      <td>0.14742105115951135</td>\n",
       "      <td>0.19270881045410695</td>\n",
       "      <td>-0.05\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_similarity_in_w</th>\n",
       "      <td>0.6567413929311748</td>\n",
       "      <td>0.2073151200225199</td>\n",
       "      <td>0.45\\textsuperscript{*}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overlap in W (tokens)</th>\n",
       "      <td>13.125</td>\n",
       "      <td>9.125</td>\n",
       "      <td>4.00\\textsuperscript{ns}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mean method          mean random  \\\n",
       "sim_eucledian           0.14742105115951135  0.19270881045410695   \n",
       "cosine_similarity_in_w   0.6567413929311748   0.2073151200225199   \n",
       "Overlap in W (tokens)                13.125                9.125   \n",
       "\n",
       "                                  method - random  \n",
       "sim_eucledian           -0.05\\textsuperscript{ns}  \n",
       "cosine_similarity_in_w    0.45\\textsuperscript{*}  \n",
       "Overlap in W (tokens)    4.00\\textsuperscript{ns}  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t = {}\n",
    "m_method = get_metrics_W(df)\n",
    "m_random = pd.concat([get_metrics_W(df_random) for df_random in dfs_random]).groupby(level=0).agg([\"mean\", \"std\"])\n",
    "\n",
    "for metric in m_method.columns:\n",
    "    tstatistic, pvalue = ttest_ind(m_method[metric], m_random[(metric, \"mean\")])\n",
    "    t[metric] = (tstatistic, pvalue, m_method[metric].mean(), m_random[(metric, \"mean\")].mean(), m_method[metric].mean() - m_random[(metric, \"mean\")].mean())\n",
    "pd.DataFrame(t, index=[\"tstatistic\", \"pvalue\",\"mean method\", \"mean random\", \"method - random\" ]).transpose().astype(\"str\").apply(get_p_asterisks_2samp, axis=1).drop([\"pvalue\",\"tstatistic\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sim_eucledian', 'cosine_similarity_in_w', 'Overlap in W (tokens)']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity a,b in W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.500401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.568650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.889212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.797556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.554297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.425916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.824385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.693515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Cosine Similarity a,b in W\n",
       "Explainer      Detector                                   \n",
       "LIME_Explainer DetectorGuo                        0.500401\n",
       "               DetectorGuo                        0.568650\n",
       "SHAP_Explainer DetectorGuo                        0.889212\n",
       "               DetectorGuo                        0.797556\n",
       "LIME_Explainer DetectorRadford                    0.554297\n",
       "               DetectorRadford                    0.425916\n",
       "SHAP_Explainer DetectorRadford                    0.824385\n",
       "               DetectorRadford                    0.693515"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_W_random(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation BOW Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim_in_W(dff, all=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "\n",
    "            for idx, row in dff.iterrows():\n",
    "                if not all and row[\"Explainer\"] != explainer.__class__.__name__ or row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = cosine_similarity(W) \n",
    "                sim = sim[0,1]\n",
    "\n",
    "                sim_eucledian = euclidean_distances(W) \n",
    "                sim_eucledian = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim_eucledian, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Cosine Similarity a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity a,b in W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.500401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.568650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.889212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.797556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.554297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.425916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.824385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.693515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Cosine Similarity a,b in W\n",
       "Explainer      Detector                                   \n",
       "LIME_Explainer DetectorGuo                        0.500401\n",
       "               DetectorGuo                        0.568650\n",
       "SHAP_Explainer DetectorGuo                        0.889212\n",
       "               DetectorGuo                        0.797556\n",
       "LIME_Explainer DetectorRadford                    0.554297\n",
       "               DetectorRadford                    0.425916\n",
       "SHAP_Explainer DetectorRadford                    0.824385\n",
       "               DetectorRadford                    0.693515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_fi_similarity.mean() Cosine Similarity a,b in W    0.656741\n",
      "dtype: float64\n",
      "df_fi_similarity_random.mean() Cosine Similarity a,b in W    0.230761\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity a,b in W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.500401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.568650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.889212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.797556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.554297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.425916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.824385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.693515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Cosine Similarity a,b in W\n",
       "Explainer      Detector                                   \n",
       "LIME_Explainer DetectorGuo                        0.500401\n",
       "               DetectorGuo                        0.568650\n",
       "SHAP_Explainer DetectorGuo                        0.889212\n",
       "               DetectorGuo                        0.797556\n",
       "LIME_Explainer DetectorRadford                    0.554297\n",
       "               DetectorRadford                    0.425916\n",
       "SHAP_Explainer DetectorRadford                    0.824385\n",
       "               DetectorRadford                    0.693515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_fi_similarity.mean() Cosine Similarity a,b in W    0.656741\n",
      "dtype: float64\n",
      "df_fi_similarity_random.mean() Cosine Similarity a,b in W    0.181469\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity a,b in W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.500401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.568650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.889212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.797556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.554297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.425916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.824385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>0.693515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Cosine Similarity a,b in W\n",
       "Explainer      Detector                                   \n",
       "LIME_Explainer DetectorGuo                        0.500401\n",
       "               DetectorGuo                        0.568650\n",
       "SHAP_Explainer DetectorGuo                        0.889212\n",
       "               DetectorGuo                        0.797556\n",
       "LIME_Explainer DetectorRadford                    0.554297\n",
       "               DetectorRadford                    0.425916\n",
       "SHAP_Explainer DetectorRadford                    0.824385\n",
       "               DetectorRadford                    0.693515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_fi_similarity.mean() Cosine Similarity a,b in W    0.656741\n",
      "dtype: float64\n",
      "df_fi_similarity_random.mean() Cosine Similarity a,b in W    0.209715\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for df_random in dfs_random:\n",
    "    df_fi_similarity_random = get_cos_sim_in_W(df_random)\n",
    "    df_fi_similarity = get_cos_sim_in_W(df)\n",
    "    display(df_fi_similarity)\n",
    "    print(\"df_fi_similarity.mean()\", df_fi_similarity.mean())\n",
    "    print(\"df_fi_similarity_random.mean()\", df_fi_similarity_random.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_random in dfs_random:\n",
    "    df_fi_similarity_random = get_cos_sim_in_W(df_random, all=True)\n",
    "    df_fi_similarity = get_cos_sim_in_W(df, all=True)\n",
    "\n",
    "    print(\"df_fi_similarity.mean()\", df_fi_similarity.mean())\n",
    "    print(\"df_fi_similarity_random.mean()\", df_fi_similarity_random.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity[\"Cosine Similarity a,b in W\"], df_fi_similarity_random[\"Cosine Similarity a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eucledian_distance_in_W(dff):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "\n",
    "            for idx, row in dff.iterrows():\n",
    "                if row[\"Explainer\"] != explainer.__class__.__name__ or row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = euclidean_distances(W) \n",
    "                sim = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Eucledian Distance a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random = get_eucledian_distance_in_W(df_random)\n",
    "df_fi_similarity_eucledian = get_eucledian_distance_in_W(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity_eucledian[\"Eucledian Distance a,b in W\"], df_fi_similarity_eucledian_random[\"Eucledian Distance a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]:\n",
    "    print(metric)\n",
    "    print(\"     \", ttest_ind(df[metric], df_random[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation overlap in non-zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap_in_W(dff, all=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in dff.iterrows():\n",
    "                if not all and row[\"Explainer\"] != explainer.__class__.__name__ or row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = np.all(W != 0, axis = 0).sum()# / np.any(W != 0, axis = 0).sum()\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Overlap in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_random in dfs_random:\n",
    "    df_fi_similarity_random = get_overlap_in_W(df_random)\n",
    "    df_fi_similarity = get_overlap_in_W(df)\n",
    "\n",
    "    print(\"get_overlap_in_W.mean()\", df_fi_similarity.mean())\n",
    "    print(\"get_overlap_in_W.mean_random()\", df_fi_similarity_random.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_random in dfs_random:\n",
    "    df_fi_similarity_random = get_overlap_in_W(df_random, all=True)\n",
    "    df_fi_similarity = get_overlap_in_W(df, all=True)\n",
    "\n",
    "    print(\"get_overlap_in_W.mean()\", df_fi_similarity.mean())\n",
    "    print(\"get_overlap_in_W.mean_random()\", df_fi_similarity_random.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_overlap_in_W_random.groupby(\"Explainer\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_overlap_in_W.groupby(\"Explainer\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_overlap_in_W.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_overlap_in_W_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_overlap_in_W[\"Overlap in W\"], df_fi_overlap_in_W_random[\"Overlap in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
