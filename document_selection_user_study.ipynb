{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: Always set this flag to `True` before git commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBFUSCATE_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "import lime\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<p><b>This is a {kind_of_document} document.</b></p>\n",
    "<p>The detector {correctly_or_wrongly} predicted that this document was... </p>\n",
    "<p>&emsp; ... machine generated with {p_machine} % confidence.</p>\n",
    "<p>&emsp; ... human written with {p_human} % confidence.</p> \n",
    "<div style=\"float:left;\">{highlighted_text}</div>\n",
    "\"\"\"\n",
    "#<div style=\"float:left; height:30em;\">{barplot_machine}{barplot_human}</div>\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "def print_template(document, gold_label, detector, explainer):\n",
    "    p_machine, p_human = detector.predict_proba([document])[0]\n",
    "   # machine, human = explainer.get_barplots_HTML(document)\n",
    "    display(HTML(template.format(\n",
    "    p_machine=int(p_machine*100) if not OBFUSCATE_RESULTS else \"<redacted>\", \n",
    "    p_human=int(p_human*100) if not OBFUSCATE_RESULTS else \"<redacted>\",\n",
    "  #  barplot_machine=machine,\n",
    "  #  barplot_human=human,\n",
    "    kind_of_document= ((\"machine generated\" if gold_label == False else \"human written\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    correctly_or_wrongly= ((\"correctly\" if detector.predict_label([document])[0] == gold_label else \"wrongly\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    highlighted_text=explainer.get_highlighted_text_HTML((document if not OBFUSCATE_RESULTS else \"<redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted>\")),\n",
    "    )))\n",
    "def print_shared_features(features, fi_scores):\n",
    "    for feature, (fi_score_a, fi_score_b) in zip(features, fi_scores):\n",
    "        print(feature)\n",
    "        print(\"\\t\\ta: {} \\t b: {}\".format(fi_score_a, fi_score_b))\n",
    "        print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer, skip_n=0):\n",
    "\n",
    "    for (a,b)in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            print(\"idx_a: <redacted> idx_b <redacted>\")\n",
    "        else:\n",
    "            print(\"idx_a: {} idx_b {}\".format(document_ids[a],document_ids[b]))\n",
    "\n",
    "        print_template(documents[a], gold_labels[a], detector, explainer)\n",
    "        print_template(documents[b], gold_labels[b], detector, explainer)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Feature Importance Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix of explanations for all documents in \"data\"\n",
    "# This function was once SubmodularPick.__init__() in LIME. It was planned to use its output for a search strategy for similar explanations. \n",
    "# Only the code for creating W from the paper (rows are explanations, cols are BOW features) remains\n",
    "def get_explanation_matrix_W(data, explainer, quiet=False):\n",
    "    # Get (cached) explanations \n",
    "    explanations_and_documents = [(d, explainer.get_fi_scores(d, fill=True)[0]) for d in tqdm(data, desc=\"Loading all explanations\",disable=quiet) ] # [0]: only irt to label machine, fill: return all words, even those with 0 fi\n",
    "\n",
    "    get_feature_name_signed = lambda feature,value : feature + (\"_+\" if value >=0 else \"_-\") # appends \"_+\" or \"_-\" to each feature name, e.g. \"example\" -> \"example_+\" if fi(example) > 0\n",
    "    # Ribeiro et al.: Find all the explanation model features used. Defines the dimension d'\n",
    "    # i.e. determine columns of W: each word (BOW) gets (up to) two columns, one for positive FI scores, one for negative FI scores\n",
    "    features_dict = {}\n",
    "    feature_iter = 0\n",
    "    for d, exp in tqdm(explanations_and_documents, desc=\"Building global dict of features\", disable=quiet):\n",
    "     #   print(\"exp\",exp)\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            feature_name = get_feature_name_signed(feature,value) # get_feature_name_signed: see above\n",
    "            if feature_name not in features_dict.keys():\n",
    "                features_dict[feature_name] = (feature_iter)\n",
    "                feature_iter += 1\n",
    "    d_prime = len(features_dict.keys())\n",
    "\n",
    "    # Ribeiro et al.: Create the n x d' dimensional 'explanation matrix', W\n",
    "    W = np.zeros((len(explanations_and_documents), d_prime))\n",
    "\n",
    "    # fill W, look up cols in dict that was just created\n",
    "    # W: one row per explanation, one col per feature in feature_dict\n",
    "    for i, (d, exp) in enumerate(tqdm(explanations_and_documents,  desc=\"Building W\",disable=quiet)):\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            # get_feature_name_signed: see above\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            W[i, features_dict[get_feature_name_signed(feature,value)]] += value\n",
    "    return W, features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples: (pair of documents whose explanations are similar, the features that overlap, fi scores of said features)\n",
    "# this maximizes similarity between documents (greedy, each document is only part of one tuple)\n",
    "# another function should select n tuples to maximize coverage in explanation space akin to SP-LIME\n",
    "sum_two_max = None\n",
    "def get_pairs(documents, W, detector, features_dict, n_pairs=None):\n",
    "    if n_pairs is None:\n",
    "        n_pairs = len(documents)//2\n",
    "    idx_pairs = [] # tuples of indices of similar documents a,b in \"data\"\n",
    "    features = [] # list of features those documents covered\n",
    "    fi_scores_pairs = []\n",
    "\n",
    "    W_ = np.copy(W)\n",
    "\n",
    "    document_indices = np.arange(0, W_.shape[0])\n",
    "   # print(document_indices.shape, W_.shape)\n",
    "    for _ in tqdm(range(0,n_pairs), desc=\"Obtaining pairs\"):\n",
    "        sim = cosine_similarity(W_) # calculate cosine similarity between all explanations\n",
    "        sim = np.triu(sim,k=1)  # remove redundant information for argmax()\n",
    "\n",
    "        idx_max = np.unravel_index(sim.argmax(), sim.shape) # get most similar pair, result is (idx_a, idx_b)\n",
    "       # print(idx_max)\n",
    "        features_non_zero_in_both = np.intersect1d(W_[idx_max[0]].nonzero(),W_[idx_max[1]].nonzero()) # get features that have non-zero fi in both explanations\n",
    "                                                                                                      # is used later for selecting a set of tuples with high coverage (as in SP-LIME)\n",
    "        non_zero_features = [] # list with features that will be returned\n",
    "        non_zero_fi_scores_tuples = [] # list of tuples with fi scores in a and b that will be returned\n",
    "    \n",
    "        # look up feature_idxs in features_dict and append them to the output\n",
    "        for iii in features_non_zero_in_both:\n",
    "           key = list(features_dict.keys())[list(features_dict.values()).index(iii)]\n",
    "           non_zero_features.append(key)\n",
    "           non_zero_fi_scores_tuples.append((W_[idx_max[0],features_dict[key]], W_[idx_max[1],features_dict[key]]))\n",
    "        \n",
    "        # Only add pair to output list if valid: at least one common feature is not zero AND f(a) == f(b) (i.e., the explanation is arguing for the same detector verdict)\n",
    "        if len(non_zero_features) > 0:\n",
    "            a,b = detector.predict_label([documents[document_indices[idx_max[0]]], documents[document_indices[idx_max[1]]]])\n",
    "            if a == b:\n",
    "                idx_pairs.append(document_indices[list(idx_max)])\n",
    "                fi_scores_pairs.append(non_zero_fi_scores_tuples)\n",
    "                features.append(non_zero_features)\n",
    "        # delete pair from W_:\n",
    "        W_ = np.delete(W_, idx_max, axis=0) \n",
    "        document_indices = np.delete(document_indices, list(idx_max))\n",
    "\n",
    "    return idx_pairs, features, fi_scores_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "# this is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# implementing a greedy algorithm here: \n",
    "#   \"In order to achieve a maximal cover for p facilities under a given service distance, \n",
    "#   the algorithm starts with an empty solution set and then adds to this set one at a \n",
    "#   time the best facility sites. The GA algorithm picks for the first facility that \n",
    "#   site which covers the most of the total population. For the second facility, GA \n",
    "#   picks the site that covers the most of the population not covered by the first \n",
    "#   facility. Then, for the third facility, GA picks the site that covers the most of the \n",
    "#   population not covered by the first and second facilities. This process is continued until either p facilities have been selected or all the population is covered. \n",
    "#   Details of the algorithm are given in Church.\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n",
    "def get_site_with_max_coverage(sites, previous_selections, W):\n",
    "    best_site = None\n",
    "    best_coverage = 0\n",
    "    for site in sites:\n",
    "        candidate = set(np.array(previous_selections).flatten()).union(site) # extend the previous selection by \"site\", this addresses \"For the second facility, GA picks the site that covers the most of the population not covered by the first \"\n",
    "        cov = coverage(candidate, W) # compute new coverage\n",
    "        if cov >= best_coverage:\n",
    "            best_coverage = cov\n",
    "            best_site = site\n",
    "    return best_site, best_coverage\n",
    "\n",
    "def get_p_tuples_with_high_coverage(indices, W, p=10):\n",
    "  sites = list(indices)\n",
    "  # \"the algorithm start with emty solution set\"\n",
    "  result = list()\n",
    "  # \"and then adds to this set one at a time the best facility sites\"\n",
    "  while True:\n",
    "      # \"The GA algorithm picks for the first facility that \n",
    "      # site which covers the most of the total population\"\n",
    "      best_site, best_coverage = get_site_with_max_coverage(sites, result, W)\n",
    "      result.append(best_site)\n",
    "      # \"This process is continued until either p facilities have been selected or all the population is covered.\"\n",
    "      if len(result) == p or best_coverage == W.shape[1]:\n",
    "          break\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two pairs, one for f(x) = machine and one for f(x) = human\n",
    "# checks texts_already_selected and chooses next best pair (for each class) if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair)\n",
    "def obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    W, features_dict = get_explanation_matrix_W(documents, explainer)\n",
    "    indices, _, _ = get_pairs(documents, W, detector, features_dict)\n",
    "    # want a dataset that is balanced irt to the two base classes:\n",
    "    # increase number of pairs returned by greedy algorithm until the p tuples include examples for both classes:\n",
    "    k = 4\n",
    "\n",
    "    pair_human = None\n",
    "    pair_machine = None\n",
    "\n",
    "    predictions= None\n",
    "    while True:\n",
    "        # obtain k pairs with high coverage\n",
    "        pairs = get_p_tuples_with_high_coverage(indices, W, p=k)\n",
    "\n",
    "        # get f(a) as one example per class is returned\n",
    "        predictions = [detector.predict_label([documents[a]])[0] for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "        # return example with highest coverage for each class\n",
    "        # if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair), the one with the next-highest coverage (for that prediction) is returned \n",
    "        for idx_pair, prediction in enumerate(predictions):\n",
    "            a,b = pairs[idx_pair]\n",
    "            # check if a or b are in texts_already_selected\n",
    "            if (documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected):\n",
    "                if prediction == 0 and pair_machine is None: # only keep first\n",
    "                    pair_machine = pairs[idx_pair] \n",
    "                if prediction == 1 and pair_human is None: # only keep first\n",
    "                    pair_human = pairs[idx_pair] \n",
    "            if pair_human is not None and pair_machine is not None:\n",
    "                return [pair_machine, pair_human] \n",
    "        k+=1 # loop until both pair_machine and pair_human not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Rule-Based Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document_a, document_b):\n",
    "    # case sensitive, on spacy tokens\n",
    "    a = list(chain(*[[token.text for token in sent] for sent in nlp(document_a).sents]))\n",
    "    b = list(chain(*[[token.text for token in sent] for sent in nlp(document_b).sents]))\n",
    "    intersection = float(len(list(set(a).intersection(b))))\n",
    "    union = float((len(set(a)) + len(set(b)))) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the order of occurence in a list of words, e.g.:\n",
    "# [\"example\", \"test\", \"example\", \"one\"] -> ['example_0', 'test_0', 'example_1', 'one_0']\n",
    "def encode_count(list_of_words):\n",
    "    d = defaultdict(lambda : 0)\n",
    "    encoded = []\n",
    "    for word in list_of_words:\n",
    "        encoded.append(word + \"_\" + str(d[word]))\n",
    "        d[word] +=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonary Anchors returns can define multiple anchors:\n",
    "# {this, is, an, example} : 0.9\n",
    "# {this, is, an}: 0.8\n",
    "# {this, is, }: 0.75\n",
    "# {this}: 0.4\n",
    "# extract all of them, only keep those with p >= 0.75 (threshold set when searching)\n",
    "def get_anchors_at_each_k(documents, explainer):\n",
    "    anchors = []\n",
    "    p = []\n",
    "    ids = []\n",
    "    for i,_ in tqdm(enumerate(documents), desc=\"Loading all explanations\"):#enumerate(documents):\n",
    "        exp = explainer.get_explanation_cached(documents[i])\n",
    "        exp[\"names\"] = encode_count(exp[\"names\"]) # Anchors is not BOW. But the algorithm is written with python set()s\n",
    "        while len(exp[\"mean\"]) >=1:#and exp[\"mean\"][-1] >= 0.75:\n",
    "            anchors.append(set(exp[\"names\"])) \n",
    "            p.append(exp[\"mean\"][-1])\n",
    "            ids.append(i)\n",
    "\n",
    "            exp[\"mean\"].pop()\n",
    "            exp[\"names\"].pop()\n",
    "    return anchors, p, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for pairs of anchors\n",
    "# returns 2 pairs of documents, one pair for f(x) = machine, one for f(x) = human, both sampled randomly\n",
    "# checks for and skips documents in \"texts_already_selected\" (i.e. it was already selected for an other explainer-detector pair)\n",
    "\n",
    "def obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "\n",
    "    anchors, p, ids = get_anchors_at_each_k(documents, explainer)\n",
    "                        # DetectGPT + Anchors is to expensive to run experiments on \n",
    "    # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "    duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "    # get the ids and p for each duplicate_anchor in  duplicate_anchors\n",
    "    # \"candidates\" is a list of lists with ids (and all other details) of each duplicate_anchor\n",
    "    candidates = [[(anchor, p, document_id) for anchor, p, document_id in zip(anchors, p, ids) if anchor == duplicate_anchor] for duplicate_anchor in duplicate_anchors ]\n",
    "    # now check for each paring of the documents in each sublist of \"candidates\":\n",
    "    #   is f(a) == f(b)?, if not: discard\n",
    "    # then pick pair with highest jaccard_score on the original documents in each \"candidate\"\n",
    "    pairs = []\n",
    "\n",
    "    predictions_cache = {}\n",
    "    def cached_predict(idx):\n",
    "        if idx not in predictions_cache:\n",
    "            predictions_cache[idx] = detector.predict_label([documents[idx]])[0]\n",
    "        return predictions_cache[idx]\n",
    "    for candidate in tqdm(candidates, desc=\"Assessing candidates\",position=1):\n",
    "        anchor_s, p, ids  = zip(*candidate)\n",
    "        c = list(combinations(ids, 2))\n",
    "        c = [(a,b) for a,b in c if cached_predict(a) == cached_predict(b)]\n",
    "        if len(c) == 0:\n",
    "            continue\n",
    "        jaccard_scores = [(a,b, jaccard_similarity(documents[a], documents[b])) for a,b in tqdm(c, desc=\"Calculating Jaccard Similarity (of documents not Anchors)\",position=0)]\n",
    "        a,b, score = max(jaccard_scores, key=lambda x: x[2])\n",
    "        pairs.append((a,b))\n",
    "\n",
    "    # sample twice: once for f(x) == human and once for f(x) == machine. f(a) == f(b) is tested earlier\n",
    "\n",
    "    predictions = [cached_predict(a) for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "    predictions_ = np.array(predictions)\n",
    "    pairs_ = np.array(pairs)\n",
    "\n",
    "    machine = pairs_[predictions_ == False]\n",
    "    human = pairs_[predictions_ == True]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    result = []\n",
    "    # select 2 pairs from pairs_: one for each class\n",
    "    # need to check if a document from the pair is in texts_already_selected\n",
    "    while True:       \n",
    "        # one explainer (DetectGPT) has no explanations for f(x) = human:\n",
    "        if not(True in predictions):\n",
    "            print(\"Warning: No examples for f(x) = human. Returning an additional example for machine\")\n",
    "            result =  machine[np.random.choice(machine.shape[0], 2, replace=False)]\n",
    "        elif not (False in predictions):\n",
    "            print(\"Warning: No examples for f(x) = machine. Returning an additional example for human\")\n",
    "            result = human[np.random.choice(human.shape[0], 2, replace=False)]\n",
    "        else:\n",
    "            result =  [machine[np.random.randint(0, machine.shape[0]),:], human[np.random.randint(0, human.shape[0]),:]] # returns a random pair for machine and a random pair for human\n",
    "\n",
    "        \n",
    "        # check for duplicates in texts_already_selected, re-sample if the pairs are duplicates.\n",
    "        if all([(documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected) for a,b in result]):\n",
    "            return result\n",
    "        else:\n",
    "            print(\"Loop: Avoiding duplicates\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        return obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)\n",
    "    else:\n",
    "        return obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test \n",
    "\n",
    "documents = list(test[\"answer\"])\n",
    "gold_labels = list(test[\"author\"] == \"human_answers\") # convention: 0: machine, 1: human, see detector.py\n",
    "document_ids = list(range(0,len(documents))) # note that the search algorithms don't use these ids. They are only used for printing and the exclude_list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Document Selection\n",
    "Some documents are excluded from the user-study for the reasons specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195, 60, 108, 228, 288, 117, 188, 110, 16, 190, 294, 27]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exclude_list = {\n",
    "    (195, 60,108, 228): \"Names forum/service explicitly\",\n",
    "    (288,117, 188, 110): \"Author introduces themselves by name\",\n",
    "    (16,): \"References earlier post by other user\",\n",
    "    (190,294): \"Names forum user who asked the question\",\n",
    "    (27,): \"NSFW\",\n",
    "    \n",
    "    \n",
    "}\n",
    "exclude_list = [x for xs in [ list(key) for key in exclude_list.keys()] for x in xs]\n",
    "exclude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply exclude_list\n",
    "documents = [d for i,d in zip(document_ids, documents) if i not in exclude_list]\n",
    "gold_labels = [gl for i,gl in zip(document_ids, gold_labels) if i not in exclude_list]\n",
    "document_ids = [i for i in document_ids if i not in exclude_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: If you plan to participate in the user study, set `OBFUSCATE_RESULTS` to `True` before proceeding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Detector\", \"Explainer\", \"Documents Phases 1+3\", \"Documents Phases 2+4\", \"f(a)\", \"f(b)\", \"GT a\", \"GT b\", \"idx a\", \"idx b\", \"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\",\"hash a\", \"hash b\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_selection(pairs, explainer, detector):\n",
    "    for a,b in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            continue\n",
    "        \n",
    "        tfidf_= tfidf.transform([documents[a], documents[b]])   \n",
    "        selection.append((detector.__class__.__name__,\n",
    "                        explainer.__class__.__name__,\n",
    "                        documents[a], documents[b],\n",
    "                        *detector.predict_label([documents[a], documents[b]]),\n",
    "                        gold_labels[a],\n",
    "                        gold_labels[b],\n",
    "                        document_ids[a],\n",
    "                        document_ids[b],\n",
    "                        nlp(documents[a]).similarity(nlp(documents[b])),\n",
    "                        jaccard_similarity(documents[a], documents[b]),\n",
    "                        (tfidf_ * tfidf_.T).toarray()[0,1],\n",
    "                        explainer.get_hash(documents[a]),\n",
    "                        explainer.get_hash(documents[b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.19s)\n",
      "DONE (0.08s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>DetectorDetectGPT</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Anchor_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 293it [00:00, 607.06it/s]\n",
      "\n",
      "Assessing candidates:   0%|          | 0/27 [00:00<?, ?it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 15.74it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 16.42it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 19.20it/s]\n",
      "\n",
      "Assessing candidates:  15%|█▍        | 4/27 [00:14<01:31,  3.99s/it]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 25.96it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 17.68it/s]\n",
      "\n",
      "Assessing candidates:  26%|██▌       | 7/27 [00:19<00:48,  2.44s/it]\n",
      "Assessing candidates:  30%|██▉       | 8/27 [00:21<00:43,  2.26s/it]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 19.98it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 20.76it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.46it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 966/966 [00:44<00:00, 21.62it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 26.31it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 21/21 [00:00<00:00, 22.94it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 23.34it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 37/37 [00:01<00:00, 24.76it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 31/31 [00:01<00:00, 22.01it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 24.33it/s]\n",
      "\n",
      "Assessing candidates:  74%|███████▍  | 20/27 [03:00<01:08,  9.79s/it]\n",
      "Assessing candidates:  78%|███████▊  | 21/27 [03:02<00:44,  7.45s/it]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 15/15 [00:00<00:00, 15.51it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.22it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 17.27it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 21.18it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 11/11 [00:00<00:00, 22.15it/s]\n",
      "\n",
      "Assessing candidates: 100%|██████████| 27/27 [03:26<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No examples for f(x) = human. Returning only examples for machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>LIME_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:00<00:00, 952.63it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:01<00:00, 151.29it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:01<00:00, 156.00it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [04:37<00:00,  1.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>SHAP_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:00<00:00, 3219.57it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:20<00:00, 14.17it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:21<00:00, 13.51it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [04:32<00:00,  1.87s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>DetectorGuo</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Anchor_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 293it [00:03, 81.49it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 16.12it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 15.99it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 18.86it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 18.92it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 17.85it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 20.56it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 19.67it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.46it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 21.73it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 8193/8193 [06:01<00:00, 22.64it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 18.34it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 23.80it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 18.52it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 21.97it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 105/105 [00:05<00:00, 20.87it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 210/210 [00:09<00:00, 23.31it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 23.71it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 21.58it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 18.68it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.73it/s]\n",
      "\n",
      "Assessing candidates: 100%|██████████| 29/29 [06:24<00:00, 13.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: Avoiding duplicates\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>LIME_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:02<00:00, 124.96it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:01<00:00, 157.95it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:01<00:00, 155.63it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [00:07<00:00, 18.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>SHAP_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:02<00:00, 137.23it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:19<00:00, 14.65it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:20<00:00, 14.02it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [00:09<00:00, 15.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>DetectorRadford</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Anchor_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 293it [00:02, 131.01it/s]\n",
      "\n",
      "Assessing candidates:   0%|          | 0/26 [00:00<?, ?it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 21.62it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 7/7 [00:00<00:00, 21.37it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 27.39it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 2/2 [00:00<00:00, 20.51it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.97it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 21.05it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 3/3 [00:00<00:00, 19.86it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1407/1407 [01:00<00:00, 23.31it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 4/4 [00:00<00:00, 21.16it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 29.77it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 66/66 [00:03<00:00, 21.78it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 18/18 [00:00<00:00, 26.33it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 23.26it/s]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 22.46it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 13/13 [00:00<00:00, 23.52it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 11/11 [00:00<00:00, 20.50it/s]\n",
      "\n",
      "Assessing candidates: 100%|██████████| 26/26 [01:10<00:00,  2.72s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>LIME_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:02<00:00, 127.58it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:01<00:00, 156.84it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:01<00:00, 150.55it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [00:08<00:00, 17.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>SHAP_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 100%|██████████| 293/293 [00:00<00:00, 875.54it/s]\n",
      "Building global dict of features: 100%|██████████| 293/293 [00:19<00:00, 14.89it/s]\n",
      "Building W: 100%|██████████| 293/293 [00:19<00:00, 14.87it/s]\n",
      "Obtaining pairs: 100%|██████████| 146/146 [00:08<00:00, 16.46it/s]\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "for detector_class in [DetectorDetectGPT,DetectorGuo, DetectorRadford]:\n",
    "    detector = detector_class()\n",
    "    display(HTML(\"<h1>{}</h1>\".format(detector.__class__.__name__)))\n",
    "    for explainer_class in [Anchor_Explainer, LIME_Explainer,SHAP_Explainer]:\n",
    "        explainer = explainer_class(detector)\n",
    "        display(HTML(\"<h2>{}</h2>\".format(explainer.__class__.__name__)))\n",
    "        \n",
    "        texts_already_selected = []\n",
    "        if len(selection) > 0:\n",
    "            texts_already_selected = list(zip(*selection))[2] + list(zip(*selection))[3]\n",
    "        pairs = obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected=texts_already_selected)\n",
    "        update_selection(pairs, explainer, detector)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Documents Phases 1+3</th>\n",
       "      <th>Documents Phases 2+4</th>\n",
       "      <th>f(a)</th>\n",
       "      <th>f(b)</th>\n",
       "      <th>GT a</th>\n",
       "      <th>GT b</th>\n",
       "      <th>idx a</th>\n",
       "      <th>idx b</th>\n",
       "      <th>Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)</th>\n",
       "      <th>Jaccard Similarity (a,b)</th>\n",
       "      <th>Cosine Similarity tfidf Vectors</th>\n",
       "      <th>hash a</th>\n",
       "      <th>hash b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Both are saying essentially the same thing.  T...</td>\n",
       "      <td>Assuming you live in the US, it is quite norma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "      <td>238</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.153790</td>\n",
       "      <td>714b04dd8923e09ea3f370b93660441d792104140d13d3...</td>\n",
       "      <td>60b992dfcad293c2fbe76d7842a4e469ba041ced7c5270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It can be a good idea to follow the advice of ...</td>\n",
       "      <td>It is generally a good idea to use sponsorship...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>217</td>\n",
       "      <td>234</td>\n",
       "      <td>0.955040</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.203377</td>\n",
       "      <td>c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...</td>\n",
       "      <td>e573296bc2021073ef94c1f60c1f097dc770793203b348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Hi,1. The anti TB drug doses have to be prescr...</td>\n",
       "      <td>Credit unions are not-for-profit financial coo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>159</td>\n",
       "      <td>161</td>\n",
       "      <td>0.910814</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>6691320ac50e6e039d039bc509148cc20d924199ff32ba...</td>\n",
       "      <td>51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>You really shouldn't be using class tracking t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>142</td>\n",
       "      <td>0.801584</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.045019</td>\n",
       "      <td>42b84c175e792cbace8c18218652b0e5fc172d0722913d...</td>\n",
       "      <td>604e277aa0ddee14c63bd188af56065e65d36880c5698d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>The conversion ratio between General Motors (G...</td>\n",
       "      <td>It is not uncommon for pain to increase after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>166</td>\n",
       "      <td>255</td>\n",
       "      <td>0.946220</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.085576</td>\n",
       "      <td>c153dc1fe06bde184f21214073db83e11ce6f526e90551...</td>\n",
       "      <td>60707fbb129a62fd3e3a762cb9a9c8d57a8781c4d65dda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>If your counterparty sent money to a correspon...</td>\n",
       "      <td>As your is a very specific case, please get an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>0.949592</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.153888</td>\n",
       "      <td>3dd1514de75255da157c91f80bf9745485cc700f81cf40...</td>\n",
       "      <td>6b25114906789fc86fa719f0394277f9289f080cbbbc7e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>If your deductions are higher than your income...</td>\n",
       "      <td>Hallucinations can be caused by a variety of f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>77</td>\n",
       "      <td>205</td>\n",
       "      <td>0.936843</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.104886</td>\n",
       "      <td>4a28a8f0d8c6fbee4a7aad31697d315f2e478277646094...</td>\n",
       "      <td>f7a467afa47b3708b8ffd00f7dfecb7c4b359dc5a9c59a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Hello, Thanks for your query.This can occur du...</td>\n",
       "      <td>thanks for your query, the bump could be secon...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>0.962259</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.255573</td>\n",
       "      <td>797c3e2456febfd1031497754c2da79b9b0cb00f770118...</td>\n",
       "      <td>b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Treatment for a spiral fracture of the ring fi...</td>\n",
       "      <td>In the United States, you will generally have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>252</td>\n",
       "      <td>274</td>\n",
       "      <td>0.924423</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.106560</td>\n",
       "      <td>3838aecf67942145b93b96e42021f369c969c3ec9b719c...</td>\n",
       "      <td>2b0771ebe6b8d46c3ce99d97700281fe561da6c7a153fc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Reuters has a service you can subscribe to tha...</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>249</td>\n",
       "      <td>289</td>\n",
       "      <td>0.932199</td>\n",
       "      <td>0.171233</td>\n",
       "      <td>0.181787</td>\n",
       "      <td>e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...</td>\n",
       "      <td>333765b183235b4a0645cd283edc9089a44efa12118002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>It is generally not possible for an adult to b...</td>\n",
       "      <td>Ex Machina is a 2014 science fiction film writ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>59</td>\n",
       "      <td>68</td>\n",
       "      <td>0.888865</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.060680</td>\n",
       "      <td>bf5cc4428ba8a20b7f40625ce7893a43aa9b3652f73d1c...</td>\n",
       "      <td>f5851c5a8dedccdfae60f3db0e8db835f5e567b6f8f88a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Yes, many hedge funds (for example) did not su...</td>\n",
       "      <td>Is that indicator can only be used for short-t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>225</td>\n",
       "      <td>0.902835</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.061248</td>\n",
       "      <td>92a81921ee9378df56debc096418b3d578da70479fa3aa...</td>\n",
       "      <td>f36eea089d93178672b8dfd1ed059927a0018c71f337f3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible that your dental implants could...</td>\n",
       "      <td>Foamy feces can be caused by a variety of fact...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>285</td>\n",
       "      <td>0.941338</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>0.198382</td>\n",
       "      <td>dec23aa4bf1874d80f869e877d1828ee8b0b9489aafe5c...</td>\n",
       "      <td>a51392d3a9976b33ff5ec8bd300646491be0b2445376f0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Hello, This condition could be related to arth...</td>\n",
       "      <td>Hello, It could be a contusion. As of now you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>97</td>\n",
       "      <td>115</td>\n",
       "      <td>0.931208</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.218506</td>\n",
       "      <td>5b623f187b07950ce603fa64696cdb827ed276dd903ffa...</td>\n",
       "      <td>7ea4abb922d5c579d3a556e31d842a23104ff018d06975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Yes, it is still possible to receive a physica...</td>\n",
       "      <td>Samuel Butler was a 19th-century English novel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "      <td>136</td>\n",
       "      <td>0.755780</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.020612</td>\n",
       "      <td>2a25f893b9a12bf8353ec30061c37287265b907b5c58ff...</td>\n",
       "      <td>aae9d9d155150f9a8f8421a113a4488af0aca6f9e6cf4d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>I'm a bit out of my element here, but my guess...</td>\n",
       "      <td>HIThank for asking to HCMI really appreciate y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>124</td>\n",
       "      <td>143</td>\n",
       "      <td>0.948306</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>283ea38cf17b09bb85b98776bcc506d1e964faa3d9b89c...</td>\n",
       "      <td>67313a7b0c9cf4befce26c398758380e13d52c2cfcda30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>It is technically possible for individuals to ...</td>\n",
       "      <td>Predictive analytics is a type of data analysi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>67</td>\n",
       "      <td>212</td>\n",
       "      <td>0.960435</td>\n",
       "      <td>0.115183</td>\n",
       "      <td>0.102896</td>\n",
       "      <td>a8b15db3fcd1140dd828966011aa264433788e5732d384...</td>\n",
       "      <td>150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>The reason is, stores want customers to use ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0.887762</td>\n",
       "      <td>0.141243</td>\n",
       "      <td>0.112985</td>\n",
       "      <td>7f8cd548cf57f327155b067b8fbdbfc3b124bea38e84e5...</td>\n",
       "      <td>1c3c9f892ad6b9339c6c9e647691a8fd193d5e7c537909...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Detector         Explainer  \\\n",
       "0   DetectorDetectGPT  Anchor_Explainer   \n",
       "1   DetectorDetectGPT  Anchor_Explainer   \n",
       "2   DetectorDetectGPT    LIME_Explainer   \n",
       "3   DetectorDetectGPT    LIME_Explainer   \n",
       "4   DetectorDetectGPT    SHAP_Explainer   \n",
       "5   DetectorDetectGPT    SHAP_Explainer   \n",
       "6         DetectorGuo  Anchor_Explainer   \n",
       "7         DetectorGuo  Anchor_Explainer   \n",
       "8         DetectorGuo    LIME_Explainer   \n",
       "9         DetectorGuo    LIME_Explainer   \n",
       "10        DetectorGuo    SHAP_Explainer   \n",
       "11        DetectorGuo    SHAP_Explainer   \n",
       "12    DetectorRadford  Anchor_Explainer   \n",
       "13    DetectorRadford  Anchor_Explainer   \n",
       "14    DetectorRadford    LIME_Explainer   \n",
       "15    DetectorRadford    LIME_Explainer   \n",
       "16    DetectorRadford    SHAP_Explainer   \n",
       "17    DetectorRadford    SHAP_Explainer   \n",
       "\n",
       "                                 Documents Phases 1+3  \\\n",
       "0   Both are saying essentially the same thing.  T...   \n",
       "1   It can be a good idea to follow the advice of ...   \n",
       "2   Hi,1. The anti TB drug doses have to be prescr...   \n",
       "3   Predictive analytics encompasses a variety of ...   \n",
       "4   The conversion ratio between General Motors (G...   \n",
       "5   If your counterparty sent money to a correspon...   \n",
       "6   If your deductions are higher than your income...   \n",
       "7   Hello, Thanks for your query.This can occur du...   \n",
       "8   Treatment for a spiral fracture of the ring fi...   \n",
       "9   Reuters has a service you can subscribe to tha...   \n",
       "10  It is generally not possible for an adult to b...   \n",
       "11  Yes, many hedge funds (for example) did not su...   \n",
       "12  It is possible that your dental implants could...   \n",
       "13  Hello, This condition could be related to arth...   \n",
       "14  Yes, it is still possible to receive a physica...   \n",
       "15  I'm a bit out of my element here, but my guess...   \n",
       "16  It is technically possible for individuals to ...   \n",
       "17  Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "\n",
       "                                 Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       "0   Assuming you live in the US, it is quite norma...     0     0   True   \n",
       "1   It is generally a good idea to use sponsorship...     0     0  False   \n",
       "2   Credit unions are not-for-profit financial coo...     0     0   True   \n",
       "3   You really shouldn't be using class tracking t...     1     1   True   \n",
       "4   It is not uncommon for pain to increase after ...     0     0  False   \n",
       "5   As your is a very specific case, please get an...     1     1   True   \n",
       "6   Hallucinations can be caused by a variety of f...     0     0  False   \n",
       "7   thanks for your query, the bump could be secon...     1     1   True   \n",
       "8   In the United States, you will generally have ...     0     0  False   \n",
       "9   On what basis did you do your initial allocati...     1     1   True   \n",
       "10  Ex Machina is a 2014 science fiction film writ...     0     0  False   \n",
       "11  Is that indicator can only be used for short-t...     1     1   True   \n",
       "12  Foamy feces can be caused by a variety of fact...     0     0  False   \n",
       "13  Hello, It could be a contusion. As of now you ...     1     1   True   \n",
       "14  Samuel Butler was a 19th-century English novel...     0     0  False   \n",
       "15  HIThank for asking to HCMI really appreciate y...     1     1   True   \n",
       "16  Predictive analytics is a type of data analysi...     0     0  False   \n",
       "17  The reason is, stores want customers to use ca...     1     1   True   \n",
       "\n",
       "     GT b  idx a  idx b  \\\n",
       "0    True     52    238   \n",
       "1   False    217    234   \n",
       "2   False    159    161   \n",
       "3    True      6    142   \n",
       "4   False    166    255   \n",
       "5    True     22     82   \n",
       "6   False     77    205   \n",
       "7    True    163    175   \n",
       "8   False    252    274   \n",
       "9    True    249    289   \n",
       "10  False     59     68   \n",
       "11   True    171    225   \n",
       "12  False     69    285   \n",
       "13   True     97    115   \n",
       "14  False     85    136   \n",
       "15   True    124    143   \n",
       "16  False     67    212   \n",
       "17   True      5    121   \n",
       "\n",
       "    Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)  \\\n",
       "0                                            0.867257                            \n",
       "1                                            0.955040                            \n",
       "2                                            0.910814                            \n",
       "3                                            0.801584                            \n",
       "4                                            0.946220                            \n",
       "5                                            0.949592                            \n",
       "6                                            0.936843                            \n",
       "7                                            0.962259                            \n",
       "8                                            0.924423                            \n",
       "9                                            0.932199                            \n",
       "10                                           0.888865                            \n",
       "11                                           0.902835                            \n",
       "12                                           0.941338                            \n",
       "13                                           0.931208                            \n",
       "14                                           0.755780                            \n",
       "15                                           0.948306                            \n",
       "16                                           0.960435                            \n",
       "17                                           0.887762                            \n",
       "\n",
       "    Jaccard Similarity (a,b)  Cosine Similarity tfidf Vectors  \\\n",
       "0                   0.176101                         0.153790   \n",
       "1                   0.164062                         0.203377   \n",
       "2                   0.117188                         0.039040   \n",
       "3                   0.063694                         0.045019   \n",
       "4                   0.149660                         0.085576   \n",
       "5                   0.142857                         0.153888   \n",
       "6                   0.191781                         0.104886   \n",
       "7                   0.236559                         0.255573   \n",
       "8                   0.156863                         0.106560   \n",
       "9                   0.171233                         0.181787   \n",
       "10                  0.114286                         0.060680   \n",
       "11                  0.136364                         0.061248   \n",
       "12                  0.221374                         0.198382   \n",
       "13                  0.279070                         0.218506   \n",
       "14                  0.092308                         0.020612   \n",
       "15                  0.119048                         0.096304   \n",
       "16                  0.115183                         0.102896   \n",
       "17                  0.141243                         0.112985   \n",
       "\n",
       "                                               hash a  \\\n",
       "0   714b04dd8923e09ea3f370b93660441d792104140d13d3...   \n",
       "1   c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...   \n",
       "2   6691320ac50e6e039d039bc509148cc20d924199ff32ba...   \n",
       "3   42b84c175e792cbace8c18218652b0e5fc172d0722913d...   \n",
       "4   c153dc1fe06bde184f21214073db83e11ce6f526e90551...   \n",
       "5   3dd1514de75255da157c91f80bf9745485cc700f81cf40...   \n",
       "6   4a28a8f0d8c6fbee4a7aad31697d315f2e478277646094...   \n",
       "7   797c3e2456febfd1031497754c2da79b9b0cb00f770118...   \n",
       "8   3838aecf67942145b93b96e42021f369c969c3ec9b719c...   \n",
       "9   e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...   \n",
       "10  bf5cc4428ba8a20b7f40625ce7893a43aa9b3652f73d1c...   \n",
       "11  92a81921ee9378df56debc096418b3d578da70479fa3aa...   \n",
       "12  dec23aa4bf1874d80f869e877d1828ee8b0b9489aafe5c...   \n",
       "13  5b623f187b07950ce603fa64696cdb827ed276dd903ffa...   \n",
       "14  2a25f893b9a12bf8353ec30061c37287265b907b5c58ff...   \n",
       "15  283ea38cf17b09bb85b98776bcc506d1e964faa3d9b89c...   \n",
       "16  a8b15db3fcd1140dd828966011aa264433788e5732d384...   \n",
       "17  7f8cd548cf57f327155b067b8fbdbfc3b124bea38e84e5...   \n",
       "\n",
       "                                               hash b  \n",
       "0   60b992dfcad293c2fbe76d7842a4e469ba041ced7c5270...  \n",
       "1   e573296bc2021073ef94c1f60c1f097dc770793203b348...  \n",
       "2   51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...  \n",
       "3   604e277aa0ddee14c63bd188af56065e65d36880c5698d...  \n",
       "4   60707fbb129a62fd3e3a762cb9a9c8d57a8781c4d65dda...  \n",
       "5   6b25114906789fc86fa719f0394277f9289f080cbbbc7e...  \n",
       "6   f7a467afa47b3708b8ffd00f7dfecb7c4b359dc5a9c59a...  \n",
       "7   b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...  \n",
       "8   2b0771ebe6b8d46c3ce99d97700281fe561da6c7a153fc...  \n",
       "9   333765b183235b4a0645cd283edc9089a44efa12118002...  \n",
       "10  f5851c5a8dedccdfae60f3db0e8db835f5e567b6f8f88a...  \n",
       "11  f36eea089d93178672b8dfd1ed059927a0018c71f337f3...  \n",
       "12  a51392d3a9976b33ff5ec8bd300646491be0b2445376f0...  \n",
       "13  7ea4abb922d5c579d3a556e31d842a23104ff018d06975...  \n",
       "14  aae9d9d155150f9a8f8421a113a4488af0aca6f9e6cf4d...  \n",
       "15  67313a7b0c9cf4befce26c398758380e13d52c2cfcda30...  \n",
       "16  150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...  \n",
       "17  1c3c9f892ad6b9339c6c9e647691a8fd193d5e7c537909...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(selection, columns=columns)\n",
    "if not OBFUSCATE_RESULTS:\n",
    "    display(df)\n",
    "else:\n",
    "    display(pd.DataFrame([], columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "52\n",
      "Both are saying essentially the same thing.  The Forbes articles says \"as much as 20% [...] up to a maximum of $50,000\".  This means the same as what the IRS page when it says the lesser of a percentage of your income or a total of $53,000.  In other words, the $53k is a cap: you can contribute a percentage of your earnings, but you can never contribute more than $53k, even if you make so much money that 20% of your earnings would be more than that. (The difference between 20% and 25% in the two sources appears to reflect a difference in contribution limits depending on whether you are making contributions for employees, or for yourself as a self-employed individual; see Publication 560.  The difference between $50k and $53k is due to the two pages being written in different years; the limits increase each year.)\n",
      "---------------\n",
      "False\n",
      "217\n",
      "It can be a good idea to follow the advice of a guru if they have a track record of making successful investments and have a solid understanding of the markets. However, it's important to do your own research and not blindly follow anyone's advice, even if they are considered a guru. It's also a good idea to diversify your investments and not put all of your money into a single investment or investment strategy. It's always a good idea to be cautious when it comes to investing and to be aware of the risks involved.\n",
      "---------------\n",
      "True\n",
      "159\n",
      "Hi,1. The anti TB drug doses have to be prescribed as per weight of the patient. Without knowing the weight it is not possible to comment on the dosing accuracy of the drug. 2. Mantoux test is NOT a follow up test to monitor disease. It is used as a supportive evidence for diagnosis of TB in general.3. Please follow up clinically with your doctor/ chest physician.RegardsDr. Mishra\n",
      "---------------\n",
      "True\n",
      "6\n",
      "Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modeling, and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.\n",
      "In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\n",
      "The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\n",
      "---------------\n",
      "False\n",
      "166\n",
      "The conversion ratio between General Motors (GM) bonds and new GM stock will depend on the specific terms of the bond and the current market conditions. The conversion ratio is the number of shares of stock that a bondholder will receive in exchange for each bond that is converted.To find out the conversion ratio for a specific GM bond, you can try contacting the company directly or checking the bond's prospectus or offering documents. The conversion ratio may also be listed on the bond's certificate or in any periodic reports that the company files with the Securities and Exchange Commission (SEC).Alternatively, you can try consulting with a financial advisor or broker who may be able to help you determine the conversion ratio for a specific GM bond. It's important to note that the conversion ratio may change over time, so it's important to get the most current information possible.\n",
      "---------------\n",
      "True\n",
      "22\n",
      "If your counterparty sent money to a correspondent account at another bank, then it is completely up to the other bank what to do with the money. If the wire transfer completed, then the account is not closed. If I were your business partner, I would immediately contact the bank to which the transfer was made and explain the situation and hopefully they will transfer the money back. Whenever a wire transfer is made, the recipients name, address, and account number are included. If that name, address and account do not belong to you, then you have a problem because you have no legal right to the money in a court of law. For this reason, you should be avoid any situation where you are wiring money to anyone except the intended recipient.\n",
      "---------------\n",
      "False\n",
      "77\n",
      "If your deductions are higher than your income, it is possible that you may not owe any taxes. This can happen if you have a loss for the tax year, which can be calculated by subtracting your deductions from your income. If the result is a negative number, then you have a loss for the tax year.However, it's important to note that losses from some types of income, such as wages, salaries, and self-employment income, may be limited in the amount that can be claimed on your tax return. Additionally, you may be able to carry over any excess loss to future tax years and use it to offset income in those years.It's a good idea to consult with a tax professional or refer to the Internal Revenue Service (IRS) guidelines to determine how to properly report and claim losses on your tax return.\n",
      "---------------\n",
      "True\n",
      "163\n",
      "Hello, Thanks for your query.This can occur due to constipation and the white thing could have a protective mucus coating. The contraction of the large bowel to expel this can give you the pain. There can be an internal problem like an ulcer or a mass contributing to this. If you continue to get the same problems, it will be better to consult an Gastro and for further investigations like colonoscopy to rule out an internal problem.I do hope that you have found something helpful and I will be glad to answer any further query.Take care\n",
      "---------------\n",
      "False\n",
      "252\n",
      "Treatment for a spiral fracture of the ring finger usually involves immobilization with a splint or cast to allow the bone to heal. This can help to reduce pain and protect the finger while it heals.\n",
      "\n",
      "If a cast has been applied and the fracture is still present after 5 weeks, it is possible that the bone is not healing as quickly as expected. In this case, your doctor may recommend additional treatment options, such as a longer period of immobilization, physical therapy to improve range of motion and strength, or surgery to repair the fracture.\n",
      "\n",
      "It is important to follow your doctor's recommendations and attend follow-up appointments as directed. If you have any concerns about your treatment or if you experience any new symptoms, such as increased pain or swelling, you should contact your doctor for further evaluation and guidance.\n",
      "---------------\n",
      "True\n",
      "249\n",
      "Reuters has a service you can subscribe to that will give you lots of Financial information that is not readily available in common feeds.  One of the things you can find is the listing/delist dates of stocks.  There are tools to build custom reports.  That would be a report you could write.  You can probably get the data for free through their rss feeds and on their website, but the custom reports is a paid feature. FWIW re-listing(listings that have been delisted but return to a status that they can be listed again) is pretty rare.  And I can not think of too many(any actually) penny stocks that have grown to be listed on a major exchange.\n",
      "---------------\n",
      "False\n",
      "59\n",
      "It is generally not possible for an adult to be claimed as a dependent on someone else's tax return. However, the Supplemental Nutrition Assistance Program (SNAP), also known as food stamps, is a federal assistance program that provides financial assistance to low-income individuals and families to help them afford food. Eligibility for SNAP is determined by several factors, including income, household size, and resources. Claiming your father as a dependent on your tax return would not affect his eligibility for SNAP.To determine your father's eligibility for SNAP, you would need to contact your local SNAP office or visit the Department of Agriculture's website for more information. You will need to provide information about your father's income, expenses, and household composition in order to determine his eligibility for the program.\n",
      "---------------\n",
      "True\n",
      "171\n",
      "Yes, many hedge funds (for example) did not survive 2008-2009.  But hedge funds were failing both before and after that period, and other hedge funds thrived.  Those types of funds are particularly risky because they depend so much on leverage (i.e. on money that isn't actually theirs). More publically-visible funds (like those of the big-name index fund companies) tended not to close because they are not leveraged.  You say that \"a great many companies\" failed during the recession, but that's not actually true.  I can't think of more than a handful of publically-traded companies that went bankrupt.  So, since the vast majority of publically-traded companies stayed in business, their stocks kept some/most of their value, and the funds that owned those stocks stayed afloat. I personally did not see a single index fund that went out of business due to the recession.\n",
      "---------------\n",
      "False\n",
      "69\n",
      "It is possible that your dental implants could be causing or contributing to the pain and other symptoms you are experiencing in your right temple and teeth. However, it is also possible that these symptoms could be related to other underlying health conditions, such as the old minor stroke that was identified on your CT scan. It is important to discuss your symptoms with a healthcare provider, who will be able to determine the cause and recommend appropriate treatment. They may also recommend that you see a dentist or oral surgeon for further evaluation and treatment of your dental implants. It is important to follow the recommendations of your healthcare provider in order to address your symptoms and ensure the best possible outcome.\n",
      "---------------\n",
      "True\n",
      "97\n",
      "Hello, This condition could be related to arthritis (local inflammation), due to chronic degerations of the joint. I would recommend taking ibuprofen against inflammation and performing some tests (uric acid plasma levels, inflammation tests). Hope I have answered your query. Let me know if I can assist you further. Take care Regards,Dr. Ilir Sharka\n",
      "---------------\n",
      "False\n",
      "85\n",
      "Yes, it is still possible to receive a physical certificate for your stock ownership, although it is becoming increasingly rare. Most stock transactions today are electronic, with ownership represented by electronic records rather than physical certificates.If you want to receive a physical certificate for your stock, you will need to request one from the company or its transfer agent. Some companies may charge a fee for issuing a physical certificate, and there may be additional fees for mailing the certificate to you.It is important to note that owning a physical certificate does not give you any additional rights as a shareholder. The rights and privileges associated with stock ownership are the same whether you hold a physical certificate or electronic records of your ownership.\n",
      "---------------\n",
      "True\n",
      "124\n",
      "I'm a bit out of my element here, but my guess is the right way to think about this is: knowing what you do now about the underlying company (NZT), pretend they  had never offered ADR shares.  Would you buy their foreign listed shares today? Another way of looking at it would be: would you know how to sell the foreign-listed shares today if you had to do so in an emergency?   If not, I'd also push gently in the direction of selling sooner than later.\n",
      "---------------\n",
      "False\n",
      "67\n",
      "It is technically possible for individuals to engage in high-frequency trading (HFT), but it can be challenging and costly to do so. HFT involves using advanced computer algorithms and specialized software to rapidly buy and sell securities in order to take advantage of small price discrepancies. This requires a significant investment in technology and infrastructure, as well as expertise in programming and financial markets.In addition to the technical challenges, there are also regulatory and compliance issues to consider. HFT is subject to stricter regulations than other forms of trading, and individuals who engage in HFT may be required to register with regulatory bodies and adhere to specific rules.Overall, HFT is a complex and specialized activity that is generally only undertaken by professional trading firms. Individual investors who are interested in day trading stocks may be better served by using more traditional methods and focusing on longer-term trading strategies.\n",
      "---------------\n",
      "True\n",
      "5\n",
      "Use VTIVX. The \"Target Retirement 2045\" and \"Target Retirement 2045 Trust Plus\" are the same underlying fund, but the latter is offered through employers.  The only differences I see are the expense ratio and the minimum investment dollars.  But for the purposes of comparing funds, it should be pretty close. Here is the list of all of Vanguard's target retirement funds. Also, note that the \"Trust Plus\" hasn't been around as long, so you don't see the returns beyond the last few years.  That's another reason to use plain VTIVX for comparison. See also: Why doesn't a mutual fund in my 401(k) have a ticker symbol?\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row[\"GT a\"])\n",
    "    print(row[\"idx a\"])\n",
    "    print(row[\"Documents Phases 1+3\"])\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "238\n",
      "Assuming you live in the US, it is quite normal when you are applying for a loan that the application will ask you to confirm your identity. One of these methods is to ask you which of the following addresses you have lived at, with some of them being very similar (i.e. same city, or maybe even the same street). Sometimes they will ask questions and your answer would be \"None of the above.\" This is done to prevent fraudsters from applying for a loan under your identity. If you see no signs of unauthorized accounts or activities on your credit reports, and you initiated the car loan application, then you should be fine.\n",
      "---------------\n",
      "False\n",
      "234\n",
      "It is generally a good idea to use sponsorship money to pay off any debts, including student debt. This can help you to reduce the amount of money you owe and avoid accruing additional interest on your loans. However, it is important to carefully consider your financial situation and make sure that you will have enough money to cover your living expenses and other necessary costs. If you are in financial difficulty, you may be able to negotiate with your lender to arrange a repayment plan that works for you. Alternatively, you could consider seeking financial advice or guidance from a professional, such as a financial advisor or a student finance officer at your university. They will be able to help you to assess your options and make a decision that is right for your circumstances.\n",
      "---------------\n",
      "False\n",
      "161\n",
      "Credit unions are not-for-profit financial cooperatives that are owned and controlled by their members. In order to join a credit union, an individual must meet the credit union's membership requirements, which may include living or working in a certain area, being affiliated with a certain organization, or meeting other eligibility requirements. Credit unions are typically open to anyone who meets these requirements and is willing to become a member.However, it is worth noting that not all credit unions have the same membership requirements, and some credit unions may have more restrictive eligibility requirements than others. Some credit unions may be open only to members of a specific organization or group, such as a union or employer, while others may be open to anyone who lives or works in a particular region. It is important to check the specific membership requirements of a credit union before attempting to join.\n",
      "---------------\n",
      "True\n",
      "142\n",
      "You really shouldn't be using class tracking to keep business and personal operations separate. I'm pretty sure the IRS and courts frown upon this, and you're probably risking losing any limited liability you may have. And for keeping separate parts of the business separate, like say stores in a franchise, one approach would be subaccounts. Messy, I'm sure.\n",
      "---------------\n",
      "False\n",
      "255\n",
      "It is not uncommon for pain to increase after a cortisone injection. This can occur for a variety of reasons. For example, the injection may cause inflammation at the injection site, which can lead to increased pain. Additionally, the injection may temporarily increase blood flow to the area, which can cause swelling and discomfort. It is also possible that the pain is unrelated to the cortisone injection and is simply a coincidence.\n",
      "\n",
      "If you are experiencing increased pain or difficulty breathing after the cortisone injection, it is important to contact your doctor. They can assess your symptoms and determine the cause of the pain. They may recommend medications or other treatments to help manage the pain. It is also important to follow your doctor's instructions for post-injection care, such as avoiding strenuous activity or applying ice to the injection site.\n",
      "---------------\n",
      "True\n",
      "82\n",
      "As your is a very specific case, please get an advice of CA. It should not cost you much and make it easier. The sale of agriculture land is taxable in certain conditions and exempt from tax in other cases. Sale of agricultural land is subject to capital gains tax. But there are certain exemptions under Section 54B, subject to conditions, which are as follows:    If deemed taxable, you can avail indexation, ie the price at which you grandfather got [the date when he inherited it as per indexation] and pay 10% on the difference. If the price is not known, you can take the govt prescribed rate.   As there is a large deposit in your fathers account, there can be tax queries and need to be answered. Technically there is no tax liable even if your grandfather gifts the money to your father. More details at http://www.telegraphindia.com/1130401/jsp/business/story_16733007.jsp and http://www.incometaxindia.gov.in/publications/4_compute_your_capital_gains/chapter2.asp\n",
      "---------------\n",
      "False\n",
      "205\n",
      "Hallucinations can be caused by a variety of factors, including medications, underlying medical conditions, and substance abuse. The treatment for hallucinations will depend on the specific cause.\n",
      "\n",
      "If your father-in-law's hallucinations are related to Parkinson's disease, his healthcare provider may adjust his medications or prescribe additional medications to help control the hallucinations. It's important that he follows the treatment plan recommended by his healthcare provider and continues to take his medications as prescribed.\n",
      "\n",
      "If the hallucinations are due to a medication he is taking, his healthcare provider may adjust the dosage or switch to a different medication.\n",
      "\n",
      "If substance abuse is causing the hallucinations, your father-in-law may need to undergo detoxification and addiction treatment.\n",
      "\n",
      "It's important to have your father-in-law evaluated by a healthcare provider as soon as possible to determine the cause of the hallucinations and to determine the appropriate treatment.\n",
      "---------------\n",
      "True\n",
      "175\n",
      "thanks for your query, the bump could be secondary to the impact or any nasal bone fracture. consult your oral physician and get radiographs done to rule out fracture. you can take antiinflammatory drugs to reduce the swelling but cause has to be ruled out. i hope my answer will help you take care\n",
      "---------------\n",
      "False\n",
      "274\n",
      "In the United States, you will generally have to pay taxes on your profits from trading stocks. If you buy and sell a stock within a year, it will be considered a short-term capital gain and will be taxed at your ordinary income tax rate. If you hold the stock for longer than a year before selling it, it will be considered a long-term capital gain and will be taxed at a lower rate. There are limits on how much of your capital gains can be taxed at the lower rate, depending on your income. You may also have to pay taxes on dividends you receive from stocks, although there are some exceptions. It's a good idea to consult a tax professional or refer to IRS guidance if you have questions about how your stock trading will be taxed.\n",
      "---------------\n",
      "True\n",
      "289\n",
      "On what basis did you do your initial allocation of funds to each stock?   If you are 're-balancing' that implies returning things to their initial allocation.  You can do this without any research or recommendations. If you started out with say 10 stocks and 10% of the funds allocated to each stock, then re-balancing would simply be either buying/selling to return to that initial allocation.  If you are contributing to the portfolio you could adjust where the new money goes to re-balance without selling.  Or if you are drawing money from the portfolio, then you could adjust what you are selling. If on the other hand you are trying to decide if you want to alter the stocks the portfolio is HOLDING, then you have an entirely different question from 're-balancing'\n",
      "---------------\n",
      "False\n",
      "68\n",
      "Ex Machina is a 2014 science fiction film written and directed by Alex Garland. The film tells the story of a young programmer named Caleb Smith who wins a contest to spend a week at the private mountain estate of the company's reclusive CEO, Nathan Bateman. While there, Caleb is tasked with conducting a Turing test on an advanced humanoid robot named Ava to determine whether she is truly sentient. As he interacts with Ava and learns more about Nathan's experiments, Caleb begins to question the nature of consciousness and his own place in the world. The film explores themes of artificial intelligence, the ethics of creating life, and the power dynamics between humans and machines. Ex Machina received widespread critical acclaim upon its release and was nominated for several awards, including an Academy Award for Best Visual Effects.\n",
      "---------------\n",
      "True\n",
      "225\n",
      "Is that indicator can only be used for short-term trade? First of all, indicator works perfect during trends and oscillator works perfectly in the range market(or flat market). So, indicator can be used for long term, as well as short term. I mean if it is a range market, using this or any other indicator will not help much, so it you should consider market direction first. If it can be used to long-term trade, is there something I need to change from the parameters used? like, only using SMMA(5,8,13)? The parameters are there to change them. Of course you can change them based on your trading style. Considering my statement above does not mean that trading is very easy. I never use indicators alone to make trading decisions. It is always good to use oscillator to filter out bad trading signals.\n",
      "---------------\n",
      "False\n",
      "285\n",
      "Foamy feces can be caused by a variety of factors, including inflammation of the intestines, changes in the production or composition of intestinal secretions, and the presence of gas. In the case of a pneumonia patient who has been taking antibiotics, it is possible that the antibiotics may be causing changes in the gut microbiome, leading to abnormal bowel movements and foamy feces. Other factors that can contribute to foamy feces include dehydration, malabsorption of nutrients, and certain medications.\n",
      "\n",
      "It is important to discuss any changes in bowel movements with a healthcare provider, as they can provide specific guidance on the cause and treatment of the issue. In the case of a pneumonia patient, it is especially important to ensure that the underlying infection is fully resolved and that the patient is receiving proper nutrition and hydration.\n",
      "---------------\n",
      "True\n",
      "115\n",
      "Hello, It could be a contusion. As of now you can use analgesics/anti-inflammatory combination like aceclofenac/serratiopeptidase for symptomatic relief. If symptoms persists better to consult an orthopedician and get evaluated. Hope I have answered your query. Let me know if I can assist you further. Regards,Dr. Shinas Hussain,General & Family Physician\n",
      "---------------\n",
      "False\n",
      "136\n",
      "Samuel Butler was a 19th-century English novelist and satirist. He is best known for his novel \"Erewhon,\" a satirical commentary on Victorian society, and \"The Way of All Flesh,\" a semi-autobiographical novel that explores the lives of three generations of a family. Butler was also a prolific writer of prose and poetry, and his works often reflected his skeptical and cynical view of society and human nature. He was a controversial figure in his time and his work was sometimes seen as radical and ahead of its time.\n",
      "---------------\n",
      "True\n",
      "143\n",
      "HIThank for asking to HCMI really appreciate your concern the redness that you are seeing might not be due to any disease or skin lesion but this could be a natural appearance, that does not need any treatment in my opinion the Vaseline, should be stop this is not the medicine, just leave it like that only, when it is not associated with any other symptoms then why to tampered it, stop worrying about this, hope this information helps you take care and have nice day.\n",
      "---------------\n",
      "False\n",
      "212\n",
      "Predictive analytics is a type of data analysis that involves using historical data and machine learning algorithms to make predictions about future events or outcomes. It involves building statistical models that can identify patterns and relationships in data, and using those models to predict what will happen in the future. Predictive analytics can be used in a variety of industries, including finance, healthcare, marketing, and manufacturing, to make informed decisions and optimize business processes.\n",
      "\n",
      "\n",
      "\n",
      "Some common applications of predictive analytics include:\n",
      "\n",
      "\n",
      "\n",
      "Forecasting demand for a product or service\n",
      "\n",
      "Identifying potential customer churn\n",
      "\n",
      "Predicting equipment failures\n",
      "\n",
      "Detecting fraudulent activity\n",
      "\n",
      "Estimating the likelihood of an event occurring\n",
      "\n",
      "To build predictive models, data scientists typically use tools such as machine learning algorithms, statistical analysis software, and data visualization tools. They may also use techniques such as regression analysis, decision trees, and clustering to analyze and interpret the data.\n",
      "---------------\n",
      "True\n",
      "121\n",
      "The reason is, stores want customers to use cash.   By giving us cash, we are more likely to use cash next time. I feel a little guilty when using my bank card at the store because I know I'm giving about 2-3% of the sale to the bank.  Unless I don't really like where I'm shopping (ie Walmart), I try to use cash if I have it.  I doubt these large stores pay extra for supplying the cash portion.  They just need to keep the cash onhand. In other countries, do they not mind paying banks a percentage of each transaction?  That's a huge loss for retailers.  (I also heard tipping isn't popular in some countries, maybe the lack of regard for vendors is related somehow??) Oh, plus, it's a value added service.  A customer is more likely to return to a store if they provide this service.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row[\"GT b\"])\n",
    "    print(row[\"idx b\"])\n",
    "    print(row[\"Documents Phases 2+4\"])\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any(df[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].stack().reset_index(drop=\"True\").duplicated()), \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df.groupby([\"Detector\", \"Explainer\"]).count()[\"Documents Phases 1+3\"] == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OBFUSCATE_RESULTS:\n",
    "    df.to_pickle(\"./dataset_user_study_mock.pkl\") # file in .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove from .gitignore after user study\n",
    "# TODO change format to something else after user study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "detector = DetectorRadford()\n",
    "explainer = SHAP_Explainer(detector)\n",
    "selection =[]\n",
    "\n",
    "candidates = list(range(0,len(documents)))\n",
    "random.shuffle(candidates)\n",
    "candidates = candidates[0:2*len(df)]\n",
    "assert len(candidates) == 2*len(df)\n",
    "\n",
    "while len(candidates) >= 2:\n",
    " \n",
    "    pair = (candidates[0], candidates[1])\n",
    "    \n",
    "    candidates = candidates[2:]\n",
    "    \n",
    "    #print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer)\n",
    "    update_selection([pair], explainer, detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = pd.DataFrame(selection, columns=columns)\n",
    "df_random.groupby(\"Explainer\")[[\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]].mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Explainer\")[[\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]].mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_ind\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Jaccard Similarity (a,b)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random[\"Jaccard Similarity (a,b)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(df[\"Jaccard Similarity (a,b)\"], df_random[\"Jaccard Similarity (a,b)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]:\n",
    "    print(ttest_rel(df[metric], df_random[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = documents[document_ids[27]]\n",
    "b = documents[document_ids[281]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation BOW Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim_in_W(dff):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class()\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in dff.iterrows():\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = cosine_similarity(W) \n",
    "                sim = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Cosine Similarity a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_random = get_cos_sim_in_W(df_random)\n",
    "df_fi_similarity = get_cos_sim_in_W(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity[\"Cosine Similarity a,b in W\"], df_fi_similarity_random[\"Cosine Similarity a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eucledian_distance_in_W(dff):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford]:\n",
    "        detector = detector_class()\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in dff.iterrows():\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "                sim = euclidean_distances(W) \n",
    "                sim = sim[0,1]\n",
    "                results.append((explainer.__class__.__name__, detector.__class__.__name__, sim))\n",
    "    df_fi_similarity = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"Eucledian Distance a,b in W\"])\n",
    "    df_fi_similarity = df_fi_similarity.set_index([\"Explainer\", \"Detector\"])\n",
    "    return df_fi_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random = get_eucledian_distance_in_W(df_random)\n",
    "df_fi_similarity_eucledian = get_eucledian_distance_in_W(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_similarity_eucledian_random.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df_fi_similarity_eucledian[\"Eucledian Distance a,b in W\"], df_fi_similarity_eucledian_random[\"Eucledian Distance a,b in W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Spacy Semantic Similarity: Cosine Similarity Average of Word Vectors (a,b)\", \"Jaccard Similarity (a,b)\", \"Cosine Similarity tfidf Vectors\"]:\n",
    "    print(metric)\n",
    "    print(\"     \", ttest_rel(df[metric], df_random[metric]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
