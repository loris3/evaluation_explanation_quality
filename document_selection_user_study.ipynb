{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAVED_DF = False # set to false to overwrite existing dataframe\n",
    "SAVE_PATH = \"./dataset_user_study.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "import lime\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import random\n",
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_ind\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Feature Importance Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix of explanations for all documents in \"data\"\n",
    "# This function was once SubmodularPick.__init__() in LIME. It was planned to use its output for a search strategy for similar explanations. \n",
    "# Only the code for creating W from the paper (rows are explanations, cols are BOW features) remains\n",
    "# This also now treats positive and negative FI scores as their own features to ease search\n",
    "def get_explanation_matrix_W(data, explainer, quiet=False):\n",
    "    # Get (cached) explanations \n",
    "    explanations_and_documents = [(d, explainer.get_fi_scores(d, fill=True)[0]) for d in tqdm(data, desc=\"Loading all explanations\",disable=quiet) ] # [0]: only irt to label machine, fill: return all words, even those with 0 fi\n",
    "\n",
    "    get_feature_name_signed = lambda feature,value : feature + (\"_+\" if value >=0 else \"_-\") # appends \"_+\" or \"_-\" to each feature name, e.g. \"example\" -> \"example_+\" if fi(example) > 0\n",
    "    # Ribeiro et al.: Find all the explanation model features used. Defines the dimension d'\n",
    "    # i.e. determine columns of W: each word (BOW) gets (up to) two columns, one for positive FI scores, one for negative FI scores\n",
    "    features_dict = {}\n",
    "    feature_iter = 0\n",
    "    for d, exp in tqdm(explanations_and_documents, desc=\"Building global dict of features\", disable=quiet):\n",
    "     #   print(\"exp\",exp)\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            feature_name = get_feature_name_signed(feature,value) # get_feature_name_signed: see above\n",
    "            if feature_name not in features_dict.keys():\n",
    "                features_dict[feature_name] = (feature_iter)\n",
    "                feature_iter += 1\n",
    "    d_prime = len(features_dict.keys())\n",
    "\n",
    "    # Ribeiro et al.: Create the n x d' dimensional 'explanation matrix', W\n",
    "    W = np.zeros((len(explanations_and_documents), d_prime))\n",
    "\n",
    "    # fill W, look up cols in dict that was just created\n",
    "    # W: one row per explanation, one col per feature in feature_dict\n",
    "    for i, (d, exp) in enumerate(tqdm(explanations_and_documents,  desc=\"Building W\",disable=quiet)):\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            # get_feature_name_signed: see above\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            W[i, features_dict[get_feature_name_signed(feature,value)]] += value\n",
    "    return W, features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples: (pair of documents whose explanations are similar, the features that overlap, fi scores of said features)\n",
    "# this maximizes similarity between documents (greedy, each document is only part of one tuple)\n",
    "# Another function will select n tuples to maximize coverage in explanation space akin to SP-LIME later on\n",
    "sum_two_max = None\n",
    "def get_pairs(documents, W, detector, features_dict, n_pairs=None):\n",
    "    if n_pairs is None:\n",
    "        n_pairs = len(documents)//2\n",
    "    idx_pairs = [] # tuples of indices of similar documents a,b in \"data\"\n",
    "    features = [] # list of features those documents covered\n",
    "    fi_scores_pairs = []\n",
    "\n",
    "    W_ = np.copy(W)\n",
    "\n",
    "    document_indices = np.arange(0, W_.shape[0])\n",
    "    for _ in tqdm(range(0,n_pairs), desc=\"Obtaining pairs\"):\n",
    "        sim = cosine_similarity(W_) # calculate cosine similarity between all explanations\n",
    "        sim = np.triu(sim,k=1)  # remove redundant information for argmax()\n",
    "\n",
    "        idx_max = np.unravel_index(sim.argmax(), sim.shape) # get most similar pair, result is (idx_a, idx_b)\n",
    "        features_non_zero_in_both = np.intersect1d(W_[idx_max[0]].nonzero(),W_[idx_max[1]].nonzero()) # get features that have non-zero fi in both explanations\n",
    "                                                                                                      # is used later for selecting a set of tuples with high coverage (as in SP-LIME)\n",
    "        non_zero_features = [] # list with features that will be returned\n",
    "        non_zero_fi_scores_tuples = [] # list of tuples with fi scores in a and b that will be returned\n",
    "    \n",
    "        # look up feature_idxs in features_dict and append them to the output\n",
    "        for iii in features_non_zero_in_both:\n",
    "           key = list(features_dict.keys())[list(features_dict.values()).index(iii)]\n",
    "           non_zero_features.append(key)\n",
    "           non_zero_fi_scores_tuples.append((W_[idx_max[0],features_dict[key]], W_[idx_max[1],features_dict[key]]))\n",
    "        \n",
    "        # Only add pair to output list if valid: \n",
    "        # -> at least one common feature is not zero \n",
    "        #            AND \n",
    "        # -> f(a) == f(b) (i.e., the explanation is arguing for the same detector verdict)\n",
    "        if len(non_zero_features) > 0:\n",
    "            a,b = detector.predict_label([documents[document_indices[idx_max[0]]], documents[document_indices[idx_max[1]]]])\n",
    "            if a == b:\n",
    "                idx_pairs.append(document_indices[list(idx_max)])\n",
    "                fi_scores_pairs.append(non_zero_fi_scores_tuples)\n",
    "                features.append(non_zero_features)\n",
    "        # delete pair from W_:\n",
    "        W_ = np.delete(W_, idx_max, axis=0) \n",
    "        document_indices = np.delete(document_indices, list(idx_max))\n",
    "\n",
    "    return idx_pairs, features, fi_scores_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have pairs of similar explanations now. But want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "# This is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# --> But: if not limited to the top k most similar pairs, a maximum coverage algorithm will select the least similar pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "# this is the maximum coverage problem: \n",
    "# implementing a greedy algorithm here: R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "#   \"In order to achieve a maximal cover for p facilities under a given service distance, \n",
    "#   the algorithm starts with an empty solution set and then adds to this set one at a \n",
    "#   time the best facility sites. The GA algorithm picks for the first facility that \n",
    "#   site which covers the most of the total population. For the second facility, GA \n",
    "#   picks the site that covers the most of the population not covered by the first \n",
    "#   facility. Then, for the third facility, GA picks the site that covers the most of the \n",
    "#   population not covered by the first and second facilities. This process is continued until either p facilities have been selected or all the population is covered. \n",
    "#   Details of the algorithm are given in Church.\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n",
    "def get_site_with_max_coverage(sites, previous_selections, W):\n",
    "    best_site = None\n",
    "    best_coverage = 0\n",
    "    for site in sites:\n",
    "        candidate = set(np.array(previous_selections).flatten()).union(site) # extend the previous selection by \"site\", this addresses \"For the second facility, GA picks the site that covers the most of the population not covered by the first \"\n",
    "        cov = coverage(candidate, W) # compute new coverage\n",
    "        if cov >= best_coverage:\n",
    "            best_coverage = cov\n",
    "            best_site = site\n",
    "    return best_site, best_coverage\n",
    "\n",
    "def get_p_tuples_with_high_coverage(indices, W, p=10):\n",
    "  sites = list(indices)\n",
    "  # \"the algorithm start with emty solution set\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "  result = list()\n",
    "  # \"and then adds to this set one at a time the best facility sites\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "  while True:\n",
    "      # \"The GA algorithm picks for the first facility that \n",
    "      # site which covers the most of the total population\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "      best_site, best_coverage = get_site_with_max_coverage(sites, result, W)\n",
    "      result.append(best_site)\n",
    "      # \"This process is continued until either p facilities have been selected or all the population is covered.\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "      if len(result) == p or best_coverage == W.shape[1]:\n",
    "          break\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two pairs, one for f(x) = machine and one for f(x) = human\n",
    "# checks texts_already_selected and chooses the next best pair (for each class) if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair)\n",
    "def obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    W, features_dict = get_explanation_matrix_W(documents, explainer)\n",
    "    similar_pairs, _, _ = get_pairs(documents, W, detector, features_dict)\n",
    "    # want a dataset that is balanced irt to the two base classes:\n",
    "    # two pairs will be returned, one with f(x) == machine, and one with f(x) == human\n",
    "    top_10_pairs_human = []\n",
    "    top_10_pairs_machine = []\n",
    "    for pair in similar_pairs:\n",
    "        if (documents[pair[0]] in texts_already_selected) or (documents[pair[1]] in texts_already_selected):\n",
    "            continue\n",
    "        if detector.predict_label([documents[pair[0]]])[0]:\n",
    "            top_10_pairs_human.append(pair)\n",
    "        else:\n",
    "            top_10_pairs_machine.append(pair)\n",
    "        if len(top_10_pairs_human) >= 10 and len(top_10_pairs_machine) >= 10:\n",
    "            top_10_pairs_human = top_10_pairs_human[0:10]\n",
    "            top_10_pairs_machine = top_10_pairs_machine[0:10]\n",
    "            break\n",
    "    pairs_human = get_p_tuples_with_high_coverage(top_10_pairs_human, W, p=3)\n",
    "    pairs_machine = get_p_tuples_with_high_coverage(top_10_pairs_machine, W, p=3)\n",
    "    return pairs_human + pairs_machine    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Rule-Based Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document_a, document_b):\n",
    "    # case sensitive, on spacy tokens\n",
    "    a = list(chain(*[[token.text for token in sent] for sent in nlp(document_a).sents]))\n",
    "    b = list(chain(*[[token.text for token in sent] for sent in nlp(document_b).sents]))\n",
    "    intersection = float(len(list(set(a).intersection(b))))\n",
    "    union = float((len(set(a)) + len(set(b)))) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the order of occurence in a list of words, e.g.:\n",
    "# [\"example\", \"test\", \"example\", \"one\"] -> ['example_0', 'test_0', 'example_1', 'one_0']\n",
    "def encode_count(list_of_words):\n",
    "    d = defaultdict(lambda : 0)\n",
    "    encoded = []\n",
    "    for word in list_of_words:\n",
    "        encoded.append(word + \"_\" + str(d[word]))\n",
    "        d[word] +=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonary Anchors returns can define multiple anchors:\n",
    "# {this, is, an, example} : 0.9\n",
    "# {this, is, an}: 0.8\n",
    "# {this, is, }: 0.75\n",
    "# {this}: 0.4\n",
    "# This function returns all of them, note that sets with theta < threshold are technically no longer anchors by the definition\n",
    "def get_anchors_at_each_k(documents, explainer, quiet=False):\n",
    "    anchors = []\n",
    "    p = []\n",
    "    ids = []\n",
    "    for i,_ in tqdm(enumerate(documents), desc=\"Loading all explanations\", disable=quiet):\n",
    "        exp = explainer.get_explanation_cached(documents[i])\n",
    "        exp[\"names\"] = encode_count(exp[\"names\"]) # Anchors is not BOW. But the algorithm is written with python set()s\n",
    "        while len(exp[\"mean\"]) >=1:#and exp[\"mean\"][-1] >= 0.75:\n",
    "            anchors.append(set(exp[\"names\"])) \n",
    "            p.append(exp[\"mean\"][-1])\n",
    "            ids.append(i)\n",
    "\n",
    "            exp[\"mean\"].pop()\n",
    "            exp[\"names\"].pop()\n",
    "    return anchors, p, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for pairs of anchors\n",
    "# returns pairs of documents, the same number for f(x) = machine and f(x) = human, both sampled randomly\n",
    "# prints a warning if there are not enough for either class and returns additional samples from the other class if so\n",
    "# checks for and skips documents in \"texts_already_selected\" (i.e. it was already selected for this detector dataset)\n",
    "def obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    anchors, p, ids = get_anchors_at_each_k(documents, explainer)\n",
    "    # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "    duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "    # get the ids and p for each duplicate_anchor in  duplicate_anchors\n",
    "    # \"candidates\" is a list of lists with ids (and all other details) of each duplicate_anchor\n",
    "    candidates = [[(anchor, p, document_id) for anchor, p, document_id in zip(anchors, p, ids) if anchor == duplicate_anchor] for duplicate_anchor in duplicate_anchors ]\n",
    "    # now check for each paring of the documents in each sublist of \"candidates\":\n",
    "    #   is f(a) == f(b)?, if not: discard\n",
    "    # then pick pair with highest jaccard_score on the original documents (NOT ANCHORS!) in each \"candidate\"\n",
    "    pairs = []\n",
    "\n",
    "    predictions_cache = {}\n",
    "    def cached_predict(idx):\n",
    "        if idx not in predictions_cache:\n",
    "            predictions_cache[idx] = detector.predict_label([documents[idx]])[0]\n",
    "        return predictions_cache[idx]\n",
    "    for candidate in tqdm(candidates, desc=\"Assessing candidates\",position=1):\n",
    "        \n",
    "        anchor_s, p, ids  = zip(*candidate)\n",
    "        c = list(combinations(ids, 2))\n",
    "        c = [(a,b) for a,b in c if cached_predict(a) == cached_predict(b) if not (documents[a] in texts_already_selected) or (documents[b] in texts_already_selected)]\n",
    "        if len(c) == 0:\n",
    "            continue\n",
    "        jaccard_scores = [(a,b, jaccard_similarity(documents[a], documents[b])) for a,b in tqdm(c, desc=\"Calculating Jaccard Similarity (of documents not Anchors)\",position=0)]\n",
    "        a,b, score = max(jaccard_scores, key=lambda x: x[2])\n",
    "        pairs.append((a,b))\n",
    "\n",
    "    # sample twice: once for f(x) == human and once for f(x) == machine. f(a) == f(b) is tested earlier\n",
    "\n",
    "    predictions = [cached_predict(a) for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "    predictions_ = np.array(predictions)\n",
    "    pairs_ = np.array(pairs)\n",
    "\n",
    "    machine = pairs_[predictions_ == False]\n",
    " \n",
    "    human = pairs_[predictions_ == True]\n",
    "   \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    result = []\n",
    "    # one explainer (DetectGPT) has no explanations for f(x) = human: return additional samples for machine\n",
    "    if not(True in predictions) or human.shape[0] < 3:\n",
    "        print(\"Warning: Not enough examples for f(x) = human. Returning additional examples for machine\")\n",
    "        result =  list(machine[np.random.choice(machine.shape[0], 6-human.shape[0], replace=False)]) + list(human[np.random.choice(human.shape[0], human.shape[0], replace=False)])\n",
    "    elif not (False in predictions) or machine.shape[0] < 3:\n",
    "        print(\"Warning: Not enough examples for f(x) = machine. Returning additional examples for human\")\n",
    "        result = list(machine[np.random.choice(machine.shape[0], machine.shape[0], replace=False)]) + list(human[np.random.choice(human.shape[0], 6 -machine.shape[0], replace=False)])\n",
    "    else:\n",
    "        result =  list(machine[np.random.choice(machine.shape[0], 3, replace=False)]) + list(human[np.random.choice(human.shape[0], 3, replace=False)]) # returns a random pair for machine and a random pair for human\n",
    "\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper\n",
    "def obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        return obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)\n",
    "    else:\n",
    "        return obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Document Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test \n",
    "\n",
    "documents = list(test[\"answer\"])\n",
    "gold_labels = list(test[\"author\"] == \"human_answers\") # convention: 0: machine, 1: human, see detector.py\n",
    "document_ids = list(range(0,len(documents))) # note that the search algorithms don't use these ids. They are only used for printing and the exclude_list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") # used to calculate metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some documents are excluded from the user-study for the reasons specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids as in the \"idx a\" and \"idx b\" columns in the final dataframe\n",
    "# to exclude a document, add the index here after generation and re-run this notebook\n",
    "exclude_list = {\n",
    "    (288,117, 188, 110, 159, 97, 105, 115,266, 158, 4,263,195): \"Contains personal information/Author introduces themselves by name\",\n",
    "    (190,294,16,): \"Names (other) forum user\",\n",
    "    (27,103,): \"NSFW\",\n",
    "    \n",
    "    \n",
    "}\n",
    "exclude_list = [x for xs in [ list(key) for key in exclude_list.keys()] for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply exclude_list\n",
    "documents = [d for i,d in zip(document_ids, documents) if i not in exclude_list]\n",
    "gold_labels = [gl for i,gl in zip(document_ids, gold_labels) if i not in exclude_list]\n",
    "document_ids = [i for i in document_ids if i not in exclude_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Detector\", \"Explainer\", \"Documents Phases 1+3\", \"Documents Phases 2+4\", \"f(a)\", \"f(b)\", \"GT a\", \"GT b\", \"idx a\", \"idx b\", \"Spacy Similarity\", \"Jaccard Similarity\", \"Cosine Similarity tfidf\",\"hash a\", \"hash b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds row to list later to be converted to a df\n",
    "def update_selection(selection, pairs, explainer, detector):\n",
    "    for a,b in pairs:\n",
    "        tfidf_= tfidf.transform([documents[a], documents[b]])   \n",
    "        selection.append((detector.__class__.__name__,\n",
    "                        explainer.__class__.__name__,\n",
    "                        documents[a], documents[b],\n",
    "                        *detector.predict_label([documents[a], documents[b]]),\n",
    "                        gold_labels[a],\n",
    "                        gold_labels[b],\n",
    "                        document_ids[a],\n",
    "                        document_ids[b],\n",
    "                        nlp(documents[a]).similarity(nlp(documents[b])),\n",
    "                        jaccard_similarity(documents[a], documents[b]),\n",
    "                        (tfidf_ * tfidf_.T).toarray()[0,1],\n",
    "                        explainer.get_hash(documents[a]),\n",
    "                        explainer.get_hash(documents[b])))\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.20s)\n",
      "DONE (0.07s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>DetectorDetectGPT</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Anchor_Explainer</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all explanations: 287it [00:05, 55.23it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 22.57it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 16.53it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 21.56it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 21.38it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 20.86it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 11/11 [00:00<00:00, 21.50it/s]\n",
      "\n",
      "Assessing candidates:  23%|██▎       | 6/26 [00:24<01:31,  4.55s/it]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 6/6 [00:00<00:00, 25.86it/s]\n",
      "\n",
      "Assessing candidates:  31%|███       | 8/26 [00:30<01:05,  3.66s/it]\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 19.55it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 21/21 [00:00<00:00, 21.83it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 1/1 [00:00<00:00, 18.34it/s]\n",
      "\n",
      "Calculating Jaccard Similarity (of documents not Anchors): 100%|██████████| 10/10 [00:00<00:00, 22.59it/s]\n",
      "\n",
      "Assessing candidates:  50%|█████     | 13/26 [00:52<00:58,  4.49s/it]"
     ]
    }
   ],
   "source": [
    "if not USE_SAVED_DF:\n",
    "    selection = []\n",
    "    for detector_class in [DetectorDetectGPT,DetectorRadford,DetectorGuo]:\n",
    "        selection_detector = []\n",
    "        detector = detector_class()\n",
    "        display(HTML(\"<h1>{}</h1>\".format(detector.__class__.__name__)))\n",
    "        for explainer_class in [Anchor_Explainer, LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            display(HTML(\"<h2>{}</h2>\".format(explainer.__class__.__name__)))\n",
    "            texts_already_selected = []\n",
    "            if len(selection_detector) > 0:\n",
    "                texts_already_selected = list(zip(*selection_detector))[2] + list(zip(*selection_detector))[3]\n",
    "            pairs = obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected=texts_already_selected)\n",
    "            selection_detector = update_selection(selection_detector, pairs, explainer, detector)\n",
    "        selection = selection + selection_detector\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_SAVED_DF:\n",
    "    df = pd.DataFrame(selection, columns=columns)\n",
    "    df.to_csv(SAVE_PATH, encoding=\"utf8\") # file in .gitignore\n",
    "df = pd.read_csv(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.groupby([\"Detector\", \"Explainer\"])[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].apply(lambda group: len(group.stack()[group.stack().duplicated(keep=False)])).sum() == 0, \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.groupby([\"Detector\"])[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].apply(lambda group: len(group.stack()[group.stack().duplicated(keep=False)])).sum() == 0, \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove dataset from .gitignore after user study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to the user study UI\n",
    "%run export_user_study.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cache = {}\n",
    "def prediction_cached(detector, document):\n",
    "    id = (detector.__class__.__name__,document)\n",
    "    if id not in prediction_cache:\n",
    "        prediction_cache[id] = detector.predict_label([document])[0]\n",
    "    return prediction_cache[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RANDOM_SELECTIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.06s)\n",
      "DONE (0.07s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detector_detectgpt = DetectorDetectGPT()\n",
    "detector_radford = DetectorRadford()\n",
    "detector_guo = DetectorGuo()\n",
    "# returns a random selection that has the same shape and splits as the one obtained with the selection strategy\n",
    "def get_random_df(df, seed=42):\n",
    "    selection = []\n",
    "    random.seed(seed)\n",
    "    for idx, row in tqdm(list(df.iterrows())):\n",
    "        detector = None\n",
    "        explainer = None\n",
    "        if row[\"Detector\"] == \"DetectorDetectGPT\":\n",
    "            detector = detector_detectgpt\n",
    "        if row[\"Detector\"] == \"DetectorRadford\":\n",
    "            detector = detector_radford\n",
    "        if row[\"Detector\"] == \"DetectorGuo\":\n",
    "            detector = detector_guo\n",
    "\n",
    "\n",
    "        if row[\"Explainer\"]  == \"Anchor_Explainer\":\n",
    "            explainer = Anchor_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"LIME_Explainer\":\n",
    "            explainer = LIME_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"SHAP_Explainer\":\n",
    "            explainer = SHAP_Explainer(detector)\n",
    "        #               all documents not in exclude_list                                                                            without replacement\n",
    "        candidates = [i for i in range(0,len(documents)) if i not in exclude_list and (len(selection) == 0 or document_ids[i] not in list(zip(*selection))[8] + list(zip(*selection))[9])]\n",
    "        random.shuffle(candidates)\n",
    "        pairs = None\n",
    "        while True:\n",
    "            pairs = [(candidates[0], candidates[1])]\n",
    "            if prediction_cached(detector, documents[candidates[0]]) == row[\"f(a)\"] and prediction_cached(detector, documents[candidates[0]]) == prediction_cached(detector, documents[candidates[1]]):\n",
    "                break\n",
    "            candidates = candidates[2:]\n",
    "        \n",
    "\n",
    "        selection = update_selection(selection, pairs, explainer, detector)\n",
    "    return pd.DataFrame(selection, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [04:47<00:00,  5.33s/it]\n",
      "100%|██████████| 54/54 [02:25<00:00,  2.70s/it]\n",
      "100%|██████████| 54/54 [01:44<00:00,  1.94s/it]\n",
      "100%|██████████| 54/54 [01:51<00:00,  2.07s/it]\n",
      "100%|██████████| 54/54 [01:36<00:00,  1.80s/it]\n",
      "100%|██████████| 54/54 [01:40<00:00,  1.86s/it]\n",
      "100%|██████████| 54/54 [01:37<00:00,  1.81s/it]\n",
      "100%|██████████| 54/54 [01:38<00:00,  1.83s/it]\n",
      "100%|██████████| 54/54 [01:35<00:00,  1.76s/it]\n",
      "100%|██████████| 54/54 [01:37<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "dfs_random = [get_random_df(df, seed=i) for i in range(0,N_RANDOM_SELECTIONS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables for LateX\n",
    "Calculates document and explanation similarity metrics. Also compares them to random selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_FI(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in tqdm(list(df.iterrows())):\n",
    "            #    print(row)\n",
    "                if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "\n",
    "                sim = cosine_similarity(W) \n",
    "                cosine_similarity_ = sim[0,1]\n",
    "\n",
    "                n_tokens_overlap_in_w = np.all(W != 0, axis = 0).sum()# / np.any(W != 0, axis = 0).sum()\n",
    "                \n",
    "\n",
    "                results.append((\n",
    "                    idx,\n",
    "                    explainer.__class__.__name__,\n",
    "                    detector.__class__.__name__,\n",
    "                    cosine_similarity_,\n",
    "                    n_tokens_overlap_in_w,\n",
    "\n",
    "                 ))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"Cosine Similarity\",\n",
    "        \"\\\\# Common Features\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_Anchor(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "\n",
    "        explainer = Anchor_Explainer(detector)\n",
    "        for idx, row in tqdm(list(df.iterrows())):\n",
    "        #    print(row)\n",
    "            if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                continue\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            anchors, p, ids = get_anchors_at_each_k([a,b], explainer, quiet=True)\n",
    "            # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "            duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "            results.append((\n",
    "                idx,\n",
    "                explainer.__class__.__name__,\n",
    "                detector.__class__.__name__,\n",
    "                len(duplicate_anchors),\n",
    "                max([len(anchor) for anchor in duplicate_anchors]) if len(duplicate_anchors) else 0,\n",
    "                p[anchors.index(max(duplicate_anchors, key=lambda anchor: len(anchor)))] if len(duplicate_anchors) else 0\n",
    "                ))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"\\\\# Matching Anchors\",\n",
    "        \"Len Longest Matching Anchor\",\n",
    "        \"$\\\\theta$ Longest Matching Anchor\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_Document(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer, Anchor_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in tqdm(list(df.iterrows())):\n",
    "            #    print(row)\n",
    "                if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "            \n",
    "                results.append((\n",
    "                    idx,\n",
    "                    explainer.__class__.__name__,\n",
    "                    detector.__class__.__name__,\n",
    "                        row[\"Spacy Similarity\"],\n",
    "                        row[\"Jaccard Similarity\"],\n",
    "                        row[\"Cosine Similarity tfidf\"]))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"Spacy Similarity\",\n",
    "        \"Jaccard Similarity\",\n",
    "        \"Cosine Similarity tfidf\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Metric\", \"tstatistic\", \"pvalue\",\"Mean of Method\", \"Mean of {} Rand. Selections\".format(N_RANDOM_SELECTIONS), \"Gain Over Random\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and aggregate results by detector\n",
    "def get_results_detector_level(m_method, m_random):\n",
    "    t = []\n",
    "    for metric in m_method.columns:\n",
    "        for (detector, group_method), ((detector_r), group_random) in zip(m_method.groupby([\"Set\"]), m_random.groupby([\"Set\"])):\n",
    "            assert detector == detector_r\n",
    "            tstatistic, pvalue = ttest_ind(group_method[metric], group_random[metric])\n",
    "\n",
    "            t.append([detector[0], metric[0], tstatistic, pvalue, group_method[metric].mean(), group_random[metric].mean(), group_method[metric].mean() - group_random[metric].mean()])\n",
    "    df_results_detector_level = pd.DataFrame(t, columns=[\"Set\"]+columns).set_index([\"Metric\", \"Set\"])#.apply(get_p_asterisks_2samp).drop([\"pvalue\",\"tstatistic\"], axis=1)\n",
    "    return df_results_detector_level.reset_index().set_index([\"Metric\", \"Set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for entire selection\n",
    "def get_results_entire_selection(m_method, m_random):\n",
    "        t = []\n",
    "        for metric in m_method.columns:\n",
    "                tstatistic, pvalue = ttest_ind(m_method[metric], m_random[metric])\n",
    "                t.append([metric[0],  tstatistic, pvalue, m_method[metric].mean(), m_random[metric].mean(), m_method[metric].mean() - m_random[metric].mean()])\n",
    "        df_results_selection_level = pd.DataFrame(t, columns=columns)#.apply(get_p_asterisks_2samp).drop([\"pvalue\",\"tstatistic\"], axis=1)\n",
    "        # add additional descriptions\n",
    "        df_results_selection_level[\"Set\"] = \"All\"\n",
    "        return df_results_selection_level.reset_index().set_index([\"Metric\", \"Set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_significant(row, props=''):\n",
    "  #  display(s)\n",
    "    styles = [''] * len(row)\n",
    "    styles[-1] = 'font-weight: bold' if row[\"pvalue\"] <= 0.05 else ''\n",
    "    return styles\n",
    "def shade_by_type(row, props=''):\n",
    "\n",
    "  if(row.name[0] == \"Explanation Similarity in W\"):\n",
    "    return ['background-color:red'] * len(row)\n",
    "  else:\n",
    "    return [''] * len(row)\n",
    "def shade_by_type_index(row, props=''):\n",
    "  return ['background-color:red'] * 8 + [''] * 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<?, ?it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 54158.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 106434.41it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53939.61it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53901.10it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53204.70it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 54068.37it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 54016.79it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53952.46it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 82973.37it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 71731.56it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 82773.24it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 95752.27it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 97697.63it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 90013.68it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 119685.28it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 107530.94it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 71799.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_db502_row0_col5, #T_db502_row2_col5, #T_db502_row3_col5, #T_db502_row4_col5, #T_db502_row6_col5, #T_db502_row7_col5, #T_db502_row8_col5, #T_db502_row9_col5, #T_db502_row10_col5, #T_db502_row11_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_db502_level0_col3, #T_db502_level0_col4, #T_db502_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_db502\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_db502_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_db502_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_db502_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">Spacy Similarity</th>\n",
       "      <th id=\"T_db502_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_db502_row0_col3\" class=\"data row0 col3\" >0.93</td>\n",
       "      <td id=\"T_db502_row0_col4\" class=\"data row0 col4\" >0.88</td>\n",
       "      <td id=\"T_db502_row0_col5\" class=\"data row0 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_db502_row1_col3\" class=\"data row1 col3\" >0.90</td>\n",
       "      <td id=\"T_db502_row1_col4\" class=\"data row1 col4\" >0.88</td>\n",
       "      <td id=\"T_db502_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_db502_row2_col3\" class=\"data row2 col3\" >0.92</td>\n",
       "      <td id=\"T_db502_row2_col4\" class=\"data row2 col4\" >0.88</td>\n",
       "      <td id=\"T_db502_row2_col5\" class=\"data row2 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_db502_row3_col3\" class=\"data row3 col3\" >0.92</td>\n",
       "      <td id=\"T_db502_row3_col4\" class=\"data row3 col4\" >0.88</td>\n",
       "      <td id=\"T_db502_row3_col5\" class=\"data row3 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Jaccard Similarity</th>\n",
       "      <th id=\"T_db502_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_db502_row4_col3\" class=\"data row4 col3\" >0.17</td>\n",
       "      <td id=\"T_db502_row4_col4\" class=\"data row4 col4\" >0.12</td>\n",
       "      <td id=\"T_db502_row4_col5\" class=\"data row4 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_db502_row5_col3\" class=\"data row5 col3\" >0.13</td>\n",
       "      <td id=\"T_db502_row5_col4\" class=\"data row5 col4\" >0.12</td>\n",
       "      <td id=\"T_db502_row5_col5\" class=\"data row5 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_db502_row6_col3\" class=\"data row6 col3\" >0.14</td>\n",
       "      <td id=\"T_db502_row6_col4\" class=\"data row6 col4\" >0.12</td>\n",
       "      <td id=\"T_db502_row6_col5\" class=\"data row6 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_db502_row7_col3\" class=\"data row7 col3\" >0.15</td>\n",
       "      <td id=\"T_db502_row7_col4\" class=\"data row7 col4\" >0.12</td>\n",
       "      <td id=\"T_db502_row7_col5\" class=\"data row7 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level0_row8\" class=\"row_heading level0 row8\" rowspan=\"4\">Cosine Similarity tfidf</th>\n",
       "      <th id=\"T_db502_level1_row8\" class=\"row_heading level1 row8\" >DetectorRadford</th>\n",
       "      <td id=\"T_db502_row8_col3\" class=\"data row8 col3\" >0.13</td>\n",
       "      <td id=\"T_db502_row8_col4\" class=\"data row8 col4\" >0.09</td>\n",
       "      <td id=\"T_db502_row8_col5\" class=\"data row8 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row9\" class=\"row_heading level1 row9\" >DetectorGuo</th>\n",
       "      <td id=\"T_db502_row9_col3\" class=\"data row9 col3\" >0.10</td>\n",
       "      <td id=\"T_db502_row9_col4\" class=\"data row9 col4\" >0.09</td>\n",
       "      <td id=\"T_db502_row9_col5\" class=\"data row9 col5\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row10\" class=\"row_heading level1 row10\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_db502_row10_col3\" class=\"data row10 col3\" >0.11</td>\n",
       "      <td id=\"T_db502_row10_col4\" class=\"data row10 col4\" >0.08</td>\n",
       "      <td id=\"T_db502_row10_col5\" class=\"data row10 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_db502_level1_row11\" class=\"row_heading level1 row11\" >All</th>\n",
       "      <td id=\"T_db502_row11_col3\" class=\"data row11 col3\" >0.11</td>\n",
       "      <td id=\"T_db502_row11_col4\" class=\"data row11 col4\" >0.09</td>\n",
       "      <td id=\"T_db502_row11_col5\" class=\"data row11 col5\" >0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b01ba309d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_Document(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_Document(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_document = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    "    .map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)  \n",
    "display(df_similarity_document)\n",
    "latex = df_similarity_document.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of documents (p < 0.05 bold)\", label=\"similarityDocuments\")\n",
    "latex_strings.append(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FI Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 61.28it/s] \n",
      "100%|██████████| 54/54 [00:07<00:00,  6.93it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 66.51it/s] \n",
      "100%|██████████| 54/54 [00:08<00:00,  6.09it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 72.59it/s]\n",
      "100%|██████████| 54/54 [00:08<00:00,  6.45it/s]\n",
      "100%|██████████| 540/540 [00:08<00:00, 66.89it/s]\n",
      "100%|██████████| 540/540 [01:23<00:00,  6.49it/s]\n",
      "100%|██████████| 540/540 [00:06<00:00, 87.65it/s] \n",
      "100%|██████████| 540/540 [00:57<00:00,  9.33it/s]\n",
      "100%|██████████| 540/540 [00:07<00:00, 76.37it/s] \n",
      "100%|██████████| 540/540 [01:05<00:00,  8.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3dba7_row4_col5, #T_3dba7_row5_col5, #T_3dba7_row6_col5, #T_3dba7_row7_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_3dba7_level0_col3, #T_3dba7_level0_col4, #T_3dba7_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3dba7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3dba7_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_3dba7_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_3dba7_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">\\# Common Features</th>\n",
       "      <th id=\"T_3dba7_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_3dba7_row0_col3\" class=\"data row0 col3\" >12.56</td>\n",
       "      <td id=\"T_3dba7_row0_col4\" class=\"data row0 col4\" >9.08</td>\n",
       "      <td id=\"T_3dba7_row0_col5\" class=\"data row0 col5\" >3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_3dba7_row1_col3\" class=\"data row1 col3\" >9.64</td>\n",
       "      <td id=\"T_3dba7_row1_col4\" class=\"data row1 col4\" >8.99</td>\n",
       "      <td id=\"T_3dba7_row1_col5\" class=\"data row1 col5\" >0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_3dba7_row2_col3\" class=\"data row2 col3\" >4.86</td>\n",
       "      <td id=\"T_3dba7_row2_col4\" class=\"data row2 col4\" >3.65</td>\n",
       "      <td id=\"T_3dba7_row2_col5\" class=\"data row2 col5\" >1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_3dba7_row3_col3\" class=\"data row3 col3\" >9.02</td>\n",
       "      <td id=\"T_3dba7_row3_col4\" class=\"data row3 col4\" >7.24</td>\n",
       "      <td id=\"T_3dba7_row3_col5\" class=\"data row3 col5\" >1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Cosine Similarity</th>\n",
       "      <th id=\"T_3dba7_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_3dba7_row4_col3\" class=\"data row4 col3\" >0.32</td>\n",
       "      <td id=\"T_3dba7_row4_col4\" class=\"data row4 col4\" >0.17</td>\n",
       "      <td id=\"T_3dba7_row4_col5\" class=\"data row4 col5\" >0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_3dba7_row5_col3\" class=\"data row5 col3\" >0.38</td>\n",
       "      <td id=\"T_3dba7_row5_col4\" class=\"data row5 col4\" >0.22</td>\n",
       "      <td id=\"T_3dba7_row5_col5\" class=\"data row5 col5\" >0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_3dba7_row6_col3\" class=\"data row6 col3\" >0.25</td>\n",
       "      <td id=\"T_3dba7_row6_col4\" class=\"data row6 col4\" >0.10</td>\n",
       "      <td id=\"T_3dba7_row6_col5\" class=\"data row6 col5\" >0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba7_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_3dba7_row7_col3\" class=\"data row7 col3\" >0.31</td>\n",
       "      <td id=\"T_3dba7_row7_col4\" class=\"data row7 col4\" >0.16</td>\n",
       "      <td id=\"T_3dba7_row7_col5\" class=\"data row7 col5\" >0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b117040510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_FI(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_FI(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_fi = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    ".map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)\n",
    "display(df_similarity_fi)\n",
    "latex = df_similarity_fi.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)\", label=\"similarityFI\")\n",
    "latex_strings.append(latex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 482.35it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 546.33it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 897.78it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 577.82it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 577.73it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 867.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_66333_row0_col5, #T_66333_row1_col5, #T_66333_row2_col5, #T_66333_row3_col5, #T_66333_row4_col5, #T_66333_row5_col5, #T_66333_row6_col5, #T_66333_row7_col5, #T_66333_row10_col5, #T_66333_row11_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_66333_level0_col3, #T_66333_level0_col4, #T_66333_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_66333\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_66333_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_66333_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_66333_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">\\# Matching Anchors</th>\n",
       "      <th id=\"T_66333_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_66333_row0_col3\" class=\"data row0 col3\" >0.39</td>\n",
       "      <td id=\"T_66333_row0_col4\" class=\"data row0 col4\" >0.08</td>\n",
       "      <td id=\"T_66333_row0_col5\" class=\"data row0 col5\" >0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_66333_row1_col3\" class=\"data row1 col3\" >0.94</td>\n",
       "      <td id=\"T_66333_row1_col4\" class=\"data row1 col4\" >0.43</td>\n",
       "      <td id=\"T_66333_row1_col5\" class=\"data row1 col5\" >0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_66333_row2_col3\" class=\"data row2 col3\" >0.44</td>\n",
       "      <td id=\"T_66333_row2_col4\" class=\"data row2 col4\" >0.06</td>\n",
       "      <td id=\"T_66333_row2_col5\" class=\"data row2 col5\" >0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_66333_row3_col3\" class=\"data row3 col3\" >0.59</td>\n",
       "      <td id=\"T_66333_row3_col4\" class=\"data row3 col4\" >0.19</td>\n",
       "      <td id=\"T_66333_row3_col5\" class=\"data row3 col5\" >0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Len Longest Matching Anchor</th>\n",
       "      <th id=\"T_66333_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_66333_row4_col3\" class=\"data row4 col3\" >0.39</td>\n",
       "      <td id=\"T_66333_row4_col4\" class=\"data row4 col4\" >0.08</td>\n",
       "      <td id=\"T_66333_row4_col5\" class=\"data row4 col5\" >0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_66333_row5_col3\" class=\"data row5 col3\" >0.94</td>\n",
       "      <td id=\"T_66333_row5_col4\" class=\"data row5 col4\" >0.43</td>\n",
       "      <td id=\"T_66333_row5_col5\" class=\"data row5 col5\" >0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_66333_row6_col3\" class=\"data row6 col3\" >0.44</td>\n",
       "      <td id=\"T_66333_row6_col4\" class=\"data row6 col4\" >0.06</td>\n",
       "      <td id=\"T_66333_row6_col5\" class=\"data row6 col5\" >0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_66333_row7_col3\" class=\"data row7 col3\" >0.59</td>\n",
       "      <td id=\"T_66333_row7_col4\" class=\"data row7 col4\" >0.19</td>\n",
       "      <td id=\"T_66333_row7_col5\" class=\"data row7 col5\" >0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level0_row8\" class=\"row_heading level0 row8\" rowspan=\"4\">$\\theta$ Longest Matching Anchor</th>\n",
       "      <th id=\"T_66333_level1_row8\" class=\"row_heading level1 row8\" >DetectorRadford</th>\n",
       "      <td id=\"T_66333_row8_col3\" class=\"data row8 col3\" >0.18</td>\n",
       "      <td id=\"T_66333_row8_col4\" class=\"data row8 col4\" >0.06</td>\n",
       "      <td id=\"T_66333_row8_col5\" class=\"data row8 col5\" >0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row9\" class=\"row_heading level1 row9\" >DetectorGuo</th>\n",
       "      <td id=\"T_66333_row9_col3\" class=\"data row9 col3\" >0.54</td>\n",
       "      <td id=\"T_66333_row9_col4\" class=\"data row9 col4\" >0.36</td>\n",
       "      <td id=\"T_66333_row9_col5\" class=\"data row9 col5\" >0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row10\" class=\"row_heading level1 row10\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_66333_row10_col3\" class=\"data row10 col3\" >0.33</td>\n",
       "      <td id=\"T_66333_row10_col4\" class=\"data row10 col4\" >0.05</td>\n",
       "      <td id=\"T_66333_row10_col5\" class=\"data row10 col5\" >0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66333_level1_row11\" class=\"row_heading level1 row11\" >All</th>\n",
       "      <td id=\"T_66333_row11_col3\" class=\"data row11 col3\" >0.35</td>\n",
       "      <td id=\"T_66333_row11_col4\" class=\"data row11 col4\" >0.16</td>\n",
       "      <td id=\"T_66333_row11_col5\" class=\"data row11 col5\" >0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b0a5a94810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_Anchor(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_Anchor(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_anchors = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    ".map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)\n",
    "display(df_similarity_anchors)\n",
    "latex = df_similarity_anchors.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of Anchor explanations (p < 0.05 bold)\", label=\"similarityAnchors\")\n",
    "latex_strings.append(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of documents (p < 0.05 bold)} \\label{similarityDocuments} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of documents (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{Spacy Similarity} & DetectorRadford & 0.93 & 0.88 & \\bfseries 0.05 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.90 & 0.88 & 0.02 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.92 & 0.88 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.92 & 0.88 & \\bfseries 0.04 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Jaccard Similarity} & DetectorRadford & 0.17 & 0.12 & \\bfseries 0.05 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.13 & 0.12 & 0.01 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.14 & 0.12 & \\bfseries 0.03 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.15 & 0.12 & \\bfseries 0.03 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Cosine Similarity tfidf} & DetectorRadford & 0.13 & 0.09 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.10 & 0.09 & \\bfseries 0.02 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.11 & 0.08 & \\bfseries 0.03 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.11 & 0.09 & \\bfseries 0.03 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n",
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)} \\label{similarityFI} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{\\# Common Features} & DetectorRadford & 12.56 & 9.08 & 3.48 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 9.64 & 8.99 & 0.65 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 4.86 & 3.65 & 1.21 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 9.02 & 7.24 & 1.78 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Cosine Similarity} & DetectorRadford & 0.32 & 0.17 & \\bfseries 0.16 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.38 & 0.22 & \\bfseries 0.15 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.25 & 0.10 & \\bfseries 0.15 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.31 & 0.16 & \\bfseries 0.15 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n",
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of Anchor explanations (p < 0.05 bold)} \\label{similarityAnchors} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of Anchor explanations (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{\\# Matching Anchors} & DetectorRadford & 0.39 & 0.08 & \\bfseries 0.31 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.94 & 0.43 & \\bfseries 0.52 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.44 & 0.06 & \\bfseries 0.38 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.59 & 0.19 & \\bfseries 0.40 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Len Longest Matching Anchor} & DetectorRadford & 0.39 & 0.08 & \\bfseries 0.31 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.94 & 0.43 & \\bfseries 0.52 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.44 & 0.06 & \\bfseries 0.38 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.59 & 0.19 & \\bfseries 0.40 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{$\\theta$ Longest Matching Anchor} & DetectorRadford & 0.18 & 0.06 & 0.12 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.54 & 0.36 & 0.18 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.33 & 0.05 & \\bfseries 0.27 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.35 & 0.16 & \\bfseries 0.19 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in latex_strings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Assignment}\n",
      "\\label{assignmentusers}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      " & Anchor & LIME & SHAP \\\\\n",
      "\\midrule\n",
      "DetectGPT & ['U1', 'U2', 'U3'] & ['U4', 'U5', 'U6'] & ['U7', 'U8', 'U9'] \\\\\n",
      "Guo & ['U10', 'U11', 'U12'] & ['U13', 'U14', 'U15'] & ['U16', 'U17', 'U18'] \\\\\n",
      "Radford & ['U19', 'U20', 'U21'] & ['U22', 'U23', 'U24'] & ['U25', 'U26', 'U27'] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [c.replace(\"_Explainer\", \"\") for c in df.groupby([\"Detector\",\"Explainer\"]).count().index.get_level_values(1).unique()]\n",
    "index = [i.replace(\"Detector\",\"\") for i in df.groupby([\"Detector\",\"Explainer\"]).count().index.get_level_values(0).unique()]\n",
    "r = []\n",
    "u = 1\n",
    "for detector_name in index:\n",
    "    row = []\n",
    "    for explainer_name in columns:\n",
    "        users = []\n",
    "        for i in range(0,3):\n",
    "            users.append(\"U\"+str(u))\n",
    "            u+=1\n",
    "        row.append(users)\n",
    "        \n",
    "    r.append(row)\n",
    "print(pd.DataFrame(r, columns = columns, index = index ).to_latex(caption=\"Assignment\",label=\"assignmentusers\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
