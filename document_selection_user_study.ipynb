{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: Always set this flag to `True` before git commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBFUSCATE_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "import lime\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<p><b>This is a {kind_of_document} document.</b></p>\n",
    "<p>The detector {correctly_or_wrongly} predicted that this document was... </p>\n",
    "<p>&emsp; ... machine generated with {p_machine} % confidence.</p>\n",
    "<p>&emsp; ... human written with {p_human} % confidence.</p> \n",
    "<div style=\"float:left;\">{highlighted_text}</div>\n",
    "\"\"\"\n",
    "#<div style=\"float:left; height:30em;\">{barplot_machine}{barplot_human}</div>\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "def print_template(document, gold_label, detector, explainer):\n",
    "    p_machine, p_human = detector.predict_proba([document])[0]\n",
    "   # machine, human = explainer.get_barplots_HTML(document)\n",
    "    display(HTML(template.format(\n",
    "    p_machine=int(p_machine*100) if not OBFUSCATE_RESULTS else \"<redacted>\", \n",
    "    p_human=int(p_human*100) if not OBFUSCATE_RESULTS else \"<redacted>\",\n",
    "  #  barplot_machine=machine,\n",
    "  #  barplot_human=human,\n",
    "    kind_of_document= ((\"machine generated\" if gold_label == False else \"human written\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    correctly_or_wrongly= ((\"correctly\" if detector.predict_label([document])[0] == gold_label else \"wrongly\") if not OBFUSCATE_RESULTS else \"<redacted>\"), \n",
    "    highlighted_text=explainer.get_highlighted_text_HTML((document if not OBFUSCATE_RESULTS else \"<redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted> <redacted>\")),\n",
    "    )))\n",
    "def print_shared_features(features, fi_scores):\n",
    "    for feature, (fi_score_a, fi_score_b) in zip(features, fi_scores):\n",
    "        print(feature)\n",
    "        print(\"\\t\\ta: {} \\t b: {}\".format(fi_score_a, fi_score_b))\n",
    "        print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer, skip_n=0):\n",
    "\n",
    "    for (a,b)in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            print(\"idx_a: <redacted> idx_b <redacted>\")\n",
    "        else:\n",
    "            print(\"idx_a: {} idx_b {}\".format(document_ids[a],document_ids[b]))\n",
    "\n",
    "        print_template(documents[a], gold_labels[a], detector, explainer)\n",
    "        print_template(documents[b], gold_labels[b], detector, explainer)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Feature Importance Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix of explanations for all documents in \"data\"\n",
    "# This function was once SubmodularPick.__init__() in LIME. It was planned to use its output for a search strategy for similar explanations. \n",
    "# Only the code for creating W from the paper (rows are explanations, cols are BOW features) remains\n",
    "def get_explanation_matrix_W(data, explainer, quiet=False):\n",
    "    # Get (cached) explanations \n",
    "    explanations_and_documents = [(d, explainer.get_fi_scores(d, fill=True)[0]) for d in tqdm(data, desc=\"Loading all explanations\",disable=quiet) ] # [0]: only irt to label machine, fill: return all words, even those with 0 fi\n",
    "\n",
    "    get_feature_name_signed = lambda feature,value : feature + (\"_+\" if value >=0 else \"_-\") # appends \"_+\" or \"_-\" to each feature name, e.g. \"example\" -> \"example_+\" if fi(example) > 0\n",
    "    # Ribeiro et al.: Find all the explanation model features used. Defines the dimension d'\n",
    "    # i.e. determine columns of W: each word (BOW) gets (up to) two columns, one for positive FI scores, one for negative FI scores\n",
    "    features_dict = {}\n",
    "    feature_iter = 0\n",
    "    for d, exp in tqdm(explanations_and_documents, desc=\"Building global dict of features\", disable=quiet):\n",
    "     #   print(\"exp\",exp)\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            feature_name = get_feature_name_signed(feature,value) # get_feature_name_signed: see above\n",
    "            if feature_name not in features_dict.keys():\n",
    "                features_dict[feature_name] = (feature_iter)\n",
    "                feature_iter += 1\n",
    "    d_prime = len(features_dict.keys())\n",
    "\n",
    "    # Ribeiro et al.: Create the n x d' dimensional 'explanation matrix', W\n",
    "    W = np.zeros((len(explanations_and_documents), d_prime))\n",
    "\n",
    "    # fill W, look up cols in dict that was just created\n",
    "    # W: one row per explanation, one col per feature in feature_dict\n",
    "    for i, (d, exp) in enumerate(tqdm(explanations_and_documents,  desc=\"Building W\",disable=quiet)):\n",
    "        for feature_idx, value in exp: # irt to label machine\n",
    "            # get_feature_name_signed: see above\n",
    "            feature = explainer.tokenize(d)[feature_idx]\n",
    "            W[i, features_dict[get_feature_name_signed(feature,value)]] += value\n",
    "    return W, features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples: (pair of documents whose explanations are similar, the features that overlap, fi scores of said features)\n",
    "# this maximizes similarity between documents (greedy, each document is only part of one tuple)\n",
    "# another function should select n tuples to maximize coverage in explanation space akin to SP-LIME\n",
    "sum_two_max = None\n",
    "def get_pairs(documents, W, detector, features_dict, n_pairs=None):\n",
    "    if n_pairs is None:\n",
    "        n_pairs = len(documents)//2\n",
    "    idx_pairs = [] # tuples of indices of similar documents a,b in \"data\"\n",
    "    features = [] # list of features those documents covered\n",
    "    fi_scores_pairs = []\n",
    "\n",
    "    W_ = np.copy(W)\n",
    "\n",
    "    document_indices = np.arange(0, W_.shape[0])\n",
    "   # print(document_indices.shape, W_.shape)\n",
    "    for _ in tqdm(range(0,n_pairs), desc=\"Obtaining pairs\"):\n",
    "        sim = cosine_similarity(W_) # calculate cosine similarity between all explanations\n",
    "        sim = np.triu(sim,k=1)  # remove redundant information for argmax()\n",
    "\n",
    "        idx_max = np.unravel_index(sim.argmax(), sim.shape) # get most similar pair, result is (idx_a, idx_b)\n",
    "       # print(idx_max)\n",
    "        features_non_zero_in_both = np.intersect1d(W_[idx_max[0]].nonzero(),W_[idx_max[1]].nonzero()) # get features that have non-zero fi in both explanations\n",
    "                                                                                                      # is used later for selecting a set of tuples with high coverage (as in SP-LIME)\n",
    "        non_zero_features = [] # list with features that will be returned\n",
    "        non_zero_fi_scores_tuples = [] # list of tuples with fi scores in a and b that will be returned\n",
    "    \n",
    "        # look up feature_idxs in features_dict and append them to the output\n",
    "        for iii in features_non_zero_in_both:\n",
    "           key = list(features_dict.keys())[list(features_dict.values()).index(iii)]\n",
    "           non_zero_features.append(key)\n",
    "           non_zero_fi_scores_tuples.append((W_[idx_max[0],features_dict[key]], W_[idx_max[1],features_dict[key]]))\n",
    "        \n",
    "        # Only add pair to output list if valid: at least one common feature is not zero AND f(a) == f(b) (i.e., the explanation is arguing for the same detector verdict)\n",
    "        if len(non_zero_features) > 0:\n",
    "            a,b = detector.predict_label([documents[document_indices[idx_max[0]]], documents[document_indices[idx_max[1]]]])\n",
    "            if a == b:\n",
    "                idx_pairs.append(document_indices[list(idx_max)])\n",
    "                fi_scores_pairs.append(non_zero_fi_scores_tuples)\n",
    "                features.append(non_zero_features)\n",
    "        # delete pair from W_:\n",
    "        W_ = np.delete(W_, idx_max, axis=0) \n",
    "        document_indices = np.delete(document_indices, list(idx_max))\n",
    "\n",
    "    return idx_pairs, features, fi_scores_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "\n",
    "# this is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# can brute force here as only the 10 top pairs by similarity are used for each class, \n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a set of pairs that maximizes coverage in explanation space akin to the SP-LIME strategy but for pairs \n",
    "# this is the maximum coverage problem: e.g., R. Church and C. ReVelle, 1974 http://yalma.fime.uanl.mx/~roger/work/teaching/class_tso/docs_project/problems/MCLP/1974-prs-Church%20ReVelle-maximal%20covering%20location.pdf\n",
    "# implementing a greedy algorithm here: \n",
    "#   \"In order to achieve a maximal cover for p facilities under a given service distance, \n",
    "#   the algorithm starts with an empty solution set and then adds to this set one at a \n",
    "#   time the best facility sites. The GA algorithm picks for the first facility that \n",
    "#   site which covers the most of the total population. For the second facility, GA \n",
    "#   picks the site that covers the most of the population not covered by the first \n",
    "#   facility. Then, for the third facility, GA picks the site that covers the most of the \n",
    "#   population not covered by the first and second facilities. This process is continued until either p facilities have been selected or all the population is covered. \n",
    "#   Details of the algorithm are given in Church.\" (R. Church and C. ReVelle, 1974, p. 105f)\n",
    "\n",
    "\n",
    "# let the coverage be the number of columns != 0 in W. And coverage((a,b))) := coverage(sum([a,b])), akin to the importance vector in SP-LIME (Note that columns in W are either negative FI or positive FI here (see get_feature_name_signed()), so scores don't cancel out in sum)\n",
    "\n",
    "def coverage(selection, W):\n",
    "    return np.count_nonzero(np.sum(W[np.array(list(selection)).flatten()], axis=0)) # coverage(selection)= number of cols in W that have at least one non-zero entry under this selection of pairs. Recall that W has (up to) two entries per word: one for positive and one for negative FI\n",
    "\n",
    "def get_site_with_max_coverage(sites, previous_selections, W):\n",
    "    best_site = None\n",
    "    best_coverage = 0\n",
    "    for site in sites:\n",
    "        candidate = set(np.array(previous_selections).flatten()).union(site) # extend the previous selection by \"site\", this addresses \"For the second facility, GA picks the site that covers the most of the population not covered by the first \"\n",
    "        cov = coverage(candidate, W) # compute new coverage\n",
    "        if cov >= best_coverage:\n",
    "            best_coverage = cov\n",
    "            best_site = site\n",
    "    return best_site, best_coverage\n",
    "\n",
    "def get_p_tuples_with_high_coverage(indices, W, p=10):\n",
    "  sites = list(indices)\n",
    "  # \"the algorithm start with emty solution set\"\n",
    "  result = list()\n",
    "  # \"and then adds to this set one at a time the best facility sites\"\n",
    "  while True:\n",
    "      # \"The GA algorithm picks for the first facility that \n",
    "      # site which covers the most of the total population\"\n",
    "      best_site, best_coverage = get_site_with_max_coverage(sites, result, W)\n",
    "      result.append(best_site)\n",
    "      # \"This process is continued until either p facilities have been selected or all the population is covered.\"\n",
    "      if len(result) == p or best_coverage == W.shape[1]:\n",
    "          break\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two pairs, one for f(x) = machine and one for f(x) = human\n",
    "# checks texts_already_selected and chooses next best pair (for each class) if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair)\n",
    "def obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    W, features_dict = get_explanation_matrix_W(documents, explainer)\n",
    "    similar_pairs, _, _ = get_pairs(documents, W, detector, features_dict)\n",
    "    # want a dataset that is balanced irt to the two base classes:\n",
    "    # two pairs will be returned, one with f(x) == machine, and one with f(x) == human\n",
    "    top_10_pairs_human = []\n",
    "    top_10_pairs_machine = []\n",
    "    for pair in similar_pairs:\n",
    "        if (documents[pair[0]] in texts_already_selected) or (documents[pair[1]] in texts_already_selected):\n",
    "            continue\n",
    "        if detector.predict_label([documents[pair[0]]])[0]:\n",
    "            top_10_pairs_human.append(pair)\n",
    "        else:\n",
    "            top_10_pairs_machine.append(pair)\n",
    "        if len(top_10_pairs_human) >= 10 and len(top_10_pairs_machine) >= 10:\n",
    "            top_10_pairs_human = top_10_pairs_human[0:10]\n",
    "            top_10_pairs_machine = top_10_pairs_machine[0:10]\n",
    "            break\n",
    "    pairs_human = get_p_tuples_with_high_coverage(top_10_pairs_human, W, p=3)\n",
    "    pairs_machine = get_p_tuples_with_high_coverage(top_10_pairs_machine, W, p=3)\n",
    "    return pairs_human + pairs_machine    \n",
    "    # combinations_ = [(a,b) for a in top_10_pairs_machine for b in top_10_pairs_human]\n",
    "    # # return two pairs maximizing coverage, one with f(x) == machine, and one with f(x) == human \n",
    "    \n",
    "    # pair_a = None\n",
    "    # pair_b = None\n",
    "    # coverage_ = [((a,b),coverage([a,b],W)) for a,b in combinations_]\n",
    "    # (pair_a, pair_b), c = max(coverage_, key=lambda item : item[1])\n",
    "    # return [pair_a, pair_b]\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        # break\n",
    "        # # get f(a) as one example per class is returned\n",
    "        # predictions = [detector.predict_label([documents[a]])[0] for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "        # # return example with highest coverage for each class\n",
    "        # # if a document is in texts_already_selected (i.e. it was already selected for another explainer-detector pair), the one with the next-highest coverage (for that prediction) is returned \n",
    "        # for idx_pair, prediction in enumerate(predictions):\n",
    "        #     a,b = pairs[idx_pair]\n",
    "        #     # check if a or b are in texts_already_selected\n",
    "        #     if (documents[a] not in texts_already_selected) and (documents[b] not in texts_already_selected):\n",
    "        #         if prediction == 0 and pair_machine is None: # only keep first\n",
    "        #             pair_machine = pairs[idx_pair] \n",
    "        #         if prediction == 1 and pair_human is None: # only keep first\n",
    "        #             pair_human = pairs[idx_pair] \n",
    "        #     if pair_human is not None and pair_machine is not None:\n",
    "        #         return [pair_machine, pair_human] \n",
    "        # k+=1 # loop until both pair_machine and pair_human not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategy for Rule-Based Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor.anchor import anchor_explanation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document_a, document_b):\n",
    "    # case sensitive, on spacy tokens\n",
    "    a = list(chain(*[[token.text for token in sent] for sent in nlp(document_a).sents]))\n",
    "    b = list(chain(*[[token.text for token in sent] for sent in nlp(document_b).sents]))\n",
    "    intersection = float(len(list(set(a).intersection(b))))\n",
    "    union = float((len(set(a)) + len(set(b)))) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the order of occurence in a list of words, e.g.:\n",
    "# [\"example\", \"test\", \"example\", \"one\"] -> ['example_0', 'test_0', 'example_1', 'one_0']\n",
    "def encode_count(list_of_words):\n",
    "    d = defaultdict(lambda : 0)\n",
    "    encoded = []\n",
    "    for word in list_of_words:\n",
    "        encoded.append(word + \"_\" + str(d[word]))\n",
    "        d[word] +=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonary Anchors returns can define multiple anchors:\n",
    "# {this, is, an, example} : 0.9\n",
    "# {this, is, an}: 0.8\n",
    "# {this, is, }: 0.75\n",
    "# {this}: 0.4\n",
    "# extract all of them, only keep those with p >= 0.75 (threshold set when searching)\n",
    "def get_anchors_at_each_k(documents, explainer, quiet=False):\n",
    "    anchors = []\n",
    "    p = []\n",
    "    ids = []\n",
    "    for i,_ in tqdm(enumerate(documents), desc=\"Loading all explanations\", disable=quiet):#enumerate(documents):\n",
    "        exp = explainer.get_explanation_cached(documents[i])\n",
    "        exp[\"names\"] = encode_count(exp[\"names\"]) # Anchors is not BOW. But the algorithm is written with python set()s\n",
    "        while len(exp[\"mean\"]) >=1:#and exp[\"mean\"][-1] >= 0.75:\n",
    "            anchors.append(set(exp[\"names\"])) \n",
    "            p.append(exp[\"mean\"][-1])\n",
    "            ids.append(i)\n",
    "\n",
    "            exp[\"mean\"].pop()\n",
    "            exp[\"names\"].pop()\n",
    "    return anchors, p, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for pairs of anchors\n",
    "# returns 2 pairs of documents, one pair for f(x) = machine, one for f(x) = human, both sampled randomly\n",
    "# checks for and skips documents in \"texts_already_selected\" (i.e. it was already selected for an other explainer-detector pair)\n",
    "\n",
    "def obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "\n",
    "    anchors, p, ids = get_anchors_at_each_k(documents, explainer)\n",
    "                        # DetectGPT + Anchors is to expensive to run experiments on \n",
    "    # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "    duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "    # get the ids and p for each duplicate_anchor in  duplicate_anchors\n",
    "    # \"candidates\" is a list of lists with ids (and all other details) of each duplicate_anchor\n",
    "    candidates = [[(anchor, p, document_id) for anchor, p, document_id in zip(anchors, p, ids) if anchor == duplicate_anchor] for duplicate_anchor in duplicate_anchors ]\n",
    "    # now check for each paring of the documents in each sublist of \"candidates\":\n",
    "    #   is f(a) == f(b)?, if not: discard\n",
    "    # then pick pair with highest jaccard_score on the original documents in each \"candidate\"\n",
    "    pairs = []\n",
    "\n",
    "    predictions_cache = {}\n",
    "    def cached_predict(idx):\n",
    "        if idx not in predictions_cache:\n",
    "            predictions_cache[idx] = detector.predict_label([documents[idx]])[0]\n",
    "        return predictions_cache[idx]\n",
    "    for candidate in tqdm(candidates, desc=\"Assessing candidates\",position=1):\n",
    "        \n",
    "        anchor_s, p, ids  = zip(*candidate)\n",
    "        c = list(combinations(ids, 2))\n",
    "        c = [(a,b) for a,b in c if cached_predict(a) == cached_predict(b) if not (documents[a] in texts_already_selected) or (documents[b] in texts_already_selected)]\n",
    "        if len(c) == 0:\n",
    "            continue\n",
    "        jaccard_scores = [(a,b, jaccard_similarity(documents[a], documents[b])) for a,b in tqdm(c, desc=\"Calculating Jaccard Similarity (of documents not Anchors)\",position=0)]\n",
    "        a,b, score = max(jaccard_scores, key=lambda x: x[2])\n",
    "        pairs.append((a,b))\n",
    "\n",
    "    # sample twice: once for f(x) == human and once for f(x) == machine. f(a) == f(b) is tested earlier\n",
    "\n",
    "    predictions = [cached_predict(a) for a,_ in pairs] # wheter a == b was tested before\n",
    "\n",
    "    predictions_ = np.array(predictions)\n",
    "    pairs_ = np.array(pairs)\n",
    "\n",
    "    machine = pairs_[predictions_ == False]\n",
    " \n",
    "    human = pairs_[predictions_ == True]\n",
    "   \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    result = []\n",
    "    # one explainer (DetectGPT) has no explanations for f(x) = human:\n",
    "    if not(True in predictions) or human.shape[0] < 3:\n",
    "        print(\"Warning: Not enough examples for f(x) = human. Returning additional examples for machine\")\n",
    "        result =  list(machine[np.random.choice(machine.shape[0], 6-human.shape[0], replace=False)]) + list(human[np.random.choice(human.shape[0], human.shape[0], replace=False)])\n",
    "    elif not (False in predictions) or machine.shape[0] < 3:\n",
    "        print(\"Warning: Not enough examples for f(x) = machine. Returning additional examples for human\")\n",
    "        result = list(machine[np.random.choice(machine.shape[0], machine.shape[0], replace=False)]) + list(human[np.random.choice(human.shape[0], 6 -machine.shape[0], replace=False)])\n",
    "    else:\n",
    "        result =  list(machine[np.random.choice(machine.shape[0], 3, replace=False)]) + list(human[np.random.choice(human.shape[0], 3, replace=False)]) # returns a random pair for machine and a random pair for human\n",
    "\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected):\n",
    "    if isinstance(explainer, Anchor_Explainer):\n",
    "        return obtain_dataset_Anchor(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)\n",
    "    else:\n",
    "        return obtain_dataset_FI_methods(explainer, detector, documents, gold_labels, document_ids, texts_already_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test \n",
    "\n",
    "documents = list(test[\"answer\"])\n",
    "gold_labels = list(test[\"author\"] == \"human_answers\") # convention: 0: machine, 1: human, see detector.py\n",
    "document_ids = list(range(0,len(documents))) # note that the search algorithms don't use these ids. They are only used for printing and the exclude_list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Document Selection\n",
    "Some documents are excluded from the user-study for the reasons specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195,\n",
       " 60,\n",
       " 108,\n",
       " 228,\n",
       " 143,\n",
       " 288,\n",
       " 117,\n",
       " 188,\n",
       " 110,\n",
       " 159,\n",
       " 97,\n",
       " 105,\n",
       " 115,\n",
       " 266,\n",
       " 158,\n",
       " 16,\n",
       " 190,\n",
       " 294,\n",
       " 27,\n",
       " 103]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exclude_list = {\n",
    "    (195, 60,108, 228,143): \"Names forum/service explicitly\",\n",
    "    (288,117, 188, 110, 159, 97, 105, 115,266, 158): \"Author introduces themselves by name\",\n",
    "    (16,): \"References earlier post by other user\",\n",
    "    (190,294,): \"Names forum user who asked the question\",\n",
    "    (27,103,): \"NSFW\",\n",
    "    \n",
    "    \n",
    "}\n",
    "exclude_list = [x for xs in [ list(key) for key in exclude_list.keys()] for x in xs]\n",
    "exclude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply exclude_list\n",
    "documents = [d for i,d in zip(document_ids, documents) if i not in exclude_list]\n",
    "gold_labels = [gl for i,gl in zip(document_ids, gold_labels) if i not in exclude_list]\n",
    "document_ids = [i for i in document_ids if i not in exclude_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**: If you plan to participate in the user study, set `OBFUSCATE_RESULTS` to `True` before proceeding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Detector\", \"Explainer\", \"Documents Phases 1+3\", \"Documents Phases 2+4\", \"f(a)\", \"f(b)\", \"GT a\", \"GT b\", \"idx a\", \"idx b\", \"Spacy Similarity\", \"Jaccard Similarity\", \"Cosine Similarity tfidf\",\"hash a\", \"hash b\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_selection(selection, pairs, explainer, detector):\n",
    "    for a,b in pairs:\n",
    "        if OBFUSCATE_RESULTS:\n",
    "            continue\n",
    "        \n",
    "        tfidf_= tfidf.transform([documents[a], documents[b]])   \n",
    "        selection.append((detector.__class__.__name__,\n",
    "                        explainer.__class__.__name__,\n",
    "                        documents[a], documents[b],\n",
    "                        *detector.predict_label([documents[a], documents[b]]),\n",
    "                        gold_labels[a],\n",
    "                        gold_labels[b],\n",
    "                        document_ids[a],\n",
    "                        document_ids[b],\n",
    "                        nlp(documents[a]).similarity(nlp(documents[b])),\n",
    "                        jaccard_similarity(documents[a], documents[b]),\n",
    "                        (tfidf_ * tfidf_.T).toarray()[0,1],\n",
    "                        explainer.get_hash(documents[a]),\n",
    "                        explainer.get_hash(documents[b])))\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection = []\n",
    "# for detector_class in [DetectorDetectGPT,DetectorRadford,DetectorGuo]:\n",
    "#     selection_detector = []\n",
    "#     detector = detector_class()\n",
    "#     display(HTML(\"<h1>{}</h1>\".format(detector.__class__.__name__)))\n",
    "#     for explainer_class in [Anchor_Explainer, LIME_Explainer,SHAP_Explainer]:\n",
    "#         explainer = explainer_class(detector)\n",
    "#         display(HTML(\"<h2>{}</h2>\".format(explainer.__class__.__name__)))\n",
    "#         texts_already_selected = []\n",
    "#         if len(selection_detector) > 0:\n",
    "#             texts_already_selected = list(zip(*selection_detector))[2] + list(zip(*selection_detector))[3]\n",
    "#         pairs = obtain_dataset(explainer, detector, documents, gold_labels, document_ids, texts_already_selected=texts_already_selected)\n",
    "#        # print_pairs(pairs, documents, gold_labels, document_ids, detector, explainer)\n",
    "#         selection_detector = update_selection(selection_detector, pairs, explainer, detector)\n",
    "#     selection = selection + selection_detector\n",
    "#         # break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(selection, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not OBFUSCATE_RESULTS:\n",
    "#     df.to_pickle(\"./dataset_user_study_new.pkl\") # file in .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Documents Phases 1+3</th>\n",
       "      <th>Documents Phases 2+4</th>\n",
       "      <th>f(a)</th>\n",
       "      <th>f(b)</th>\n",
       "      <th>GT a</th>\n",
       "      <th>GT b</th>\n",
       "      <th>idx a</th>\n",
       "      <th>idx b</th>\n",
       "      <th>Spacy Similarity</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "      <th>Cosine Similarity tfidf</th>\n",
       "      <th>hash a</th>\n",
       "      <th>hash b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Both are saying essentially the same thing.  T...</td>\n",
       "      <td>Assuming you live in the US, it is quite norma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "      <td>238</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.153790</td>\n",
       "      <td>714b04dd8923e09ea3f370b93660441d792104140d13d3...</td>\n",
       "      <td>60b992dfcad293c2fbe76d7842a4e469ba041ced7c5270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It can be a good idea to follow the advice of ...</td>\n",
       "      <td>It is generally a good idea to use sponsorship...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>217</td>\n",
       "      <td>234</td>\n",
       "      <td>0.955040</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.203377</td>\n",
       "      <td>c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...</td>\n",
       "      <td>e573296bc2021073ef94c1f60c1f097dc770793203b348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible that a medical bill may have be...</td>\n",
       "      <td>Credit unions are not-for-profit financial coo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>145</td>\n",
       "      <td>161</td>\n",
       "      <td>0.968658</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>dd5eebda3bd1fe324db9194206ddba635deb68071485af...</td>\n",
       "      <td>51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Technical analysts use charts and other techni...</td>\n",
       "      <td>Option contracts are generally not subject to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>284</td>\n",
       "      <td>299</td>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.187225</td>\n",
       "      <td>c8a011be98cb45f35f081b77177e54806135e858720719...</td>\n",
       "      <td>5b452735e59499d5c471fefba9bbcc41bc42763a5d8553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible that stress could be contributi...</td>\n",
       "      <td>It is generally not recommended for individual...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>72</td>\n",
       "      <td>244</td>\n",
       "      <td>0.969343</td>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.156450</td>\n",
       "      <td>2896cd03f5a9e20c7aabbc57edfa78908bc88025779a18...</td>\n",
       "      <td>ccda7c8a282ef5799ad05651df3cd89230aaec28541207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It's important to talk to a healthcare provide...</td>\n",
       "      <td>It is important to get the lump on your grands...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>216</td>\n",
       "      <td>0.961467</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.317253</td>\n",
       "      <td>f16ea55e488aaf8c01f3e55cb6a6f44112e5f3795c4962...</td>\n",
       "      <td>739372dc4bf92366bc991aab94a8f80ee06f90e30c51cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Yes. I can by all means start my own company a...</td>\n",
       "      <td>thanks for your query, the bump could be secon...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>54</td>\n",
       "      <td>175</td>\n",
       "      <td>0.914531</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.112285</td>\n",
       "      <td>20951d0d491f061e02e85cf742afd26ae0ebec6b895af3...</td>\n",
       "      <td>b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "      <td>It doesn't generally matter, and I'm not sure ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.880217</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.054935</td>\n",
       "      <td>99e2f97d4e1e44bdba155c393c422e43995942290aac2f...</td>\n",
       "      <td>5ae5387b74d5d39364702c87dfff6d52121b249c4e7e6b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Limit books are managed by exchanges. If an or...</td>\n",
       "      <td>Reuters has a service you can subscribe to tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>191</td>\n",
       "      <td>249</td>\n",
       "      <td>0.943610</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.072292</td>\n",
       "      <td>cb18fc73b58feeb14211103bba236c3a96f4bc881f323c...</td>\n",
       "      <td>e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Multivariate statistics is a branch of statist...</td>\n",
       "      <td>There are many potential causes of fever, shiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>301</td>\n",
       "      <td>0.916336</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.039615</td>\n",
       "      <td>4c1cbef644e5ad6a64c2209098c975c58e5f80d93e25f5...</td>\n",
       "      <td>0e65e1bbbf8133d14e940529817a55398d439910f1376e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Yes, buying REITs (Real Estate Investment Trus...</td>\n",
       "      <td>Investing in stocks can be a good idea if you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>0.963769</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>0.234383</td>\n",
       "      <td>398430643258bb00cedee78053fb66f240d53d21b390dc...</td>\n",
       "      <td>f022d78aadca7c71859f5ac0face8af6733d02c0e59b37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>In literature, a trope is a common theme or el...</td>\n",
       "      <td>You only have to hold the shares at the openin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>73</td>\n",
       "      <td>109</td>\n",
       "      <td>0.797912</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.043512</td>\n",
       "      <td>0e207de7f9a7ecde671349b9850fff2eabec0b8012acee...</td>\n",
       "      <td>d2acb3efcbd10c6b0656218ca86c3820e9c746e04749e4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>If your counterparty sent money to a correspon...</td>\n",
       "      <td>As your is a very specific case, please get an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>0.949592</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.153888</td>\n",
       "      <td>3dd1514de75255da157c91f80bf9745485cc700f81cf40...</td>\n",
       "      <td>6b25114906789fc86fa719f0394277f9289f080cbbbc7e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Most of the side effects of radiotherapy, incl...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service. I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>128</td>\n",
       "      <td>263</td>\n",
       "      <td>0.952658</td>\n",
       "      <td>0.097015</td>\n",
       "      <td>0.109509</td>\n",
       "      <td>f84e28f87753147cf76ac12e491d3b6718e52a2160ef32...</td>\n",
       "      <td>927b2d3618050b8fae3f7fd9de001dc9fb5c0708b52efd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>I agree, one should not let the tax tail wag t...</td>\n",
       "      <td>The UK has historically aggressive financial l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>164</td>\n",
       "      <td>237</td>\n",
       "      <td>0.915177</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.094831</td>\n",
       "      <td>2c05fd1a84cc8d7fedd6bed6fba06a00a6a7c0997fec7f...</td>\n",
       "      <td>8f1e6fbc370ec179204b1bddc3deb954e294f8e3c4e61c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Thanks for your question on Health Care Magic....</td>\n",
       "      <td>Thanks for your question on Healthcare Magic. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>63</td>\n",
       "      <td>194</td>\n",
       "      <td>0.964194</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.289584</td>\n",
       "      <td>477d8f805d1a6f0f573a60d6dee3dc0aad68fc4eb7ca59...</td>\n",
       "      <td>47e9700fa53a47fb40d1025a89aec77680c41248c82119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Alonzo Church (June 14, 1903 – August 11, 1995...</td>\n",
       "      <td>Stock support and resistance levels are genera...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>140</td>\n",
       "      <td>267</td>\n",
       "      <td>0.764695</td>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>652189ac593cc7b3afcb7e0bd1c48f5b8209e21ca51594...</td>\n",
       "      <td>a98eedec2f3faa2bedc5b48424bffa0fb467ad56d4bcb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>AT&amp;T Labs is the research &amp; development divisi...</td>\n",
       "      <td>Automated decision-making (ADM) involves the u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>0.919497</td>\n",
       "      <td>0.123377</td>\n",
       "      <td>0.090163</td>\n",
       "      <td>419fe06a0a8966dddad1519cb2bca2a274c12f552c6461...</td>\n",
       "      <td>a076837b4d14ff5dbce23f39055fe4e80396f079894657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Waldenstrom's macroglobulinemia is a type of l...</td>\n",
       "      <td>It is not uncommon for pain to increase after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>132</td>\n",
       "      <td>255</td>\n",
       "      <td>0.983331</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.161284</td>\n",
       "      <td>4f3d9b0506f8558d1e36d0c1dfa7ad6b17e57a1eff8400...</td>\n",
       "      <td>60707fbb129a62fd3e3a762cb9a9c8d57a8781c4d65dda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is generally not possible for an adult to b...</td>\n",
       "      <td>It is not appropriate for me to provide specif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>59</td>\n",
       "      <td>127</td>\n",
       "      <td>0.961798</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.096715</td>\n",
       "      <td>bf5cc4428ba8a20b7f40625ce7893a43aa9b3652f73d1c...</td>\n",
       "      <td>589cebdd144cf18ca850a3a1526f48d78b1dd95ba3a931...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible that your dental implants could...</td>\n",
       "      <td>Foamy feces can be caused by a variety of fact...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>285</td>\n",
       "      <td>0.941338</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>0.198382</td>\n",
       "      <td>dec23aa4bf1874d80f869e877d1828ee8b0b9489aafe5c...</td>\n",
       "      <td>a51392d3a9976b33ff5ec8bd300646491be0b2445376f0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It can be a good idea to follow the advice of ...</td>\n",
       "      <td>It's difficult to diagnose a medical condition...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>217</td>\n",
       "      <td>258</td>\n",
       "      <td>0.962063</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.160950</td>\n",
       "      <td>c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...</td>\n",
       "      <td>20426911ee138f3f21f1cd78d1e567e2055a2c9124c047...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible to get pregnant if you have sex...</td>\n",
       "      <td>It is possible that you may have bruised your ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>113</td>\n",
       "      <td>297</td>\n",
       "      <td>0.975657</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.163071</td>\n",
       "      <td>7683b9bff3eca91a14d3f4db144631325c4989a486b9ef...</td>\n",
       "      <td>7633a568e18a48d3009a3b9bbe4302d09d60e320a8a29d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>Both are saying essentially the same thing.  T...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.887495</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.090905</td>\n",
       "      <td>6a66e7a115420485acd54c0cb9e5dc4e9e508b29af4b54...</td>\n",
       "      <td>714b04dd8923e09ea3f370b93660441d792104140d13d3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>It's possible that the pain in your back is re...</td>\n",
       "      <td>If a trendline or pattern breaks due to some b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>74</td>\n",
       "      <td>248</td>\n",
       "      <td>0.942111</td>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.177328</td>\n",
       "      <td>344f46edfe91a144674da8ef3262a58698950066cae548...</td>\n",
       "      <td>436c7e65081fa1a6597579ee32d8b6a0d7f88f1a3d8af0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>It doesn't generally matter, and I'm not sure ...</td>\n",
       "      <td>Hello, Thanks for your query.This can occur du...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>163</td>\n",
       "      <td>0.928879</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>5ae5387b74d5d39364702c87dfff6d52121b249c4e7e6b...</td>\n",
       "      <td>797c3e2456febfd1031497754c2da79b9b0cb00f770118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>TL;DR: The date they were granted.  (Usually, ...</td>\n",
       "      <td>It is possible for cysts to develop in the liv...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>227</td>\n",
       "      <td>0.963289</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.095777</td>\n",
       "      <td>da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...</td>\n",
       "      <td>9eac8625f1ce76e3ab8a5f3aac8669a179e0abf5fff35c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>The price of fuel in Australia, like in any ot...</td>\n",
       "      <td>I'm not familiar with the term \"cake and under...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "      <td>125</td>\n",
       "      <td>0.886263</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.104183</td>\n",
       "      <td>a7b3bdfabc7637684099747710ffcb3511ba513b324be2...</td>\n",
       "      <td>3d19b3cac118bd0523a41e321108d01137a1f51436fde8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Investing in stocks can be a good idea if you ...</td>\n",
       "      <td>Share owner services, also known as shareholde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>196</td>\n",
       "      <td>272</td>\n",
       "      <td>0.939235</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.108089</td>\n",
       "      <td>f022d78aadca7c71859f5ac0face8af6733d02c0e59b37...</td>\n",
       "      <td>3fe9da95aee2d9fb76a7f18840ca38645ba725f62686e4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Credit unions are not-for-profit financial coo...</td>\n",
       "      <td>Gordon Bell is a computer scientist and electr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>161</td>\n",
       "      <td>204</td>\n",
       "      <td>0.865433</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.035942</td>\n",
       "      <td>51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...</td>\n",
       "      <td>49afdd008b7667f8c575f34ad378723e39feb10666c2e9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Human intelligence is the intellectual capabil...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>247</td>\n",
       "      <td>0.955458</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.065098</td>\n",
       "      <td>42b84c175e792cbace8c18218652b0e5fc172d0722913d...</td>\n",
       "      <td>936af4095ba223d41fa40efae657dfe1169abfac77176d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>If your counterparty sent money to a correspon...</td>\n",
       "      <td>As your is a very specific case, please get an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>0.949592</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.153888</td>\n",
       "      <td>3dd1514de75255da157c91f80bf9745485cc700f81cf40...</td>\n",
       "      <td>6b25114906789fc86fa719f0394277f9289f080cbbbc7e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Alonzo Church (June 14, 1903 – August 11, 1995...</td>\n",
       "      <td>Sir Charles Antony Richard Hoare (Tony Hoare o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>140</td>\n",
       "      <td>269</td>\n",
       "      <td>0.971317</td>\n",
       "      <td>0.145570</td>\n",
       "      <td>0.166877</td>\n",
       "      <td>652189ac593cc7b3afcb7e0bd1c48f5b8209e21ca51594...</td>\n",
       "      <td>0d824782624b2690ac2bde813a1cf6a7df527e243a4875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Trailing 12-month total returns, or TTM return...</td>\n",
       "      <td>There are several possible causes for shiverin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>130</td>\n",
       "      <td>286</td>\n",
       "      <td>0.853402</td>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>e398f19e5fbb9d1db41882037d2b1f98d0bf09b76e0e01...</td>\n",
       "      <td>e3aaa179881fc9ae90eafbb14cbdb02797b2699121df7f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>It is possible that your son is experiencing a...</td>\n",
       "      <td>There are many potential causes of fever, shiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>170</td>\n",
       "      <td>301</td>\n",
       "      <td>0.919242</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.089471</td>\n",
       "      <td>53bb29915fb86c3bd323ddf3342af288e3110b3d8c470c...</td>\n",
       "      <td>0e65e1bbbf8133d14e940529817a55398d439910f1376e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Ex Machina is a 2014 science fiction film writ...</td>\n",
       "      <td>Yes, an individual share of stock has a unique...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>0.820365</td>\n",
       "      <td>0.104895</td>\n",
       "      <td>0.051510</td>\n",
       "      <td>9c2819093af7132a0a8561724a8df7917529cfc64b2f8c...</td>\n",
       "      <td>e755f987a374165af6f73c08941038e34ee0062bf016af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is not appropriate for me to provide specif...</td>\n",
       "      <td>It is not possible for me to diagnose the caus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>127</td>\n",
       "      <td>226</td>\n",
       "      <td>0.967646</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.371143</td>\n",
       "      <td>589cebdd144cf18ca850a3a1526f48d78b1dd95ba3a931...</td>\n",
       "      <td>cff5672845fcb077e279a866a9eb1390ef9e9d7b29f2af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It's not appropriate for me to provide specifi...</td>\n",
       "      <td>It is important to note that alternative medic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>283</td>\n",
       "      <td>0.949829</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.253266</td>\n",
       "      <td>1c35a009e503d36bb2c4626ddfba2ae6b24be70146daec...</td>\n",
       "      <td>84534927d51051602ae5f88b2ae19eb988ea070fa35837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>It is possible for a company to sell all of it...</td>\n",
       "      <td>It is possible that you have an eye infection ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>219</td>\n",
       "      <td>287</td>\n",
       "      <td>0.902927</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.085823</td>\n",
       "      <td>cf4d4ae33be0d63098895485b069421edfb7479f3843df...</td>\n",
       "      <td>dfc5ff6c3f98621ef4cf550d48e04ae18eb366f4643ac9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>Both are saying essentially the same thing.  T...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.887495</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.090905</td>\n",
       "      <td>6a66e7a115420485acd54c0cb9e5dc4e9e508b29af4b54...</td>\n",
       "      <td>714b04dd8923e09ea3f370b93660441d792104140d13d3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>they apply it to my next payment That's what m...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.844469</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.066986</td>\n",
       "      <td>7f8cd548cf57f327155b067b8fbdbfc3b124bea38e84e5...</td>\n",
       "      <td>f3372df44a7f627dbf381c0e0732206e97b01b859f5172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>Anchor_Explainer</td>\n",
       "      <td>Hello, Thanks for your query.This can occur du...</td>\n",
       "      <td>thanks for your query, the bump could be secon...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>0.962259</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.255573</td>\n",
       "      <td>797c3e2456febfd1031497754c2da79b9b0cb00f770118...</td>\n",
       "      <td>b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>You can receive all the Money in your Bank. By...</td>\n",
       "      <td>Yes, PMI is what the lender requires to loan y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>253</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.101139</td>\n",
       "      <td>24d5ad95c1e2717ba84af4c43ce69d1a5f8e291f22a73f...</td>\n",
       "      <td>09bf24d18aaa05cf3b4bef3bd89ef036504ef42e46f235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Hello dearWarm  welcome to Healthcaremagic.com...</td>\n",
       "      <td>Thanks for your question on Healthcare Magic. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>129</td>\n",
       "      <td>194</td>\n",
       "      <td>0.871972</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>0.093720</td>\n",
       "      <td>e5e2abf2af1e944f04c8c9d000f7dbf4e66c4d234f83a9...</td>\n",
       "      <td>47e9700fa53a47fb40d1025a89aec77680c41248c82119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>You'll likely see several more scary market ev...</td>\n",
       "      <td>There is more than one exchange where stock ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>173</td>\n",
       "      <td>176</td>\n",
       "      <td>0.871878</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.095888</td>\n",
       "      <td>7bd59b306ee4f86d66ea0c02c7c6ad2f0c42df557f3731...</td>\n",
       "      <td>01a0585172d6ef251986f3a3c948aba03c79e8c8bdef42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>In computer science, a dependency refers to th...</td>\n",
       "      <td>It is important to get the lump on your grands...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>207</td>\n",
       "      <td>216</td>\n",
       "      <td>0.904087</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.091357</td>\n",
       "      <td>4798eabcb307138abb04e6269df94f070e1aa25515cc7d...</td>\n",
       "      <td>739372dc4bf92366bc991aab94a8f80ee06f90e30c51cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>A Tensor Processing Unit (TPU) is a custom acc...</td>\n",
       "      <td>It is difficult to determine the frequency wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>256</td>\n",
       "      <td>0.935912</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.070008</td>\n",
       "      <td>06146f733e03754c43c81770ce1e2b06934eb318f50d93...</td>\n",
       "      <td>ed76b4ba45aefde60af94db8dd4cb92a1c331f5ce954bf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>Yes, it is still possible to receive a physica...</td>\n",
       "      <td>If a company's stock is de-listed from a stock...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "      <td>199</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.138780</td>\n",
       "      <td>2a25f893b9a12bf8353ec30061c37287265b907b5c58ff...</td>\n",
       "      <td>1ee4d90a14aa0daa48c63a9928631614de89049effce8a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>TL;DR: The date they were granted.  (Usually, ...</td>\n",
       "      <td>Reuters has a service you can subscribe to tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>71</td>\n",
       "      <td>249</td>\n",
       "      <td>0.953523</td>\n",
       "      <td>0.122093</td>\n",
       "      <td>0.073148</td>\n",
       "      <td>da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...</td>\n",
       "      <td>e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>AT&amp;T Labs is the research &amp; development divisi...</td>\n",
       "      <td>Automated decision-making (ADM) involves the u...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>0.919497</td>\n",
       "      <td>0.123377</td>\n",
       "      <td>0.090163</td>\n",
       "      <td>419fe06a0a8966dddad1519cb2bca2a274c12f552c6461...</td>\n",
       "      <td>a076837b4d14ff5dbce23f39055fe4e80396f079894657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>I agree, one should not let the tax tail wag t...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service. I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>164</td>\n",
       "      <td>263</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>2c05fd1a84cc8d7fedd6bed6fba06a00a6a7c0997fec7f...</td>\n",
       "      <td>927b2d3618050b8fae3f7fd9de001dc9fb5c0708b52efd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Automated decision-making refers to the use of...</td>\n",
       "      <td>Predictive analytics is a type of data analysi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>182</td>\n",
       "      <td>212</td>\n",
       "      <td>0.965771</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.215914</td>\n",
       "      <td>bab7c7ca0e1ba531721cb2418eb0b0fde40769c709323d...</td>\n",
       "      <td>150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Institutional investors are organizations that...</td>\n",
       "      <td>It is difficult to accurately diagnose a medic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.060458</td>\n",
       "      <td>0e734fca68abfc1f7fddb8983be80b9192ed2dfb0a16a8...</td>\n",
       "      <td>2b591b40c2ea9a0aefe618acd99c2b499c7eb759592aaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>Pimples on the face and bumps on the forehead ...</td>\n",
       "      <td>Peter Wegner is a computer scientist and profe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>87</td>\n",
       "      <td>153</td>\n",
       "      <td>0.820352</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>aba7b27f71d2c00c2a0d59dc92483be3d2dc9f0e368c31...</td>\n",
       "      <td>1ef829cbe6540a96630abe7a67b0d0397a44b2321cbdb9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Detector         Explainer  \\\n",
       "0   DetectorDetectGPT  Anchor_Explainer   \n",
       "1   DetectorDetectGPT  Anchor_Explainer   \n",
       "2   DetectorDetectGPT  Anchor_Explainer   \n",
       "3   DetectorDetectGPT  Anchor_Explainer   \n",
       "4   DetectorDetectGPT  Anchor_Explainer   \n",
       "5   DetectorDetectGPT  Anchor_Explainer   \n",
       "6   DetectorDetectGPT    LIME_Explainer   \n",
       "7   DetectorDetectGPT    LIME_Explainer   \n",
       "8   DetectorDetectGPT    LIME_Explainer   \n",
       "9   DetectorDetectGPT    LIME_Explainer   \n",
       "10  DetectorDetectGPT    LIME_Explainer   \n",
       "11  DetectorDetectGPT    LIME_Explainer   \n",
       "12  DetectorDetectGPT    SHAP_Explainer   \n",
       "13  DetectorDetectGPT    SHAP_Explainer   \n",
       "14  DetectorDetectGPT    SHAP_Explainer   \n",
       "15  DetectorDetectGPT    SHAP_Explainer   \n",
       "16  DetectorDetectGPT    SHAP_Explainer   \n",
       "17  DetectorDetectGPT    SHAP_Explainer   \n",
       "18    DetectorRadford  Anchor_Explainer   \n",
       "19    DetectorRadford  Anchor_Explainer   \n",
       "20    DetectorRadford  Anchor_Explainer   \n",
       "21    DetectorRadford  Anchor_Explainer   \n",
       "22    DetectorRadford  Anchor_Explainer   \n",
       "23    DetectorRadford  Anchor_Explainer   \n",
       "24    DetectorRadford    LIME_Explainer   \n",
       "25    DetectorRadford    LIME_Explainer   \n",
       "26    DetectorRadford    LIME_Explainer   \n",
       "27    DetectorRadford    LIME_Explainer   \n",
       "28    DetectorRadford    LIME_Explainer   \n",
       "29    DetectorRadford    LIME_Explainer   \n",
       "30    DetectorRadford    SHAP_Explainer   \n",
       "31    DetectorRadford    SHAP_Explainer   \n",
       "32    DetectorRadford    SHAP_Explainer   \n",
       "33    DetectorRadford    SHAP_Explainer   \n",
       "34    DetectorRadford    SHAP_Explainer   \n",
       "35    DetectorRadford    SHAP_Explainer   \n",
       "36        DetectorGuo  Anchor_Explainer   \n",
       "37        DetectorGuo  Anchor_Explainer   \n",
       "38        DetectorGuo  Anchor_Explainer   \n",
       "39        DetectorGuo  Anchor_Explainer   \n",
       "40        DetectorGuo  Anchor_Explainer   \n",
       "41        DetectorGuo  Anchor_Explainer   \n",
       "42        DetectorGuo    LIME_Explainer   \n",
       "43        DetectorGuo    LIME_Explainer   \n",
       "44        DetectorGuo    LIME_Explainer   \n",
       "45        DetectorGuo    LIME_Explainer   \n",
       "46        DetectorGuo    LIME_Explainer   \n",
       "47        DetectorGuo    LIME_Explainer   \n",
       "48        DetectorGuo    SHAP_Explainer   \n",
       "49        DetectorGuo    SHAP_Explainer   \n",
       "50        DetectorGuo    SHAP_Explainer   \n",
       "51        DetectorGuo    SHAP_Explainer   \n",
       "52        DetectorGuo    SHAP_Explainer   \n",
       "53        DetectorGuo    SHAP_Explainer   \n",
       "\n",
       "                                 Documents Phases 1+3  \\\n",
       "0   Both are saying essentially the same thing.  T...   \n",
       "1   It can be a good idea to follow the advice of ...   \n",
       "2   It is possible that a medical bill may have be...   \n",
       "3   Technical analysts use charts and other techni...   \n",
       "4   It is possible that stress could be contributi...   \n",
       "5   It's important to talk to a healthcare provide...   \n",
       "6   Yes. I can by all means start my own company a...   \n",
       "7   Hello and Welcome to ‘Ask A Doctor’ service.I ...   \n",
       "8   Limit books are managed by exchanges. If an or...   \n",
       "9   Multivariate statistics is a branch of statist...   \n",
       "10  Yes, buying REITs (Real Estate Investment Trus...   \n",
       "11  In literature, a trope is a common theme or el...   \n",
       "12  If your counterparty sent money to a correspon...   \n",
       "13  Most of the side effects of radiotherapy, incl...   \n",
       "14  I agree, one should not let the tax tail wag t...   \n",
       "15  Thanks for your question on Health Care Magic....   \n",
       "16  Alonzo Church (June 14, 1903 – August 11, 1995...   \n",
       "17  AT&T Labs is the research & development divisi...   \n",
       "18  Waldenstrom's macroglobulinemia is a type of l...   \n",
       "19  It is generally not possible for an adult to b...   \n",
       "20  It is possible that your dental implants could...   \n",
       "21  It can be a good idea to follow the advice of ...   \n",
       "22  It is possible to get pregnant if you have sex...   \n",
       "23  I've heard of handyman type people making a li...   \n",
       "24  It's possible that the pain in your back is re...   \n",
       "25  It doesn't generally matter, and I'm not sure ...   \n",
       "26  TL;DR: The date they were granted.  (Usually, ...   \n",
       "27  The price of fuel in Australia, like in any ot...   \n",
       "28  Investing in stocks can be a good idea if you ...   \n",
       "29  Credit unions are not-for-profit financial coo...   \n",
       "30  Predictive analytics encompasses a variety of ...   \n",
       "31  If your counterparty sent money to a correspon...   \n",
       "32  Alonzo Church (June 14, 1903 – August 11, 1995...   \n",
       "33  Trailing 12-month total returns, or TTM return...   \n",
       "34  It is possible that your son is experiencing a...   \n",
       "35  Ex Machina is a 2014 science fiction film writ...   \n",
       "36  It is not appropriate for me to provide specif...   \n",
       "37  It's not appropriate for me to provide specifi...   \n",
       "38  It is possible for a company to sell all of it...   \n",
       "39  I've heard of handyman type people making a li...   \n",
       "40  Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "41  Hello, Thanks for your query.This can occur du...   \n",
       "42  You can receive all the Money in your Bank. By...   \n",
       "43  Hello dearWarm  welcome to Healthcaremagic.com...   \n",
       "44  You'll likely see several more scary market ev...   \n",
       "45  In computer science, a dependency refers to th...   \n",
       "46  A Tensor Processing Unit (TPU) is a custom acc...   \n",
       "47  Yes, it is still possible to receive a physica...   \n",
       "48  TL;DR: The date they were granted.  (Usually, ...   \n",
       "49  AT&T Labs is the research & development divisi...   \n",
       "50  I agree, one should not let the tax tail wag t...   \n",
       "51  Automated decision-making refers to the use of...   \n",
       "52  Institutional investors are organizations that...   \n",
       "53  Pimples on the face and bumps on the forehead ...   \n",
       "\n",
       "                                 Documents Phases 2+4  f(a)  f(b)   GT a  \\\n",
       "0   Assuming you live in the US, it is quite norma...     0     0   True   \n",
       "1   It is generally a good idea to use sponsorship...     0     0  False   \n",
       "2   Credit unions are not-for-profit financial coo...     0     0  False   \n",
       "3   Option contracts are generally not subject to ...     0     0  False   \n",
       "4   It is generally not recommended for individual...     0     0  False   \n",
       "5   It is important to get the lump on your grands...     0     0  False   \n",
       "6   thanks for your query, the bump could be secon...     1     1   True   \n",
       "7   It doesn't generally matter, and I'm not sure ...     1     1   True   \n",
       "8   Reuters has a service you can subscribe to tha...     1     1   True   \n",
       "9   There are many potential causes of fever, shiv...     0     0  False   \n",
       "10  Investing in stocks can be a good idea if you ...     0     0  False   \n",
       "11  You only have to hold the shares at the openin...     0     0  False   \n",
       "12  As your is a very specific case, please get an...     1     1   True   \n",
       "13  Hello and Welcome to ‘Ask A Doctor’ service. I...     1     1   True   \n",
       "14  The UK has historically aggressive financial l...     1     1   True   \n",
       "15  Thanks for your question on Healthcare Magic. ...     0     0   True   \n",
       "16  Stock support and resistance levels are genera...     0     0   True   \n",
       "17  Automated decision-making (ADM) involves the u...     0     0   True   \n",
       "18  It is not uncommon for pain to increase after ...     0     0  False   \n",
       "19  It is not appropriate for me to provide specif...     0     0  False   \n",
       "20  Foamy feces can be caused by a variety of fact...     0     0  False   \n",
       "21  It's difficult to diagnose a medical condition...     0     0  False   \n",
       "22  It is possible that you may have bruised your ...     0     0  False   \n",
       "23  Both are saying essentially the same thing.  T...     1     1   True   \n",
       "24  If a trendline or pattern breaks due to some b...     1     1  False   \n",
       "25  Hello, Thanks for your query.This can occur du...     1     1   True   \n",
       "26  It is possible for cysts to develop in the liv...     1     1   True   \n",
       "27  I'm not familiar with the term \"cake and under...     0     0  False   \n",
       "28  Share owner services, also known as shareholde...     0     0  False   \n",
       "29  Gordon Bell is a computer scientist and electr...     0     0  False   \n",
       "30  Human intelligence is the intellectual capabil...     1     1   True   \n",
       "31  As your is a very specific case, please get an...     1     1   True   \n",
       "32  Sir Charles Antony Richard Hoare (Tony Hoare o...     1     1   True   \n",
       "33  There are several possible causes for shiverin...     0     0  False   \n",
       "34  There are many potential causes of fever, shiv...     0     0  False   \n",
       "35  Yes, an individual share of stock has a unique...     0     0   True   \n",
       "36  It is not possible for me to diagnose the caus...     0     0  False   \n",
       "37  It is important to note that alternative medic...     0     0  False   \n",
       "38  It is possible that you have an eye infection ...     0     0  False   \n",
       "39  Both are saying essentially the same thing.  T...     1     1   True   \n",
       "40  they apply it to my next payment That's what m...     1     1   True   \n",
       "41  thanks for your query, the bump could be secon...     1     1   True   \n",
       "42  Yes, PMI is what the lender requires to loan y...     1     1   True   \n",
       "43  Thanks for your question on Healthcare Magic. ...     1     1   True   \n",
       "44  There is more than one exchange where stock ca...     1     1   True   \n",
       "45  It is important to get the lump on your grands...     0     0  False   \n",
       "46  It is difficult to determine the frequency wit...     0     0  False   \n",
       "47  If a company's stock is de-listed from a stock...     0     0  False   \n",
       "48  Reuters has a service you can subscribe to tha...     1     1   True   \n",
       "49  Automated decision-making (ADM) involves the u...     1     1   True   \n",
       "50  Hello and Welcome to ‘Ask A Doctor’ service. I...     1     1   True   \n",
       "51  Predictive analytics is a type of data analysi...     0     0  False   \n",
       "52  It is difficult to accurately diagnose a medic...     0     0  False   \n",
       "53  Peter Wegner is a computer scientist and profe...     0     0  False   \n",
       "\n",
       "     GT b  idx a  idx b  Spacy Similarity  Jaccard Similarity  \\\n",
       "0    True     52    238          0.867257            0.176101   \n",
       "1   False    217    234          0.955040            0.164062   \n",
       "2   False    145    161          0.968658            0.190476   \n",
       "3   False    284    299          0.970607            0.179487   \n",
       "4   False     72    244          0.969343            0.195804   \n",
       "5   False     29    216          0.961467            0.318182   \n",
       "6    True     54    175          0.914531            0.127907   \n",
       "7    True      4     40          0.880217            0.101449   \n",
       "8    True    191    249          0.943610            0.137931   \n",
       "9   False    180    301          0.916336            0.108108   \n",
       "10  False      2    196          0.963769            0.215278   \n",
       "11   True     73    109          0.797912            0.063636   \n",
       "12   True     22     82          0.949592            0.142857   \n",
       "13   True    128    263          0.952658            0.097015   \n",
       "14   True    164    237          0.915177            0.128205   \n",
       "15   True     63    194          0.964194            0.309859   \n",
       "16  False    140    267          0.764695            0.064748   \n",
       "17   True     92    102          0.919497            0.123377   \n",
       "18  False    132    255          0.983331            0.240602   \n",
       "19  False     59    127          0.961798            0.185185   \n",
       "20  False     69    285          0.941338            0.221374   \n",
       "21  False    217    258          0.962063            0.168000   \n",
       "22  False    113    297          0.975657            0.198675   \n",
       "23   True      0     52          0.887495            0.135135   \n",
       "24  False     74    248          0.942111            0.160305   \n",
       "25   True     40    163          0.928879            0.108108   \n",
       "26  False     71    227          0.963289            0.157576   \n",
       "27  False     47    125          0.886263            0.202020   \n",
       "28  False    196    272          0.939235            0.135338   \n",
       "29  False    161    204          0.865433            0.100000   \n",
       "30   True      6    247          0.955458            0.108374   \n",
       "31   True     22     82          0.949592            0.142857   \n",
       "32   True    140    269          0.971317            0.145570   \n",
       "33  False    130    286          0.853402            0.119497   \n",
       "34  False    170    301          0.919242            0.160000   \n",
       "35  False    131    138          0.820365            0.104895   \n",
       "36  False    127    226          0.967646            0.372727   \n",
       "37  False     64    283          0.949829            0.252101   \n",
       "38  False    219    287          0.902927            0.114286   \n",
       "39   True      0     52          0.887495            0.135135   \n",
       "40   True      5     50          0.844469            0.166667   \n",
       "41   True    163    175          0.962259            0.236559   \n",
       "42   True    100    253          0.951815            0.128571   \n",
       "43   True    129    194          0.871972            0.092715   \n",
       "44   True    173    176          0.871878            0.128834   \n",
       "45  False    207    216          0.904087            0.122449   \n",
       "46  False     33    256          0.935912            0.108571   \n",
       "47  False     85    199          0.964400            0.174242   \n",
       "48   True     71    249          0.953523            0.122093   \n",
       "49   True     92    102          0.919497            0.123377   \n",
       "50   True    164    263          0.886619            0.131034   \n",
       "51  False    182    212          0.965771            0.166667   \n",
       "52  False      3     80          0.927414            0.088608   \n",
       "53  False     87    153          0.820352            0.093333   \n",
       "\n",
       "    Cosine Similarity tfidf  \\\n",
       "0                  0.153790   \n",
       "1                  0.203377   \n",
       "2                  0.080400   \n",
       "3                  0.187225   \n",
       "4                  0.156450   \n",
       "5                  0.317253   \n",
       "6                  0.112285   \n",
       "7                  0.054935   \n",
       "8                  0.072292   \n",
       "9                  0.039615   \n",
       "10                 0.234383   \n",
       "11                 0.043512   \n",
       "12                 0.153888   \n",
       "13                 0.109509   \n",
       "14                 0.094831   \n",
       "15                 0.289584   \n",
       "16                 0.047884   \n",
       "17                 0.090163   \n",
       "18                 0.161284   \n",
       "19                 0.096715   \n",
       "20                 0.198382   \n",
       "21                 0.160950   \n",
       "22                 0.163071   \n",
       "23                 0.090905   \n",
       "24                 0.177328   \n",
       "25                 0.108308   \n",
       "26                 0.095777   \n",
       "27                 0.104183   \n",
       "28                 0.108089   \n",
       "29                 0.035942   \n",
       "30                 0.065098   \n",
       "31                 0.153888   \n",
       "32                 0.166877   \n",
       "33                 0.058908   \n",
       "34                 0.089471   \n",
       "35                 0.051510   \n",
       "36                 0.371143   \n",
       "37                 0.253266   \n",
       "38                 0.085823   \n",
       "39                 0.090905   \n",
       "40                 0.066986   \n",
       "41                 0.255573   \n",
       "42                 0.101139   \n",
       "43                 0.093720   \n",
       "44                 0.095888   \n",
       "45                 0.091357   \n",
       "46                 0.070008   \n",
       "47                 0.138780   \n",
       "48                 0.073148   \n",
       "49                 0.090163   \n",
       "50                 0.080872   \n",
       "51                 0.215914   \n",
       "52                 0.060458   \n",
       "53                 0.066055   \n",
       "\n",
       "                                               hash a  \\\n",
       "0   714b04dd8923e09ea3f370b93660441d792104140d13d3...   \n",
       "1   c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...   \n",
       "2   dd5eebda3bd1fe324db9194206ddba635deb68071485af...   \n",
       "3   c8a011be98cb45f35f081b77177e54806135e858720719...   \n",
       "4   2896cd03f5a9e20c7aabbc57edfa78908bc88025779a18...   \n",
       "5   f16ea55e488aaf8c01f3e55cb6a6f44112e5f3795c4962...   \n",
       "6   20951d0d491f061e02e85cf742afd26ae0ebec6b895af3...   \n",
       "7   99e2f97d4e1e44bdba155c393c422e43995942290aac2f...   \n",
       "8   cb18fc73b58feeb14211103bba236c3a96f4bc881f323c...   \n",
       "9   4c1cbef644e5ad6a64c2209098c975c58e5f80d93e25f5...   \n",
       "10  398430643258bb00cedee78053fb66f240d53d21b390dc...   \n",
       "11  0e207de7f9a7ecde671349b9850fff2eabec0b8012acee...   \n",
       "12  3dd1514de75255da157c91f80bf9745485cc700f81cf40...   \n",
       "13  f84e28f87753147cf76ac12e491d3b6718e52a2160ef32...   \n",
       "14  2c05fd1a84cc8d7fedd6bed6fba06a00a6a7c0997fec7f...   \n",
       "15  477d8f805d1a6f0f573a60d6dee3dc0aad68fc4eb7ca59...   \n",
       "16  652189ac593cc7b3afcb7e0bd1c48f5b8209e21ca51594...   \n",
       "17  419fe06a0a8966dddad1519cb2bca2a274c12f552c6461...   \n",
       "18  4f3d9b0506f8558d1e36d0c1dfa7ad6b17e57a1eff8400...   \n",
       "19  bf5cc4428ba8a20b7f40625ce7893a43aa9b3652f73d1c...   \n",
       "20  dec23aa4bf1874d80f869e877d1828ee8b0b9489aafe5c...   \n",
       "21  c4c411239613f735fe43246c9c0108a876ff3ebcf723ee...   \n",
       "22  7683b9bff3eca91a14d3f4db144631325c4989a486b9ef...   \n",
       "23  6a66e7a115420485acd54c0cb9e5dc4e9e508b29af4b54...   \n",
       "24  344f46edfe91a144674da8ef3262a58698950066cae548...   \n",
       "25  5ae5387b74d5d39364702c87dfff6d52121b249c4e7e6b...   \n",
       "26  da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...   \n",
       "27  a7b3bdfabc7637684099747710ffcb3511ba513b324be2...   \n",
       "28  f022d78aadca7c71859f5ac0face8af6733d02c0e59b37...   \n",
       "29  51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...   \n",
       "30  42b84c175e792cbace8c18218652b0e5fc172d0722913d...   \n",
       "31  3dd1514de75255da157c91f80bf9745485cc700f81cf40...   \n",
       "32  652189ac593cc7b3afcb7e0bd1c48f5b8209e21ca51594...   \n",
       "33  e398f19e5fbb9d1db41882037d2b1f98d0bf09b76e0e01...   \n",
       "34  53bb29915fb86c3bd323ddf3342af288e3110b3d8c470c...   \n",
       "35  9c2819093af7132a0a8561724a8df7917529cfc64b2f8c...   \n",
       "36  589cebdd144cf18ca850a3a1526f48d78b1dd95ba3a931...   \n",
       "37  1c35a009e503d36bb2c4626ddfba2ae6b24be70146daec...   \n",
       "38  cf4d4ae33be0d63098895485b069421edfb7479f3843df...   \n",
       "39  6a66e7a115420485acd54c0cb9e5dc4e9e508b29af4b54...   \n",
       "40  7f8cd548cf57f327155b067b8fbdbfc3b124bea38e84e5...   \n",
       "41  797c3e2456febfd1031497754c2da79b9b0cb00f770118...   \n",
       "42  24d5ad95c1e2717ba84af4c43ce69d1a5f8e291f22a73f...   \n",
       "43  e5e2abf2af1e944f04c8c9d000f7dbf4e66c4d234f83a9...   \n",
       "44  7bd59b306ee4f86d66ea0c02c7c6ad2f0c42df557f3731...   \n",
       "45  4798eabcb307138abb04e6269df94f070e1aa25515cc7d...   \n",
       "46  06146f733e03754c43c81770ce1e2b06934eb318f50d93...   \n",
       "47  2a25f893b9a12bf8353ec30061c37287265b907b5c58ff...   \n",
       "48  da9b052c0d269c574dcea123c6cb2f9fd6c299427e64b2...   \n",
       "49  419fe06a0a8966dddad1519cb2bca2a274c12f552c6461...   \n",
       "50  2c05fd1a84cc8d7fedd6bed6fba06a00a6a7c0997fec7f...   \n",
       "51  bab7c7ca0e1ba531721cb2418eb0b0fde40769c709323d...   \n",
       "52  0e734fca68abfc1f7fddb8983be80b9192ed2dfb0a16a8...   \n",
       "53  aba7b27f71d2c00c2a0d59dc92483be3d2dc9f0e368c31...   \n",
       "\n",
       "                                               hash b  \n",
       "0   60b992dfcad293c2fbe76d7842a4e469ba041ced7c5270...  \n",
       "1   e573296bc2021073ef94c1f60c1f097dc770793203b348...  \n",
       "2   51d1e47a3a3e56608b1fbc9a3f63dfd38a5d07ecd93837...  \n",
       "3   5b452735e59499d5c471fefba9bbcc41bc42763a5d8553...  \n",
       "4   ccda7c8a282ef5799ad05651df3cd89230aaec28541207...  \n",
       "5   739372dc4bf92366bc991aab94a8f80ee06f90e30c51cd...  \n",
       "6   b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...  \n",
       "7   5ae5387b74d5d39364702c87dfff6d52121b249c4e7e6b...  \n",
       "8   e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...  \n",
       "9   0e65e1bbbf8133d14e940529817a55398d439910f1376e...  \n",
       "10  f022d78aadca7c71859f5ac0face8af6733d02c0e59b37...  \n",
       "11  d2acb3efcbd10c6b0656218ca86c3820e9c746e04749e4...  \n",
       "12  6b25114906789fc86fa719f0394277f9289f080cbbbc7e...  \n",
       "13  927b2d3618050b8fae3f7fd9de001dc9fb5c0708b52efd...  \n",
       "14  8f1e6fbc370ec179204b1bddc3deb954e294f8e3c4e61c...  \n",
       "15  47e9700fa53a47fb40d1025a89aec77680c41248c82119...  \n",
       "16  a98eedec2f3faa2bedc5b48424bffa0fb467ad56d4bcb3...  \n",
       "17  a076837b4d14ff5dbce23f39055fe4e80396f079894657...  \n",
       "18  60707fbb129a62fd3e3a762cb9a9c8d57a8781c4d65dda...  \n",
       "19  589cebdd144cf18ca850a3a1526f48d78b1dd95ba3a931...  \n",
       "20  a51392d3a9976b33ff5ec8bd300646491be0b2445376f0...  \n",
       "21  20426911ee138f3f21f1cd78d1e567e2055a2c9124c047...  \n",
       "22  7633a568e18a48d3009a3b9bbe4302d09d60e320a8a29d...  \n",
       "23  714b04dd8923e09ea3f370b93660441d792104140d13d3...  \n",
       "24  436c7e65081fa1a6597579ee32d8b6a0d7f88f1a3d8af0...  \n",
       "25  797c3e2456febfd1031497754c2da79b9b0cb00f770118...  \n",
       "26  9eac8625f1ce76e3ab8a5f3aac8669a179e0abf5fff35c...  \n",
       "27  3d19b3cac118bd0523a41e321108d01137a1f51436fde8...  \n",
       "28  3fe9da95aee2d9fb76a7f18840ca38645ba725f62686e4...  \n",
       "29  49afdd008b7667f8c575f34ad378723e39feb10666c2e9...  \n",
       "30  936af4095ba223d41fa40efae657dfe1169abfac77176d...  \n",
       "31  6b25114906789fc86fa719f0394277f9289f080cbbbc7e...  \n",
       "32  0d824782624b2690ac2bde813a1cf6a7df527e243a4875...  \n",
       "33  e3aaa179881fc9ae90eafbb14cbdb02797b2699121df7f...  \n",
       "34  0e65e1bbbf8133d14e940529817a55398d439910f1376e...  \n",
       "35  e755f987a374165af6f73c08941038e34ee0062bf016af...  \n",
       "36  cff5672845fcb077e279a866a9eb1390ef9e9d7b29f2af...  \n",
       "37  84534927d51051602ae5f88b2ae19eb988ea070fa35837...  \n",
       "38  dfc5ff6c3f98621ef4cf550d48e04ae18eb366f4643ac9...  \n",
       "39  714b04dd8923e09ea3f370b93660441d792104140d13d3...  \n",
       "40  f3372df44a7f627dbf381c0e0732206e97b01b859f5172...  \n",
       "41  b2ae8a1624e51a0894124a9aaae1fb99c3557b284a96c7...  \n",
       "42  09bf24d18aaa05cf3b4bef3bd89ef036504ef42e46f235...  \n",
       "43  47e9700fa53a47fb40d1025a89aec77680c41248c82119...  \n",
       "44  01a0585172d6ef251986f3a3c948aba03c79e8c8bdef42...  \n",
       "45  739372dc4bf92366bc991aab94a8f80ee06f90e30c51cd...  \n",
       "46  ed76b4ba45aefde60af94db8dd4cb92a1c331f5ce954bf...  \n",
       "47  1ee4d90a14aa0daa48c63a9928631614de89049effce8a...  \n",
       "48  e6b72b7969bbad53385251e1baf30d9f72a75067fc1707...  \n",
       "49  a076837b4d14ff5dbce23f39055fe4e80396f079894657...  \n",
       "50  927b2d3618050b8fae3f7fd9de001dc9fb5c0708b52efd...  \n",
       "51  150471a9c2522b83a5910e0cd1e92838a39d967a2a183c...  \n",
       "52  2b591b40c2ea9a0aefe618acd99c2b499c7eb759592aaa...  \n",
       "53  1ef829cbe6540a96630abe7a67b0d0397a44b2321cbdb9...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./dataset_user_study_new.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].stack()[df[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].stack().duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.groupby([\"Detector\", \"Explainer\"])[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].apply(lambda group: len(group.stack()[group.stack().duplicated(keep=False)])).sum() == 0, \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.groupby([\"Detector\"])[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].apply(lambda group: len(group.stack()[group.stack().duplicated(keep=False)])).sum() == 0, \"Duplicate documents!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explainer\n",
       "Anchor_Explainer     9\n",
       "LIME_Explainer       4\n",
       "SHAP_Explainer      14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"Explainer\"])[[\"Documents Phases 1+3\", \"Documents Phases 2+4\"]].apply(lambda group: len(group.stack()[group.stack().duplicated(keep=False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove from .gitignore after user study\n",
    "# TODO change format to something else after user study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cache = {}\n",
    "def prediction_cached(detector, document):\n",
    "    id = (detector.__class__.__name__,document)\n",
    "    if id not in prediction_cache:\n",
    "        prediction_cache[id] = detector.predict_label([document])[0]\n",
    "    return prediction_cache[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache dir ./.cache\n",
      "Loading BASE model EleutherAI/pythia-70m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.20s)\n",
      "DONE (0.07s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "detector_detectgpt = DetectorDetectGPT()\n",
    "detector_radford = DetectorRadford()\n",
    "detector_guo = DetectorGuo()\n",
    "def get_random_df(df, seed=42):\n",
    "    selection = []\n",
    "    random.seed(seed)\n",
    "    for idx, row in tqdm(list(df.iterrows())):\n",
    "        detector = None\n",
    "        explainer = None\n",
    "        if row[\"Detector\"] == \"DetectorDetectGPT\":\n",
    "            detector = detector_detectgpt\n",
    "        if row[\"Detector\"] == \"DetectorRadford\":\n",
    "            detector = detector_radford\n",
    "        if row[\"Detector\"] == \"DetectorGuo\":\n",
    "            detector = detector_guo\n",
    "\n",
    "        if row[\"Explainer\"]  == \"Anchor_Explainer\":\n",
    "            explainer = Anchor_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"LIME_Explainer\":\n",
    "            explainer = LIME_Explainer(detector)\n",
    "        if row[\"Explainer\"]  == \"SHAP_Explainer\":\n",
    "            explainer = SHAP_Explainer(detector)\n",
    "        #               all documents not in exclude_list                                                                            without replacement\n",
    "        candidates = [i for i in range(0,len(documents)) if i not in exclude_list and (len(selection) == 0 or document_ids[i] not in list(zip(*selection))[8] + list(zip(*selection))[9])]\n",
    "        random.shuffle(candidates)\n",
    "        pairs = None\n",
    "        while True:\n",
    "            pairs = [(candidates[0], candidates[1])]\n",
    "            if prediction_cached(detector, documents[candidates[0]]) == row[\"f(a)\"] and prediction_cached(detector, documents[candidates[0]]) == prediction_cached(detector, documents[candidates[1]]):\n",
    "                break\n",
    "            candidates = candidates[2:]\n",
    "        \n",
    "\n",
    "        selection = update_selection(selection, pairs, explainer, detector)\n",
    "    return pd.DataFrame(selection, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RANDOM_SELECTIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [03:51<00:00,  4.28s/it]\n",
      "100%|██████████| 54/54 [02:08<00:00,  2.37s/it]\n",
      "100%|██████████| 54/54 [01:52<00:00,  2.09s/it]\n",
      "100%|██████████| 54/54 [01:41<00:00,  1.89s/it]\n",
      "100%|██████████| 54/54 [01:35<00:00,  1.77s/it]\n",
      "100%|██████████| 54/54 [01:32<00:00,  1.72s/it]\n",
      "100%|██████████| 54/54 [01:24<00:00,  1.57s/it]\n",
      "100%|██████████| 54/54 [01:28<00:00,  1.63s/it]\n",
      "100%|██████████| 54/54 [01:29<00:00,  1.66s/it]\n",
      "100%|██████████| 54/54 [01:28<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "dfs_random = [get_random_df(df, seed=i) for i in range(0,N_RANDOM_SELECTIONS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that produce dataframes for export to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_ind\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_FI(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in tqdm(list(df.iterrows())):\n",
    "            #    print(row)\n",
    "                if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "                W, _ = get_explanation_matrix_W([a,b], explainer, quiet=True)\n",
    "\n",
    "                sim = cosine_similarity(W) \n",
    "                cosine_similarity_ = sim[0,1]\n",
    "\n",
    "                n_tokens_overlap_in_w = np.all(W != 0, axis = 0).sum()# / np.any(W != 0, axis = 0).sum()\n",
    "                \n",
    "\n",
    "                results.append((\n",
    "                    idx,\n",
    "                    explainer.__class__.__name__,\n",
    "                    detector.__class__.__name__,\n",
    "                    cosine_similarity_,\n",
    "                    n_tokens_overlap_in_w,\n",
    "\n",
    "                 ))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"Cosine Similarity\",\n",
    "        \"\\\\# Common Features\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_Anchor(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "\n",
    "        explainer = Anchor_Explainer(detector)\n",
    "        for idx, row in tqdm(list(df.iterrows())):\n",
    "        #    print(row)\n",
    "            if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                continue\n",
    "            a = row[\"Documents Phases 1+3\"]\n",
    "            b = row[\"Documents Phases 2+4\"]\n",
    "            anchors, p, ids = get_anchors_at_each_k([a,b], explainer, quiet=True)\n",
    "            # find anchors that occur more than once in the dataset, then remove duplicates (created by looping) with set()\n",
    "            duplicate_anchors = [set(anchor) for anchor in set([frozenset(anchor) for anchor in anchors if anchors.count(anchor) > 1])]\n",
    "            results.append((\n",
    "                idx,\n",
    "                explainer.__class__.__name__,\n",
    "                detector.__class__.__name__,\n",
    "                len(duplicate_anchors),\n",
    "                max([len(anchor) for anchor in duplicate_anchors]) if len(duplicate_anchors) else 0,\n",
    "                p[anchors.index(max(duplicate_anchors, key=lambda anchor: len(anchor)))] if len(duplicate_anchors) else 0\n",
    "                ))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"\\\\# Matching Anchors\",\n",
    "        \"Len Longest Matching Anchor\",\n",
    "        \"$\\\\theta$ Longest Matching Anchor\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_Document(df, selecting_combinations_only=False):\n",
    "    results = []\n",
    "    for detector_class in [DetectorGuo, DetectorRadford, DetectorDetectGPT]:\n",
    "        detector = detector_class(metadata_only=True)\n",
    "        for explainer_class in [LIME_Explainer,SHAP_Explainer, Anchor_Explainer]:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, row in tqdm(list(df.iterrows())):\n",
    "            #    print(row)\n",
    "                if selecting_combinations_only and row[\"Detector\"] != detector.__class__.__name__:\n",
    "                    continue\n",
    "                a = row[\"Documents Phases 1+3\"]\n",
    "                b = row[\"Documents Phases 2+4\"]\n",
    "            \n",
    "                results.append((\n",
    "                    idx,\n",
    "                    explainer.__class__.__name__,\n",
    "                    detector.__class__.__name__,\n",
    "                        row[\"Spacy Similarity\"],\n",
    "                        row[\"Jaccard Similarity\"],\n",
    "                        row[\"Cosine Similarity tfidf\"]))\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"idx\",\n",
    "        \"Explainer\",\n",
    "        \"Set\",\n",
    "        \"Spacy Similarity\",\n",
    "        \"Jaccard Similarity\",\n",
    "        \"Cosine Similarity tfidf\",\n",
    "        ])\n",
    "    df_results = df_results.set_index([\"Explainer\", \"Set\"])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Metric\", \"tstatistic\", \"pvalue\",\"Mean of Method\", \"Mean of {} Rand. Selections\".format(N_RANDOM_SELECTIONS), \"Gain Over Random\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and aggregate results by detector\n",
    "def get_results_detector_level(m_method, m_random):\n",
    "    t = []\n",
    "    for metric in m_method.columns:\n",
    "        for (detector, group_method), ((detector_r), group_random) in zip(m_method.groupby([\"Set\"]), m_random.groupby([\"Set\"])):\n",
    "            assert detector == detector_r\n",
    "            tstatistic, pvalue = ttest_ind(group_method[metric], group_random[metric])\n",
    "\n",
    "            t.append([detector[0], metric[0], tstatistic, pvalue, group_method[metric].mean(), group_random[metric].mean(), group_method[metric].mean() - group_random[metric].mean()])\n",
    "    df_results_detector_level = pd.DataFrame(t, columns=[\"Set\"]+columns).set_index([\"Metric\", \"Set\"])#.apply(get_p_asterisks_2samp).drop([\"pvalue\",\"tstatistic\"], axis=1)\n",
    "    return df_results_detector_level.reset_index().set_index([\"Metric\", \"Set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for entire selection\n",
    "def get_results_entire_selection(m_method, m_random):\n",
    "        t = []\n",
    "        for metric in m_method.columns:\n",
    "                tstatistic, pvalue = ttest_ind(m_method[metric], m_random[metric])\n",
    "                t.append([metric[0],  tstatistic, pvalue, m_method[metric].mean(), m_random[metric].mean(), m_method[metric].mean() - m_random[metric].mean()])\n",
    "        df_results_selection_level = pd.DataFrame(t, columns=columns)#.apply(get_p_asterisks_2samp).drop([\"pvalue\",\"tstatistic\"], axis=1)\n",
    "        # add additional descriptions\n",
    "        df_results_selection_level[\"Set\"] = \"All\"\n",
    "        return df_results_selection_level.reset_index().set_index([\"Metric\", \"Set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_significant(row, props=''):\n",
    "  #  display(s)\n",
    "    styles = [''] * len(row)\n",
    "    styles[-1] = 'font-weight: bold' if row[\"pvalue\"] <= 0.05 else ''\n",
    "    return styles\n",
    "def shade_by_type(row, props=''):\n",
    "\n",
    "  if(row.name[0] == \"Explanation Similarity in W\"):\n",
    "    return ['background-color:red'] * len(row)\n",
    "  else:\n",
    "    return [''] * len(row)\n",
    "def shade_by_type_index(row, props=''):\n",
    "  return ['background-color:red'] * 8 + [''] * 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 53991.04it/s]\n",
      "100%|██████████| 54/54 [00:00<?, ?it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53965.31it/s]\n",
      "100%|██████████| 54/54 [00:00<?, ?it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53658.47it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 53798.67it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 54107.12it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 27095.64it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 54223.71it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 119792.89it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 97752.45it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 82676.55it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 97853.80it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 108090.30it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 119875.31it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 107992.38it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 108162.57it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 179485.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5d4e_row0_col5, #T_a5d4e_row1_col5, #T_a5d4e_row2_col5, #T_a5d4e_row3_col5, #T_a5d4e_row4_col5, #T_a5d4e_row5_col5, #T_a5d4e_row6_col5, #T_a5d4e_row7_col5, #T_a5d4e_row8_col5, #T_a5d4e_row9_col5, #T_a5d4e_row10_col5, #T_a5d4e_row11_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_a5d4e_level0_col3, #T_a5d4e_level0_col4, #T_a5d4e_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5d4e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a5d4e_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_a5d4e_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_a5d4e_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">Spacy Similarity</th>\n",
       "      <th id=\"T_a5d4e_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_a5d4e_row0_col3\" class=\"data row0 col3\" >0.93</td>\n",
       "      <td id=\"T_a5d4e_row0_col4\" class=\"data row0 col4\" >0.88</td>\n",
       "      <td id=\"T_a5d4e_row0_col5\" class=\"data row0 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_a5d4e_row1_col3\" class=\"data row1 col3\" >0.92</td>\n",
       "      <td id=\"T_a5d4e_row1_col4\" class=\"data row1 col4\" >0.88</td>\n",
       "      <td id=\"T_a5d4e_row1_col5\" class=\"data row1 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_a5d4e_row2_col3\" class=\"data row2 col3\" >0.92</td>\n",
       "      <td id=\"T_a5d4e_row2_col4\" class=\"data row2 col4\" >0.88</td>\n",
       "      <td id=\"T_a5d4e_row2_col5\" class=\"data row2 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_a5d4e_row3_col3\" class=\"data row3 col3\" >0.92</td>\n",
       "      <td id=\"T_a5d4e_row3_col4\" class=\"data row3 col4\" >0.88</td>\n",
       "      <td id=\"T_a5d4e_row3_col5\" class=\"data row3 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Jaccard Similarity</th>\n",
       "      <th id=\"T_a5d4e_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_a5d4e_row4_col3\" class=\"data row4 col3\" >0.16</td>\n",
       "      <td id=\"T_a5d4e_row4_col4\" class=\"data row4 col4\" >0.12</td>\n",
       "      <td id=\"T_a5d4e_row4_col5\" class=\"data row4 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_a5d4e_row5_col3\" class=\"data row5 col3\" >0.15</td>\n",
       "      <td id=\"T_a5d4e_row5_col4\" class=\"data row5 col4\" >0.12</td>\n",
       "      <td id=\"T_a5d4e_row5_col5\" class=\"data row5 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_a5d4e_row6_col3\" class=\"data row6 col3\" >0.16</td>\n",
       "      <td id=\"T_a5d4e_row6_col4\" class=\"data row6 col4\" >0.12</td>\n",
       "      <td id=\"T_a5d4e_row6_col5\" class=\"data row6 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_a5d4e_row7_col3\" class=\"data row7 col3\" >0.16</td>\n",
       "      <td id=\"T_a5d4e_row7_col4\" class=\"data row7 col4\" >0.12</td>\n",
       "      <td id=\"T_a5d4e_row7_col5\" class=\"data row7 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level0_row8\" class=\"row_heading level0 row8\" rowspan=\"4\">Cosine Similarity tfidf</th>\n",
       "      <th id=\"T_a5d4e_level1_row8\" class=\"row_heading level1 row8\" >DetectorRadford</th>\n",
       "      <td id=\"T_a5d4e_row8_col3\" class=\"data row8 col3\" >0.12</td>\n",
       "      <td id=\"T_a5d4e_row8_col4\" class=\"data row8 col4\" >0.09</td>\n",
       "      <td id=\"T_a5d4e_row8_col5\" class=\"data row8 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row9\" class=\"row_heading level1 row9\" >DetectorGuo</th>\n",
       "      <td id=\"T_a5d4e_row9_col3\" class=\"data row9 col3\" >0.13</td>\n",
       "      <td id=\"T_a5d4e_row9_col4\" class=\"data row9 col4\" >0.08</td>\n",
       "      <td id=\"T_a5d4e_row9_col5\" class=\"data row9 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row10\" class=\"row_heading level1 row10\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_a5d4e_row10_col3\" class=\"data row10 col3\" >0.14</td>\n",
       "      <td id=\"T_a5d4e_row10_col4\" class=\"data row10 col4\" >0.08</td>\n",
       "      <td id=\"T_a5d4e_row10_col5\" class=\"data row10 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d4e_level1_row11\" class=\"row_heading level1 row11\" >All</th>\n",
       "      <td id=\"T_a5d4e_row11_col3\" class=\"data row11 col3\" >0.13</td>\n",
       "      <td id=\"T_a5d4e_row11_col4\" class=\"data row11 col4\" >0.08</td>\n",
       "      <td id=\"T_a5d4e_row11_col5\" class=\"data row11 col5\" >0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2cdf6a350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_Document(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_Document(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_document = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    "    .map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)  \n",
    "display(df_similarity_document)\n",
    "latex = df_similarity_document.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of documents (p < 0.05 bold)\", label=\"similarityDocuments\")\n",
    "latex_strings.append(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 64.74it/s] \n",
      "100%|██████████| 54/54 [00:05<00:00,  9.45it/s] \n",
      "100%|██████████| 54/54 [00:00<00:00, 62.06it/s] \n",
      "100%|██████████| 54/54 [00:06<00:00,  8.57it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 69.71it/s]\n",
      "100%|██████████| 54/54 [00:05<00:00,  9.75it/s]\n",
      "100%|██████████| 540/540 [00:06<00:00, 86.37it/s] \n",
      "100%|██████████| 540/540 [00:49<00:00, 10.88it/s]\n",
      "100%|██████████| 540/540 [00:06<00:00, 80.70it/s] \n",
      "100%|██████████| 540/540 [00:52<00:00, 10.24it/s]\n",
      "100%|██████████| 540/540 [00:06<00:00, 86.17it/s] \n",
      "100%|██████████| 540/540 [00:53<00:00, 10.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ab808_row4_col5, #T_ab808_row5_col5, #T_ab808_row6_col5, #T_ab808_row7_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_ab808_level0_col3, #T_ab808_level0_col4, #T_ab808_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ab808\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ab808_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_ab808_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_ab808_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">\\# Common Features</th>\n",
       "      <th id=\"T_ab808_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_ab808_row0_col3\" class=\"data row0 col3\" >12.22</td>\n",
       "      <td id=\"T_ab808_row0_col4\" class=\"data row0 col4\" >9.22</td>\n",
       "      <td id=\"T_ab808_row0_col5\" class=\"data row0 col5\" >3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_ab808_row1_col3\" class=\"data row1 col3\" >10.61</td>\n",
       "      <td id=\"T_ab808_row1_col4\" class=\"data row1 col4\" >8.70</td>\n",
       "      <td id=\"T_ab808_row1_col5\" class=\"data row1 col5\" >1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_ab808_row2_col3\" class=\"data row2 col3\" >5.00</td>\n",
       "      <td id=\"T_ab808_row2_col4\" class=\"data row2 col4\" >3.75</td>\n",
       "      <td id=\"T_ab808_row2_col5\" class=\"data row2 col5\" >1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_ab808_row3_col3\" class=\"data row3 col3\" >9.28</td>\n",
       "      <td id=\"T_ab808_row3_col4\" class=\"data row3 col4\" >7.22</td>\n",
       "      <td id=\"T_ab808_row3_col5\" class=\"data row3 col5\" >2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Cosine Similarity</th>\n",
       "      <th id=\"T_ab808_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_ab808_row4_col3\" class=\"data row4 col3\" >0.33</td>\n",
       "      <td id=\"T_ab808_row4_col4\" class=\"data row4 col4\" >0.18</td>\n",
       "      <td id=\"T_ab808_row4_col5\" class=\"data row4 col5\" >0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_ab808_row5_col3\" class=\"data row5 col3\" >0.38</td>\n",
       "      <td id=\"T_ab808_row5_col4\" class=\"data row5 col4\" >0.22</td>\n",
       "      <td id=\"T_ab808_row5_col5\" class=\"data row5 col5\" >0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_ab808_row6_col3\" class=\"data row6 col3\" >0.25</td>\n",
       "      <td id=\"T_ab808_row6_col4\" class=\"data row6 col4\" >0.10</td>\n",
       "      <td id=\"T_ab808_row6_col5\" class=\"data row6 col5\" >0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab808_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_ab808_row7_col3\" class=\"data row7 col3\" >0.32</td>\n",
       "      <td id=\"T_ab808_row7_col4\" class=\"data row7 col4\" >0.17</td>\n",
       "      <td id=\"T_ab808_row7_col5\" class=\"data row7 col5\" >0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2d8a151d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_FI(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_FI(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_fi = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    ".map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)\n",
    "display(df_similarity_fi)\n",
    "latex = df_similarity_fi.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)\", label=\"similarityFI\")\n",
    "latex_strings.append(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 588.66it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 551.75it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1110.60it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 772.62it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 788.71it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 1255.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dcb1c_row1_col5, #T_dcb1c_row2_col5, #T_dcb1c_row3_col5, #T_dcb1c_row5_col5, #T_dcb1c_row6_col5, #T_dcb1c_row7_col5, #T_dcb1c_row10_col5, #T_dcb1c_row11_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_dcb1c_level0_col3, #T_dcb1c_level0_col4, #T_dcb1c_level0_col5 {\n",
       "  rotatebox: {45}--rwrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dcb1c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dcb1c_level0_col3\" class=\"col_heading level0 col3\" >Mean of Method</th>\n",
       "      <th id=\"T_dcb1c_level0_col4\" class=\"col_heading level0 col4\" >Mean of 10 Rand. Selections</th>\n",
       "      <th id=\"T_dcb1c_level0_col5\" class=\"col_heading level0 col5\" >Gain Over Random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Set</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"4\">\\# Matching Anchors</th>\n",
       "      <th id=\"T_dcb1c_level1_row0\" class=\"row_heading level1 row0\" >DetectorRadford</th>\n",
       "      <td id=\"T_dcb1c_row0_col3\" class=\"data row0 col3\" >0.39</td>\n",
       "      <td id=\"T_dcb1c_row0_col4\" class=\"data row0 col4\" >0.10</td>\n",
       "      <td id=\"T_dcb1c_row0_col5\" class=\"data row0 col5\" >0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row1\" class=\"row_heading level1 row1\" >DetectorGuo</th>\n",
       "      <td id=\"T_dcb1c_row1_col3\" class=\"data row1 col3\" >0.94</td>\n",
       "      <td id=\"T_dcb1c_row1_col4\" class=\"data row1 col4\" >0.47</td>\n",
       "      <td id=\"T_dcb1c_row1_col5\" class=\"data row1 col5\" >0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row2\" class=\"row_heading level1 row2\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_dcb1c_row2_col3\" class=\"data row2 col3\" >0.39</td>\n",
       "      <td id=\"T_dcb1c_row2_col4\" class=\"data row2 col4\" >0.08</td>\n",
       "      <td id=\"T_dcb1c_row2_col5\" class=\"data row2 col5\" >0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row3\" class=\"row_heading level1 row3\" >All</th>\n",
       "      <td id=\"T_dcb1c_row3_col3\" class=\"data row3 col3\" >0.57</td>\n",
       "      <td id=\"T_dcb1c_row3_col4\" class=\"data row3 col4\" >0.21</td>\n",
       "      <td id=\"T_dcb1c_row3_col5\" class=\"data row3 col5\" >0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"4\">Len Longest Matching Anchor</th>\n",
       "      <th id=\"T_dcb1c_level1_row4\" class=\"row_heading level1 row4\" >DetectorRadford</th>\n",
       "      <td id=\"T_dcb1c_row4_col3\" class=\"data row4 col3\" >0.39</td>\n",
       "      <td id=\"T_dcb1c_row4_col4\" class=\"data row4 col4\" >0.10</td>\n",
       "      <td id=\"T_dcb1c_row4_col5\" class=\"data row4 col5\" >0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row5\" class=\"row_heading level1 row5\" >DetectorGuo</th>\n",
       "      <td id=\"T_dcb1c_row5_col3\" class=\"data row5 col3\" >0.94</td>\n",
       "      <td id=\"T_dcb1c_row5_col4\" class=\"data row5 col4\" >0.47</td>\n",
       "      <td id=\"T_dcb1c_row5_col5\" class=\"data row5 col5\" >0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row6\" class=\"row_heading level1 row6\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_dcb1c_row6_col3\" class=\"data row6 col3\" >0.39</td>\n",
       "      <td id=\"T_dcb1c_row6_col4\" class=\"data row6 col4\" >0.08</td>\n",
       "      <td id=\"T_dcb1c_row6_col5\" class=\"data row6 col5\" >0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row7\" class=\"row_heading level1 row7\" >All</th>\n",
       "      <td id=\"T_dcb1c_row7_col3\" class=\"data row7 col3\" >0.57</td>\n",
       "      <td id=\"T_dcb1c_row7_col4\" class=\"data row7 col4\" >0.21</td>\n",
       "      <td id=\"T_dcb1c_row7_col5\" class=\"data row7 col5\" >0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level0_row8\" class=\"row_heading level0 row8\" rowspan=\"4\">$\\theta$ Longest Matching Anchor</th>\n",
       "      <th id=\"T_dcb1c_level1_row8\" class=\"row_heading level1 row8\" >DetectorRadford</th>\n",
       "      <td id=\"T_dcb1c_row8_col3\" class=\"data row8 col3\" >0.14</td>\n",
       "      <td id=\"T_dcb1c_row8_col4\" class=\"data row8 col4\" >0.06</td>\n",
       "      <td id=\"T_dcb1c_row8_col5\" class=\"data row8 col5\" >0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row9\" class=\"row_heading level1 row9\" >DetectorGuo</th>\n",
       "      <td id=\"T_dcb1c_row9_col3\" class=\"data row9 col3\" >0.52</td>\n",
       "      <td id=\"T_dcb1c_row9_col4\" class=\"data row9 col4\" >0.33</td>\n",
       "      <td id=\"T_dcb1c_row9_col5\" class=\"data row9 col5\" >0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row10\" class=\"row_heading level1 row10\" >DetectorDetectGPT</th>\n",
       "      <td id=\"T_dcb1c_row10_col3\" class=\"data row10 col3\" >0.29</td>\n",
       "      <td id=\"T_dcb1c_row10_col4\" class=\"data row10 col4\" >0.08</td>\n",
       "      <td id=\"T_dcb1c_row10_col5\" class=\"data row10 col5\" >0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1c_level1_row11\" class=\"row_heading level1 row11\" >All</th>\n",
       "      <td id=\"T_dcb1c_row11_col3\" class=\"data row11 col3\" >0.32</td>\n",
       "      <td id=\"T_dcb1c_row11_col4\" class=\"data row11 col4\" >0.16</td>\n",
       "      <td id=\"T_dcb1c_row11_col5\" class=\"data row11 col5\" >0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2c9948510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_method = get_metrics_Anchor(df, selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\"]) # note that nothing is aggregated here, this is just to match the shape of the next line:\n",
    "m_random = get_metrics_Anchor(pd.concat(dfs_random), selecting_combinations_only=True).groupby([\"Set\", \"Explainer\", \"idx\"]).agg([\"mean\", \"std\"]) # take mean score across random runs for each metric\n",
    "\n",
    "df_similarity_anchors = pd.concat([get_results_entire_selection(m_method, m_random),get_results_detector_level(m_method, m_random)]).sort_index(ascending=False).style.apply(highlight_significant, axis=1).hide([\"tstatistic\", \"pvalue\",\"index\"], axis=1)\\\n",
    ".map_index(lambda v: \"rotatebox:{45}--rwrap;\", level=0, axis=1).format(precision=2)\n",
    "display(df_similarity_anchors)\n",
    "latex = df_similarity_anchors.to_latex(environment=\"longtable\", convert_css=True, clines=\"all;data\", hrules=True, caption=\"Similarity of Anchor explanations (p < 0.05 bold)\", label=\"similarityAnchors\")\n",
    "latex_strings.append(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of documents (p < 0.05 bold)} \\label{similarityDocuments} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of documents (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{Spacy Similarity} & DetectorRadford & 0.93 & 0.88 & \\bfseries 0.05 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.92 & 0.88 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.92 & 0.88 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.92 & 0.88 & \\bfseries 0.04 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Jaccard Similarity} & DetectorRadford & 0.16 & 0.12 & \\bfseries 0.03 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.15 & 0.12 & \\bfseries 0.03 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.16 & 0.12 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.16 & 0.12 & \\bfseries 0.03 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Cosine Similarity tfidf} & DetectorRadford & 0.12 & 0.09 & \\bfseries 0.03 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.13 & 0.08 & \\bfseries 0.04 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.14 & 0.08 & \\bfseries 0.05 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.13 & 0.08 & \\bfseries 0.04 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n",
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)} \\label{similarityFI} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of FI explanations. Cosine similarity in $W$ is significantly higher then when using random pairs (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{\\# Common Features} & DetectorRadford & 12.22 & 9.22 & 3.00 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 10.61 & 8.70 & 1.91 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 5.00 & 3.75 & 1.25 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 9.28 & 7.22 & 2.05 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Cosine Similarity} & DetectorRadford & 0.33 & 0.18 & \\bfseries 0.14 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.38 & 0.22 & \\bfseries 0.16 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.25 & 0.10 & \\bfseries 0.14 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.32 & 0.17 & \\bfseries 0.15 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n",
      "\\begin{longtable}{llrrr}\n",
      "\\caption{Similarity of Anchor explanations (p < 0.05 bold)} \\label{similarityAnchors} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{Similarity of Anchor explanations (p < 0.05 bold)} \\\\\n",
      "\\toprule\n",
      " &  & \\rotatebox{45}{Mean of Method} & \\rotatebox{45}{Mean of 10 Rand. Selections} & \\rotatebox{45}{Gain Over Random} \\\\\n",
      "Metric & Set &  &  &  \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\midrule\n",
      "\\multicolumn{5}{r}{Continued on next page} \\\\\n",
      "\\midrule\n",
      "\\endfoot\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "\\multirow[c]{4}{*}{\\# Matching Anchors} & DetectorRadford & 0.39 & 0.10 & 0.29 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.94 & 0.47 & \\bfseries 0.48 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.39 & 0.08 & \\bfseries 0.31 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.57 & 0.21 & \\bfseries 0.36 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{Len Longest Matching Anchor} & DetectorRadford & 0.39 & 0.10 & 0.29 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.94 & 0.47 & \\bfseries 0.48 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.39 & 0.08 & \\bfseries 0.31 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.57 & 0.21 & \\bfseries 0.36 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\multirow[c]{4}{*}{$\\theta$ Longest Matching Anchor} & DetectorRadford & 0.14 & 0.06 & 0.08 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorGuo & 0.52 & 0.33 & 0.19 \\\\\n",
      "\\cline{2-5}\n",
      " & DetectorDetectGPT & 0.29 & 0.08 & \\bfseries 0.21 \\\\\n",
      "\\cline{2-5}\n",
      " & All & 0.32 & 0.16 & \\bfseries 0.16 \\\\\n",
      "\\cline{1-5} \\cline{2-5}\n",
      "\\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in latex_strings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Assignment}\n",
      "\\label{assignmentusers}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      " & Anchor & LIME & SHAP \\\\\n",
      "\\midrule\n",
      "DetectGPT & ['U1', 'U2', 'U3'] & ['U4', 'U5', 'U6'] & ['U7', 'U8', 'U9'] \\\\\n",
      "Guo & ['U10', 'U11', 'U12'] & ['U13', 'U14', 'U15'] & ['U16', 'U17', 'U18'] \\\\\n",
      "Radford & ['U19', 'U20', 'U21'] & ['U22', 'U23', 'U24'] & ['U25', 'U26', 'U27'] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [c.replace(\"_Explainer\", \"\") for c in df.groupby([\"Detector\",\"Explainer\"]).count().index.get_level_values(1).unique()]\n",
    "index = [i.replace(\"Detector\",\"\") for i in df.groupby([\"Detector\",\"Explainer\"]).count().index.get_level_values(0).unique()]\n",
    "r = []\n",
    "u = 1\n",
    "for detector_name in index:\n",
    "    row = []\n",
    "    for explainer_name in columns:\n",
    "        users = []\n",
    "        for i in range(0,3):\n",
    "            users.append(\"U\"+str(u))\n",
    "            u+=1\n",
    "        row.append(users)\n",
    "        \n",
    "    r.append(row)\n",
    "print(pd.DataFrame(r, columns = columns, index = index ).to_latex(caption=\"Assignment\",label=\"assignmentusers\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
