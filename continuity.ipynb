{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results/continuity/perturbations.csv\"\n",
    "N_PERTURBATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Random_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test # always load the full dataset! (np.random.shuffle(tokenized_sentences)). slice the actual hybrid_documents if debugging!\n",
    "\n",
    "\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "detector_classes = [DetectorRadford, DetectorGuo]\n",
    "\n",
    "explainer_classes = [LIME_Explainer,SHAP_Explainer, Random_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "\n",
    "# model used for generating perturbations\n",
    "model = \"t5-small\"\n",
    "cache_dir=\"./.cache\"\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(model, model_max_length=mask_model.config.n_positions, cache_dir=cache_dir)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pertubed_text(detector, text, n=1):\n",
    "    \"\"\"Generates perturbations, similar to how it is done in detectgpt/detector_detectgpt.py. Always edits one token. \n",
    "\n",
    "    Args:\n",
    "        detector: Detector to use when verifying that label doesn't flip.\n",
    "        text: Original document\n",
    "        n: How many perturbations to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        n edited documents\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    # select 1 token in the original document to mask\n",
    "    mask = np.zeros_like(tokens, dtype=bool)\n",
    "    mask[np.random.randint(0, len(mask))] = 1 # TODO number of tokens to mask\n",
    "\n",
    "    prediction_original = detector.predict_label([text])[0]\n",
    "\n",
    "    past_generations = []\n",
    "    perturbed_text = text\n",
    "    # generate n unique perturbations (replace the same masked word(s) with one or more words)\n",
    "    for _ in range(0,n):\n",
    "        replacement_attempts = 0\n",
    "        while True: # do while \n",
    "            i = 0\n",
    "            for ii, (m, token) in enumerate(zip(mask, tokens)):\n",
    "                if m:\n",
    "                    tokens[ii] = \"<extra_id_{}>\".format(i)\n",
    "                    i+=1\n",
    "            i-=1\n",
    "            masked_text = ' '.join(tokens)\n",
    "            stop_id = mask_tokenizer.encode(f\"<extra_id_{i+1}>\")[0]\n",
    "\n",
    "\n",
    "            tok = mask_tokenizer(masked_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            outputs = mask_model.generate(**tok, max_length=150, do_sample=True, top_p=1, num_return_sequences=1, eos_token_id=stop_id,)\n",
    "            mt = mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "            fills = [x for x in re.split(r\"<extra_id_\\d*>\", mt[0]) if x != \"<pad>\"]\n",
    "\n",
    "            for i, (token, m) in enumerate(zip(tokens, mask)):\n",
    "                if m:\n",
    "                    if replacement_attempts < 100:\n",
    "                        tokens[i] = fills.pop(0).strip()\n",
    "                    else: # sometimes t5 can't come up with 5 unique new perturbations that match the constraints below. use a random word from the vocabulary instead\n",
    "                        # have to change seed here as detector.predict_label() below sets it (wich results in endless loop)\n",
    "                        np.random.seed(replacement_attempts)\n",
    "                        random_token = [np.random.randint(0, mask_tokenizer.vocab_size)]\n",
    "                        np.random.seed(42) # reset seed just to be sure, is reset with the next detector.predict_label() anyways \n",
    "                        tokens[i] = mask_tokenizer.batch_decode(random_token, skip_special_tokens=False)[0]\n",
    "            perturbed_text = \" \".join(tokens)\n",
    "\n",
    "            # check if this is a valid and new perturbation\n",
    "            if (perturbed_text == text) or (perturbed_text in past_generations):\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            # verify that label didn't flip\n",
    "            if detector.predict_label([perturbed_text])[0] != prediction_original:\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        past_generations.append(perturbed_text)\n",
    "    return past_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Perturbation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(RESULTS_PATH):\n",
    "    df = pd.read_csv(RESULTS_PATH)\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(RESULTS_PATH, encoding=\"UTF-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating perturbations: 100%|██████████| 305/305 [00:00<00:00, 1059.84it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating perturbations: 100%|██████████| 305/305 [00:00<00:00, 1143.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for document in tqdm(documents, total=len(documents), desc=\"Generating perturbations\"):\n",
    "        if df[(df[\"Original\"] == document) & (df[\"Detector\"] == detector.__class__.__name__)][\"Original\"].count() > 0:\n",
    "            continue\n",
    "        # set seeds here so perturbed documents are the same regardless of slice for documents when debugging (and explanations don't have to be regenerated)\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        for perturbation in get_pertubed_text(detector, document, N_PERTURBATIONS):\n",
    "            row = ((detector.__class__.__name__, document, perturbation))\n",
    "            pd.DataFrame([row], columns=columns).to_csv(RESULTS_PATH, mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n",
    "        #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Original</th>\n",
       "      <th>Perturbation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4527 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Detector                                           Original  \\\n",
       "0           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "1           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "2           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "3           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "4           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "...                 ...                                                ...   \n",
       "4522  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4523  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4524  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4525  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4526  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "\n",
       "                                           Perturbation  \n",
       "0     I've heard of handyman type people making a li...  \n",
       "1     I've heard of handyman type people making a li...  \n",
       "2     I've heard of handyman type people making a li...  \n",
       "3     I've heard of handyman type people making a li...  \n",
       "4     I've heard of handyman type people making a li...  \n",
       "...                                                 ...  \n",
       "4522  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4523  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4524  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4525  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4526  In financial markets, the terms \"bid\" and \"ask...  \n",
       "\n",
       "[4527 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectorRadford\n",
      "LIME_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:00<00:00, 1756.21it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectorGuo\n",
      "LIME_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:00<00:00, 1725.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate all explanations\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    print(detector.__class__.__name__)\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__)\n",
    "        for original, perturbation in tqdm([(o, p) for o,p in zip(df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Original\"], df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Perturbation\"]) if not explainer.is_cached(o) or not explainer.is_cached(p) or not all([explainer.is_cached(o, alt=\"alt_{}_\".format(i)) for i in range(1,5)])]):\n",
    "            explainer.get_explanation_cached(original)\n",
    "            explainer.get_explanation_cached(perturbation)\n",
    "            for i in range(1,5):\n",
    "                explainer.get_explanation_cached(original, alt=\"alt_{}_\".format(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this experiment, words are treated as variables and fi-scores as responses in a coding task\n",
    "# this function therefore transforms a string \"An example is an example\"\n",
    "# into a list of pairs:\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"an\", 0), (\"example\",1)]\n",
    "# \"An example is not an example\"\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"not\", 0), (\"an\", 0), (\"example\",1)]\n",
    "\n",
    "# when calculating the reliability measure, replaced and/or missing words are treated as unobserved in the other explanations\n",
    "\n",
    "def get_tokens_with_pos(explainer, document):\n",
    "    \"\"\"Returns a list of tokens in the document, in an encoding that allows for treating explanations as observations in an experiment\n",
    "    \"\"\"\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for token in explainer.tokenize(document):\n",
    "        tokens_with_pos.append((token, p_counter[token]))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, but also includes the position in the original document given by enumerate(explainer.tokenize(document))\n",
    "# this is useful for indexing the original explanation (a dict)\n",
    "def get_tokens_with_pos_and_id(explainer, document):\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for id, token in enumerate(explainer.tokenize(document)):\n",
    "        tokens_with_pos.append((token, p_counter[token], id))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_to_cannonical_form(experiment, explainer):\n",
    "    fi_scores = [tuple(zip(*explainer.get_fi_scores(d,fill=True)[0]))[1] for d in experiment] # fi scores towards label machine\n",
    "    # each word is treated as an item, each explanation as an observation\n",
    "    tokenized = [explainer.tokenize(d) for d in experiment]\n",
    "    \n",
    "    # determine bounds of left common part\n",
    "    i = 0\n",
    "    while all(x[0:i] == tokenized[0][0:i] for x in tokenized):\n",
    "        i+=1\n",
    "    i-=1\n",
    "    # determine bounds of right common part\n",
    "    j = 1\n",
    "    while all(x[-j:] == tokenized[0][-j:] for x in tokenized):\n",
    "        j+=1\n",
    "    j-=1\n",
    "\n",
    "    # this matrix will be passed to krippendorff.alpha as reliability_data\n",
    "    left_part = np.vstack([e[0:i] for e in fi_scores])\n",
    "    if j > 0: \n",
    "        right_part = np.vstack([e[-j:] for e in fi_scores])\n",
    "        cannonical_form = np.hstack([left_part, right_part])\n",
    "    else: # if no tokens on the right part match\n",
    "        cannonical_form = left_part\n",
    "\n",
    "    return cannonical_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [00:28<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [02:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [05:14<00:00,  1.03s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME_Explainer DetectorGuo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [00:28<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer DetectorGuo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [02:52<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer DetectorGuo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [05:18<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    df_detector = df[df[\"Detector\"] == detector.__class__.__name__]\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__, detector.__class__.__name__)\n",
    "        for original, perturbations in tqdm(df_detector.groupby(\"Original\"), desc=\"Calculating agreement\"):\n",
    "            # perturbations\n",
    "            experiment = [original]+perturbations[\"Perturbation\"].tolist()\n",
    "            cannonical_form = experiment_to_cannonical_form(experiment, explainer)\n",
    "            # re-runs on the original document\n",
    "            #           original document                                                  # 4 re-runs\n",
    "            # print(explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(1)))\n",
    "            fi_scores = [tuple(zip(*explainer.get_fi_scores(original,fill=True)[0]))[1]] + [tuple(zip(*explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(i))[0]))[1] for i in range(1,5)] # fi scores towards label machine\n",
    "            cannonical_form_rerun = np.vstack(fi_scores)\n",
    "   \n",
    "            results.append((\n",
    "                explainer.__class__.__name__, \n",
    "                explainer.detector.__class__.__name__,\n",
    "                krippendorff.alpha(reliability_data=cannonical_form, level_of_measurement=\"interval\"),\n",
    "                krippendorff.alpha(reliability_data=cannonical_form_rerun, level_of_measurement=\"interval\")\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"$\\\\alpha$\",\"$\\\\alpha$ re-run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\alpha$</th>\n",
       "      <th>$\\alpha$ re-run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>6.100000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.854028</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.137144</td>\n",
       "      <td>1.381722e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.184561</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.793722</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.893396</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.957900</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         $\\alpha$  $\\alpha$ re-run\n",
       "count  610.000000     6.100000e+02\n",
       "mean     0.854028     1.000000e+00\n",
       "std      0.137144     1.381722e-09\n",
       "min      0.184561     1.000000e+00\n",
       "25%      0.793722     1.000000e+00\n",
       "50%      0.893396     1.000000e+00\n",
       "75%      0.957900     1.000000e+00\n",
       "max      1.000000     1.000000e+00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[df_results[\"Explainer\"] == \"SHAP_Explainer\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"Explainer\"] = df_results[\"Explainer\"].str.replace(\"_Explainer\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4a03c_row4_col0, #T_4a03c_row5_col1 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4a03c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4a03c_level0_col0\" class=\"col_heading level0 col0\" >$\\alpha$</th>\n",
       "      <th id=\"T_4a03c_level0_col1\" class=\"col_heading level0 col1\" >$\\alpha$ re-run</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Explainer</th>\n",
       "      <th class=\"index_name level1\" >Detector</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"2\">LIME</th>\n",
       "      <th id=\"T_4a03c_level1_row0\" class=\"row_heading level1 row0\" >DetectorGuo</th>\n",
       "      <td id=\"T_4a03c_row0_col0\" class=\"data row0 col0\" >0.478115</td>\n",
       "      <td id=\"T_4a03c_row0_col1\" class=\"data row0 col1\" >0.179067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level1_row1\" class=\"row_heading level1 row1\" >DetectorRadford</th>\n",
       "      <td id=\"T_4a03c_row1_col0\" class=\"data row1 col0\" >0.439034</td>\n",
       "      <td id=\"T_4a03c_row1_col1\" class=\"data row1 col1\" >0.204442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level0_row2\" class=\"row_heading level0 row2\" rowspan=\"2\">Random</th>\n",
       "      <th id=\"T_4a03c_level1_row2\" class=\"row_heading level1 row2\" >DetectorGuo</th>\n",
       "      <td id=\"T_4a03c_row2_col0\" class=\"data row2 col0\" >-0.137300</td>\n",
       "      <td id=\"T_4a03c_row2_col1\" class=\"data row2 col1\" >-0.167180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level1_row3\" class=\"row_heading level1 row3\" >DetectorRadford</th>\n",
       "      <td id=\"T_4a03c_row3_col0\" class=\"data row3 col0\" >-0.137083</td>\n",
       "      <td id=\"T_4a03c_row3_col1\" class=\"data row3 col1\" >-0.167180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"2\">SHAP</th>\n",
       "      <th id=\"T_4a03c_level1_row4\" class=\"row_heading level1 row4\" >DetectorGuo</th>\n",
       "      <td id=\"T_4a03c_row4_col0\" class=\"data row4 col0\" >0.896235</td>\n",
       "      <td id=\"T_4a03c_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a03c_level1_row5\" class=\"row_heading level1 row5\" >DetectorRadford</th>\n",
       "      <td id=\"T_4a03c_row5_col0\" class=\"data row5 col0\" >0.811821</td>\n",
       "      <td id=\"T_4a03c_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x205776c8310>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_explainer_detector = df_results.groupby([\"Explainer\", \"Detector\"]).mean()\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"])\n",
    "results_explainer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{table}[h!]\\n\\\\centering\\n\\\\caption{Results aggregated by detector}\\n\\\\label{continuity-results-explainer}\\n\\\\begin{tabular}{lr}\\n\\\\toprule\\n & $\\\\alpha$ \\\\\\\\\\nExplainer &  \\\\\\\\\\n\\\\midrule\\nSHAP & \\\\bfseries 0.854 \\\\\\\\\\n\\\\cline{1-2}\\nLIME & 0.459 \\\\\\\\\\n\\\\cline{1-2}\\nRandom & -0.137 \\\\\\\\\\n\\\\cline{1-2}\\n\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latex tables per explainer\n",
    "tex_explainer_continuity = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[ \"$\\\\alpha$\"]).groupby([\"Explainer\"]).mean().sort_values(by=[\"$\\\\alpha$\"],  ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"continuity-results-explainer\")\n",
    "\n",
    "tex_explainer_consistency = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[\"$\\\\alpha$ re-run\"]).groupby([\"Explainer\"]).mean().sort_values(by=[\"$\\\\alpha$ re-run\"], ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"consistency-results-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latex tables per detector-explainer combination\n",
    "tex_explainer_continuity_detector = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[ \"$\\\\alpha$\"]).groupby([\"Explainer\", \"Detector\"]).mean().sort_values(by=[\"$\\\\alpha$\"],  ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"continuity-results-explainer\")\n",
    "\n",
    "tex_explainer_consistency_detector = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[\"$\\\\alpha$ re-run\"]).groupby([\"Explainer\", \"Detector\"]).mean().sort_values(by=[\"$\\\\alpha$ re-run\"], ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"consistency-results-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_latex(string):\n",
    "    return string\\\n",
    "    .replace(\"_Explainer\", \"\")\\\n",
    "    .replace(\"DetectorRadford\", \"Radford\")\\\n",
    "    .replace(\"DetectorDetectGPT\", \"DetectGPT\")\\\n",
    "    .replace(\"DetectorGuo\", \"Guo\")\\\n",
    "    .replace(\"Pointing Game Scores\", \"Score\")\\\n",
    "    .replace(\"$\\\\alpha$ re-run\", \"$\\\\alpha$\")\\\n",
    "    .replace(r\"\"\"\\begin{subfigure}\"\"\", r\"\"\"\\begin{subfigure}{\\columnwidth}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tex_explainer_continuity\n",
    "out += tex_explainer_continuity_detector\n",
    "\n",
    "out += tex_explainer_consistency\n",
    "out += tex_explainer_consistency_detector\n",
    "with open(\"figures/tables_continuity.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(shorten_latex(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
