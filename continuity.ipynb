{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results/continuity/perturbations.csv\"\n",
    "N_PERTURBATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bootstrap_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbootstrap_alpha\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bootstrap, get_example_data, demo\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bootstrap_alpha'"
     ]
    }
   ],
   "source": [
    "from bootstrap_alpha import bootstrap, get_example_data, demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Random_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test # always load the full dataset! (np.random.shuffle(tokenized_sentences)). slice the actual hybrid_documents if debugging!\n",
    "\n",
    "\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "detector_classes = [DetectorRadford, DetectorGuo]\n",
    "\n",
    "explainer_classes = [LIME_Explainer,SHAP_Explainer, Random_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "\n",
    "# model used for generating perturbations\n",
    "model = \"t5-small\"\n",
    "cache_dir=\"./.cache\"\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(model, model_max_length=mask_model.config.n_positions, cache_dir=cache_dir)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pertubed_text(detector, text, n=1):\n",
    "    \"\"\"Generates perturbations, similar to how it is done in detectgpt/detector_detectgpt.py. Always edits one token. \n",
    "\n",
    "    Args:\n",
    "        detector: Detector to use when verifying that label doesn't flip.\n",
    "        text: Original document\n",
    "        n: How many perturbations to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        n edited documents\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    # select 1 token in the original document to mask\n",
    "    mask = np.zeros_like(tokens, dtype=bool)\n",
    "    mask[np.random.randint(0, len(mask))] = 1 # TODO number of tokens to mask\n",
    "\n",
    "    prediction_original = detector.predict_label([text])[0]\n",
    "\n",
    "    past_generations = []\n",
    "    perturbed_text = text\n",
    "    # generate n unique perturbations (replace the same masked word(s) with one or more words)\n",
    "    for _ in range(0,n):\n",
    "        replacement_attempts = 0\n",
    "        while True: # do while \n",
    "            i = 0\n",
    "            for ii, (m, token) in enumerate(zip(mask, tokens)):\n",
    "                if m:\n",
    "                    tokens[ii] = \"<extra_id_{}>\".format(i)\n",
    "                    i+=1\n",
    "            i-=1\n",
    "            masked_text = ' '.join(tokens)\n",
    "            stop_id = mask_tokenizer.encode(f\"<extra_id_{i+1}>\")[0]\n",
    "\n",
    "\n",
    "            tok = mask_tokenizer(masked_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            outputs = mask_model.generate(**tok, max_length=150, do_sample=True, top_p=1, num_return_sequences=1, eos_token_id=stop_id,)\n",
    "            mt = mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "            fills = [x for x in re.split(r\"<extra_id_\\d*>\", mt[0]) if x != \"<pad>\"]\n",
    "\n",
    "            for i, (token, m) in enumerate(zip(tokens, mask)):\n",
    "                if m:\n",
    "                    if replacement_attempts < 100:\n",
    "                        tokens[i] = fills.pop(0).strip()\n",
    "                    else: # sometimes t5 can't come up with 5 unique new perturbations that match the constraints below. use a random word from the vocabulary instead\n",
    "                        # have to change seed here as detector.predict_label() below sets it (wich results in endless loop)\n",
    "                        np.random.seed(replacement_attempts)\n",
    "                        random_token = [np.random.randint(0, mask_tokenizer.vocab_size)]\n",
    "                        np.random.seed(42) # reset seed just to be sure, is reset with the next detector.predict_label() anyways \n",
    "                        tokens[i] = mask_tokenizer.batch_decode(random_token, skip_special_tokens=False)[0]\n",
    "            perturbed_text = \" \".join(tokens)\n",
    "\n",
    "            # check if this is a valid and new perturbation\n",
    "            if (perturbed_text == text) or (perturbed_text in past_generations):\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            # verify that label didn't flip\n",
    "            if detector.predict_label([perturbed_text])[0] != prediction_original:\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        past_generations.append(perturbed_text)\n",
    "    return past_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Perturbation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(RESULTS_PATH):\n",
    "    df = pd.read_csv(RESULTS_PATH)\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(RESULTS_PATH, encoding=\"UTF-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating perturbations: 100%|██████████| 305/305 [00:00<00:00, 627.11it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating perturbations: 100%|██████████| 305/305 [00:00<00:00, 824.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for document in tqdm(documents, total=len(documents), desc=\"Generating perturbations\"):\n",
    "        if df[(df[\"Original\"] == document) & (df[\"Detector\"] == detector.__class__.__name__)][\"Original\"].count() > 0:\n",
    "            continue\n",
    "        # set seeds here so perturbed documents are the same regardless of slice for documents when debugging (and explanations don't have to be regenerated)\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        for perturbation in get_pertubed_text(detector, document, N_PERTURBATIONS):\n",
    "            row = ((detector.__class__.__name__, document, perturbation))\n",
    "            pd.DataFrame([row], columns=columns).to_csv(RESULTS_PATH, mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n",
    "        #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Original</th>\n",
       "      <th>Perturbation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "      <td>In financial markets, the terms \"bid\" and \"ask...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4527 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Detector                                           Original  \\\n",
       "0           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "1           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "2           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "3           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "4           DetectorGuo  I've heard of handyman type people making a li...   \n",
       "...                 ...                                                ...   \n",
       "4522  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4523  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4524  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4525  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "4526  DetectorDetectGPT  In financial markets, the terms \"bid\" and \"ask...   \n",
       "\n",
       "                                           Perturbation  \n",
       "0     I've heard of handyman type people making a li...  \n",
       "1     I've heard of handyman type people making a li...  \n",
       "2     I've heard of handyman type people making a li...  \n",
       "3     I've heard of handyman type people making a li...  \n",
       "4     I've heard of handyman type people making a li...  \n",
       "...                                                 ...  \n",
       "4522  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4523  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4524  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4525  In financial markets, the terms \"bid\" and \"ask...  \n",
       "4526  In financial markets, the terms \"bid\" and \"ask...  \n",
       "\n",
       "[4527 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectorRadford\n",
      "LIME_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:02<00:00, 725.25it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectorGuo\n",
      "LIME_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:01<00:00, 1486.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate all explanations\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    print(detector.__class__.__name__)\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__)\n",
    "        for original, perturbation in tqdm([(o, p) for o,p in zip(df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Original\"], df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Perturbation\"]) if not explainer.is_cached(o) or not explainer.is_cached(p) or not all([explainer.is_cached(o, alt=\"alt_{}_\".format(i)) for i in range(1,5)])]):\n",
    "            explainer.get_explanation_cached(original)\n",
    "            explainer.get_explanation_cached(perturbation)\n",
    "            for i in range(1,5):\n",
    "                explainer.get_explanation_cached(original, alt=\"alt_{}_\".format(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this experiment, words are treated as variables and fi-scores as responses in a coding task\n",
    "# this function therefore transforms a string \"An example is an example\"\n",
    "# into a list of pairs:\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"an\", 0), (\"example\",1)]\n",
    "# \"An example is not an example\"\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"not\", 0), (\"an\", 0), (\"example\",1)]\n",
    "\n",
    "# when calculating the reliability measure, replaced and/or missing words are treated as unobserved in the other explanations\n",
    "\n",
    "def get_tokens_with_pos(explainer, document):\n",
    "    \"\"\"Returns a list of tokens in the document, in an encoding that allows for treating explanations as observations in an experiment\n",
    "    \"\"\"\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for token in explainer.tokenize(document):\n",
    "        tokens_with_pos.append((token, p_counter[token]))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, but also includes the position in the original document given by enumerate(explainer.tokenize(document))\n",
    "# this is useful for indexing the original explanation (a dict)\n",
    "def get_tokens_with_pos_and_id(explainer, document):\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for id, token in enumerate(explainer.tokenize(document)):\n",
    "        tokens_with_pos.append((token, p_counter[token], id))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_to_cannonical_form(experiment, explainer):\n",
    "    fi_scores = [tuple(zip(*explainer.get_fi_scores(d,fill=True)[0]))[1] for d in experiment] # fi scores towards label machine\n",
    "    # each word is treated as an item, each explanation as an observation\n",
    "    tokenized = [explainer.tokenize(d) for d in experiment]\n",
    "    \n",
    "    # determine bounds of left common part\n",
    "    i = 0\n",
    "    while all(x[0:i] == tokenized[0][0:i] for x in tokenized):\n",
    "        i+=1\n",
    "    i-=1\n",
    "    # determine bounds of right common part\n",
    "    j = 1\n",
    "    while all(x[-j:] == tokenized[0][-j:] for x in tokenized):\n",
    "        j+=1\n",
    "    j-=1\n",
    "\n",
    "    # this matrix will be passed to krippendorff.alpha as reliability_data\n",
    "    left_part = np.vstack([e[0:i] for e in fi_scores])\n",
    "    if j > 0: \n",
    "        right_part = np.vstack([e[-j:] for e in fi_scores])\n",
    "        cannonical_form = np.hstack([left_part, right_part])\n",
    "    else: # if no tokens on the right part match\n",
    "        cannonical_form = left_part\n",
    "\n",
    "    return cannonical_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME_Explainer DetectorRadford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating agreement: 100%|██████████| 305/305 [00:02<00:00, 126.44it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 77.4 TiB for an array with shape (33357, 17855, 17855) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     13\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(cannonical_form)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# re-runs on the original document\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#           original document                                                  # 4 re-runs\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# print(explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(1)))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# fi_scores = [tuple(zip(*explainer.get_fi_scores(original,fill=True)[0]))[1]] + [tuple(zip(*explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(i))[0]))[1] for i in range(1,5)] # fi scores towards label machine\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# cannonical_form_rerun = np.vstack(fi_scores)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((\n\u001b[0;32m     21\u001b[0m     explainer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \n\u001b[0;32m     22\u001b[0m     explainer\u001b[38;5;241m.\u001b[39mdetector\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mkrippendorff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreliability_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_of_measurement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;66;03m#  krippendorff.alpha(reliability_data=cannonical_form_rerun, level_of_measurement=\"interval\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     ))\n",
      "File \u001b[1;32mc:\\Users\\loris\\thesis\\.venv\\Lib\\site-packages\\krippendorff\\krippendorff.py:360\u001b[0m, in \u001b[0;36malpha\u001b[1;34m(reliability_data, value_counts, value_domain, level_of_measurement, dtype)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` must be an inexact type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    358\u001b[0m distance_metric \u001b[38;5;241m=\u001b[39m _distance_metric(level_of_measurement)\n\u001b[1;32m--> 360\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_coincidences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m n_v \u001b[38;5;241m=\u001b[39m o\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    362\u001b[0m e \u001b[38;5;241m=\u001b[39m _random_coincidences(n_v, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\loris\\thesis\\.venv\\Lib\\site-packages\\krippendorff\\krippendorff.py:116\u001b[0m, in \u001b[0;36m_coincidences\u001b[1;34m(value_counts, dtype)\u001b[0m\n\u001b[0;32m    114\u001b[0m N, V \u001b[38;5;241m=\u001b[39m value_counts\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    115\u001b[0m pairable \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(value_counts\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m diagonals \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    117\u001b[0m unnormalized_coincidences \u001b[38;5;241m=\u001b[39m value_counts[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m value_counts[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :] \u001b[38;5;241m-\u001b[39m diagonals\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdivide(unnormalized_coincidences, (pairable \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 77.4 TiB for an array with shape (33357, 17855, 17855) and data type float64"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    df_detector = df[df[\"Detector\"] == detector.__class__.__name__]\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__, detector.__class__.__name__)\n",
    "        for original, perturbations in tqdm(df_detector.groupby(\"Original\"), desc=\"Calculating agreement\"):\n",
    "            # perturbations\n",
    "            experiment = [original]+perturbations[\"Perturbation\"].tolist()\n",
    "            cannonical_form = experiment_to_cannonical_form(experiment, explainer)\n",
    "            # re-runs on the original document\n",
    "            #           original document                                                  # 4 re-runs\n",
    "            # print(explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(1)))\n",
    "            fi_scores = [tuple(zip(*explainer.get_fi_scores(original,fill=True)[0]))[1]] + [tuple(zip(*explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(i))[0]))[1] for i in range(1,5)] # fi scores towards label machine\n",
    "            cannonical_form_rerun = np.vstack(fi_scores)\n",
    "   \n",
    "            results.append((\n",
    "                explainer.__class__.__name__, \n",
    "                explainer.detector.__class__.__name__,\n",
    "                krippendorff.alpha(reliability_data=cannonical_form, level_of_measurement=\"interval\"),\n",
    "                krippendorff.alpha(reliability_data=cannonical_form_rerun, level_of_measurement=\"interval\")\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"$\\\\alpha$\",])#\"$\\\\alpha$ re-run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results[\"Explainer\"] == \"SHAP_Explainer\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"Explainer\"] = df_results[\"Explainer\"].str.replace(\"_Explainer\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_explainer_detector = df_results.groupby([\"Explainer\", \"Detector\"]).mean()\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"])\n",
    "results_explainer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tex_explainer_detector = \\\n",
    "        results_explainer_detector.to_latex(environment=\"table\", position_float=\"centering\", position=\"h!\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector and explainer\", label=\"continuity-results-detector-explainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_explainer = df_results.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\"]).mean()\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"])\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"continuity-results-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_latex(string):\n",
    "    return string\\\n",
    "    .replace(\"_Explainer\", \"\")\\\n",
    "    .replace(\"DetectorRadford\", \"Radford\")\\\n",
    "    .replace(\"DetectorDetectGPT\", \"DetectGPT\")\\\n",
    "    .replace(\"DetectorGuo\", \"Guo\")\\\n",
    "    .replace(\"Pointing Game Scores\", \"Score\")\\\n",
    "    .replace(r\"\"\"\\begin{subfigure}\"\"\", r\"\"\"\\begin{subfigure}{\\columnwidth}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tex_explainer\n",
    "out += tex_explainer_detector\n",
    "with open(\"figures/tables_continuity.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(shorten_latex(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
