{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results/continuity/perturbations.csv\"\n",
    "N_PERTURBATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_guo import DetectorGuo\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Random_Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "test = test # always load the full dataset! (np.random.shuffle(tokenized_sentences)). slice the actual hybrid_documents if debugging!\n",
    "\n",
    "\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "detector_classes = [DetectorRadford, DetectorGuo]\n",
    "\n",
    "explainer_classes = [LIME_Explainer,SHAP_Explainer, Random_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "\n",
    "# model used for generating perturbations\n",
    "model = \"t5-small\"\n",
    "cache_dir=\"./.cache\"\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(model, model_max_length=mask_model.config.n_positions, cache_dir=cache_dir)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pertubed_text(detector, text, n=1):\n",
    "    \"\"\"Generates perturbations, similar to how it is done in detectgpt/detector_detectgpt.py. Always edits one token. \n",
    "\n",
    "    Args:\n",
    "        detector: Detector to use when verifying that label doesn't flip.\n",
    "        text: Original document\n",
    "        n: How many perturbations to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        n edited documents\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    # select 1 token in the original document to mask\n",
    "    mask = np.zeros_like(tokens, dtype=bool)\n",
    "    mask[np.random.randint(0, len(mask))] = 1 # TODO number of tokens to mask\n",
    "\n",
    "    prediction_original = detector.predict_label([text])[0]\n",
    "\n",
    "    past_generations = []\n",
    "    perturbed_text = text\n",
    "    # generate n unique perturbations (replace the same masked word(s) with one or more words)\n",
    "    for _ in range(0,n):\n",
    "        replacement_attempts = 0\n",
    "        while True: # do while \n",
    "            i = 0\n",
    "            for ii, (m, token) in enumerate(zip(mask, tokens)):\n",
    "                if m:\n",
    "                    tokens[ii] = \"<extra_id_{}>\".format(i)\n",
    "                    i+=1\n",
    "            i-=1\n",
    "            masked_text = ' '.join(tokens)\n",
    "            stop_id = mask_tokenizer.encode(f\"<extra_id_{i+1}>\")[0]\n",
    "\n",
    "\n",
    "            tok = mask_tokenizer(masked_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            outputs = mask_model.generate(**tok, max_length=150, do_sample=True, top_p=1, num_return_sequences=1, eos_token_id=stop_id,)\n",
    "            mt = mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "            fills = [x for x in re.split(r\"<extra_id_\\d*>\", mt[0]) if x != \"<pad>\"]\n",
    "\n",
    "            for i, (token, m) in enumerate(zip(tokens, mask)):\n",
    "                if m:\n",
    "                    if replacement_attempts < 100:\n",
    "                        tokens[i] = fills.pop(0).strip()\n",
    "                    else: # sometimes t5 can't come up with 5 unique new perturbations that match the constraints below. use a random word from the vocabulary instead\n",
    "                        # have to change seed here as detector.predict_label() below sets it (wich results in endless loop)\n",
    "                        np.random.seed(replacement_attempts)\n",
    "                        random_token = [np.random.randint(0, mask_tokenizer.vocab_size)]\n",
    "                        np.random.seed(42) # reset seed just to be sure, is reset with the next detector.predict_label() anyways \n",
    "                        tokens[i] = mask_tokenizer.batch_decode(random_token, skip_special_tokens=False)[0]\n",
    "            perturbed_text = \" \".join(tokens)\n",
    "\n",
    "            # check if this is a valid and new perturbation\n",
    "            if (perturbed_text == text) or (perturbed_text in past_generations):\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            # verify that label didn't flip\n",
    "            if detector.predict_label([perturbed_text])[0] != prediction_original:\n",
    "                replacement_attempts+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        past_generations.append(perturbed_text)\n",
    "    return past_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Perturbation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(RESULTS_PATH):\n",
    "    df = pd.read_csv(RESULTS_PATH)\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(RESULTS_PATH, encoding=\"UTF-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for document in tqdm(documents, total=len(documents), desc=\"Generating perturbations\"):\n",
    "        if df[(df[\"Original\"] == document) & (df[\"Detector\"] == detector.__class__.__name__)][\"Original\"].count() > 0:\n",
    "            continue\n",
    "        # set seeds here so perturbed documents are the same regardless of slice for documents when debugging (and explanations don't have to be regenerated)\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        for perturbation in get_pertubed_text(detector, document, N_PERTURBATIONS):\n",
    "            row = ((detector.__class__.__name__, document, perturbation))\n",
    "            pd.DataFrame([row], columns=columns).to_csv(RESULTS_PATH, mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n",
    "        #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all explanations\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    print(detector.__class__.__name__)\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__)\n",
    "        for original, perturbation in tqdm([(o, p) for o,p in zip(df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Original\"], df.loc[df[\"Detector\"] == detector.__class__.__name__,\"Perturbation\"]) if not explainer.is_cached(o) or not explainer.is_cached(p) or not all([explainer.is_cached(o, alt=\"alt_{}_\".format(i)) for i in range(1,5)])]):\n",
    "            explainer.get_explanation_cached(original)\n",
    "            explainer.get_explanation_cached(perturbation)\n",
    "            for i in range(1,5):\n",
    "                explainer.get_explanation_cached(original, alt=\"alt_{}_\".format(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this experiment, words are treated as variables and fi-scores as responses in a coding task\n",
    "# this function therefore transforms a string \"An example is an example\"\n",
    "# into a list of pairs:\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"an\", 0), (\"example\",1)]\n",
    "# \"An example is not an example\"\n",
    "# [(\"An\", 0), (\"example\", 0), (\"is\", 0), (\"not\", 0), (\"an\", 0), (\"example\",1)]\n",
    "\n",
    "# when calculating the reliability measure, replaced and/or missing words are treated as unobserved in the other explanations\n",
    "\n",
    "def get_tokens_with_pos(explainer, document):\n",
    "    \"\"\"Returns a list of tokens in the document, in an encoding that allows for treating explanations as observations in an experiment\n",
    "    \"\"\"\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for token in explainer.tokenize(document):\n",
    "        tokens_with_pos.append((token, p_counter[token]))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, but also includes the position in the original document given by enumerate(explainer.tokenize(document))\n",
    "# this is useful for indexing the original explanation (a dict)\n",
    "def get_tokens_with_pos_and_id(explainer, document):\n",
    "    p_counter = defaultdict(lambda : 0)\n",
    "    tokens_with_pos = []\n",
    "    for id, token in enumerate(explainer.tokenize(document)):\n",
    "        tokens_with_pos.append((token, p_counter[token], id))\n",
    "        p_counter[token] += 1\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_to_cannonical_form(experiment, explainer):\n",
    "    fi_scores = [tuple(zip(*explainer.get_fi_scores(d,fill=True)[0]))[1] for d in experiment] # fi scores towards label machine\n",
    "    # each word is treated as an item, each explanation as an observation\n",
    "    tokenized = [explainer.tokenize(d) for d in experiment]\n",
    "    \n",
    "    # determine bounds of left common part\n",
    "    i = 0\n",
    "    while all(x[0:i] == tokenized[0][0:i] for x in tokenized):\n",
    "        i+=1\n",
    "    i-=1\n",
    "    # determine bounds of right common part\n",
    "    j = 1\n",
    "    while all(x[-j:] == tokenized[0][-j:] for x in tokenized):\n",
    "        j+=1\n",
    "    j-=1\n",
    "\n",
    "    # this matrix will be passed to krippendorff.alpha as reliability_data\n",
    "    left_part = np.vstack([e[0:i] for e in fi_scores])\n",
    "    if j > 0: \n",
    "        right_part = np.vstack([e[-j:] for e in fi_scores])\n",
    "        cannonical_form = np.hstack([left_part, right_part])\n",
    "    else: # if no tokens on the right part match\n",
    "        cannonical_form = left_part\n",
    "\n",
    "    return cannonical_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    df_detector = df[df[\"Detector\"] == detector.__class__.__name__]\n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        print(explainer.__class__.__name__, detector.__class__.__name__)\n",
    "        for original, perturbations in tqdm(df_detector.groupby(\"Original\"), desc=\"Calculating agreement\"):\n",
    "            # perturbations\n",
    "            experiment = [original]+perturbations[\"Perturbation\"].tolist()\n",
    "            cannonical_form = experiment_to_cannonical_form(experiment, explainer)\n",
    "            # re-runs on the original document\n",
    "            #           original document                                                  # 4 re-runs\n",
    "            # print(explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(1)))\n",
    "            fi_scores = [tuple(zip(*explainer.get_fi_scores(original,fill=True)[0]))[1]] + [tuple(zip(*explainer.get_fi_scores(original,fill=True, alt=\"alt_{}_\".format(i))[0]))[1] for i in range(1,5)] # fi scores towards label machine\n",
    "            cannonical_form_rerun = np.vstack(fi_scores)\n",
    "   \n",
    "            results.append((\n",
    "                explainer.__class__.__name__, \n",
    "                explainer.detector.__class__.__name__,\n",
    "                krippendorff.alpha(reliability_data=cannonical_form, level_of_measurement=\"interval\"),\n",
    "                krippendorff.alpha(reliability_data=cannonical_form_rerun, level_of_measurement=\"interval\")\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Explainer\", \"Detector\", \"$\\\\alpha$\",\"$\\\\alpha$ re-run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results[\"Explainer\"] == \"SHAP_Explainer\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"Explainer\"] = df_results[\"Explainer\"].str.replace(\"_Explainer\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_explainer_detector = df_results.groupby([\"Explainer\", \"Detector\"]).mean()\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"])\n",
    "results_explainer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latex tables per explainer\n",
    "tex_explainer_continuity = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[ \"$\\\\alpha$\"]).groupby([\"Explainer\"]).mean().sort_values(by=[\"$\\\\alpha$\"],  ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"continuity-results-explainer\")\n",
    "\n",
    "tex_explainer_consistency = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[\"$\\\\alpha$ re-run\"]).groupby([\"Explainer\"]).mean().sort_values(by=[\"$\\\\alpha$ re-run\"], ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"consistency-results-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latex tables per detector-explainer combination\n",
    "tex_explainer_continuity_detector = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[ \"$\\\\alpha$\"]).groupby([\"Explainer\", \"Detector\"]).mean().sort_values(by=[\"$\\\\alpha$\"],  ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"continuity-results-explainer\")\n",
    "\n",
    "tex_explainer_consistency_detector = pd.DataFrame(df_results.set_index([\"Explainer\", \"Detector\"])[\"$\\\\alpha$ re-run\"]).groupby([\"Explainer\", \"Detector\"]).mean().sort_values(by=[\"$\\\\alpha$ re-run\"], ascending=False)\\\n",
    "        .style.highlight_max(props=[\"font-weight: bold;\"]).format(precision=3)\\\n",
    "        .to_latex(environment=\"table\", position=\"h!\", position_float=\"centering\",convert_css=True, clines=\"all;data\", hrules=True, caption=\"Results aggregated by detector\", label=\"consistency-results-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_latex(string):\n",
    "    return string\\\n",
    "    .replace(\"_Explainer\", \"\")\\\n",
    "    .replace(\"DetectorRadford\", \"Radford\")\\\n",
    "    .replace(\"DetectorDetectGPT\", \"DetectGPT\")\\\n",
    "    .replace(\"DetectorGuo\", \"Guo\")\\\n",
    "    .replace(\"Pointing Game Scores\", \"Score\")\\\n",
    "    .replace(\"$\\\\alpha$ re-run\", \"$\\\\alpha$\")\\\n",
    "    .replace(r\"\"\"\\begin{subfigure}\"\"\", r\"\"\"\\begin{subfigure}{\\columnwidth}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tex_explainer_continuity\n",
    "out += tex_explainer_continuity_detector\n",
    "\n",
    "out += tex_explainer_consistency\n",
    "out += tex_explainer_consistency_detector\n",
    "with open(\"figures/tables_continuity.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(shorten_latex(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
