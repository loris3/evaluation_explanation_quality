{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "N_DEBUG = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import sklearn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "\n",
    "test = test[test[\"author\"] == \"human_answers\"]\n",
    "print(\"len(test_human)\", len(test))\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "#from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorRadford,DetectorGuo,DetectorDetectGPT]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer\n",
    "explainer_classes = [LIME_Explainer,SHAP_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "base_model_name=\"facebook/opt-350m\"\n",
    "openai_model = False\n",
    "\n",
    "cache_dir=\"./.cache\"\n",
    "# mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "# mask_tokenizer = transformers.AutoTokenizer.from_pretrained(model, model_max_length=mask_model.config.n_positions, cache_dir=cache_dir)#.to(DEVICE)\n",
    "do_top_k= False\n",
    "do_top_p= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir=cache_dir).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir, padding_side='left',)# use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate/Load documents\n",
    "Generate or load documents from `./contrastivity_label_flip_pairs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"./contrastivity_label_flip_pairs.csv\"):\n",
    "    df = pd.read_csv(\"./contrastivity_label_flip_pairs.csv\")\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(\"./contrastivity_label_flip_pairs.csv\", encoding=\"UTF-8\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for document in tqdm(documents[detector.predict_label(documents).astype(bool)], desc=\"Generating perturbations\"): # only use those where f(x) = human\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        if df[df[\"Detector\"] == detector.__class__.__name__][\"Original\"].str.contains(document, regex=False).any(): # check if document is in csv, if yes, skip\n",
    "            continue\n",
    "        doc = nlp(document)\n",
    "        n_tokens_original = len(base_tokenizer(document, return_tensors=\"pt\", padding=True).to(DEVICE).input_ids[0])\n",
    "\n",
    "        substrings = [''.join(token.text_with_ws for token in doc[:-i]) for i in range(1,len(document)) ]\n",
    "        substrings = [substring for substring in substrings if substring != \"\"] # at least one token\n",
    "\n",
    "        n_generations_per_lenght = 5\n",
    "        substrings = list(itertools.chain.from_iterable(itertools.repeat(s, n_generations_per_lenght) for s in substrings))\n",
    "\n",
    "        batch_size = 20 # adjust so it fits in your GPU memory\n",
    "        row = (detector.__class__.__name__, document, None, None) # to mark the document in the csv cache if no pair is found\n",
    "        for batch in (sklearn.utils.gen_batches(len(substrings), batch_size)):\n",
    "            encoded = base_tokenizer(substrings[batch], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "\n",
    "            outputs = base_model.generate(**encoded, min_length=n_tokens_original-5, max_length=n_tokens_original+5, do_sample=True, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)\n",
    "            decoded = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            predictions = detector.predict_label(decoded)\n",
    "            if any(predictions != 1):\n",
    "                first_new_label = (predictions!=1).argmax(axis=0)\n",
    "\n",
    "                #                                   original  prompt                              first instance that flips label\n",
    "                row = (detector.__class__.__name__, document, substrings[batch][first_new_label], decoded[first_new_label])\n",
    "                break\n",
    "        pd.DataFrame([row], columns=columns).to_csv(\"./contrastivity_label_flip_pairs.csv\", mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./contrastivity_label_flip_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic idea: assert that exp(original)[original - prompt] <substantially different than> exp(label_flip_example)[label_flip_example - prompt]\n",
    "# i.e. the new/changed section is assigned the opposite label (TODO hard coded: \"machine\") more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for explainer_class in explainer_classes:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, (_, original, prompt, edited) in tqdm(df[df[\"Detector\"] == detector.__class__.__name__].iterrows(), desc=\"Gathering results\"):\n",
    "                \n",
    "\n",
    "                # The generation strategy above uses spacy's tokenizer (where punctuation chars are end up in seperate tokens). \n",
    "                # The explanation methods have their own tokenizers and FI scores are reported irt to those tokens\n",
    "                # For LIME, multiple punctuation chars can end up in the same token e.g.: \"!)\" is one token, \"!\" too, \"#+#++..,++##\" as well. \n",
    "                # This is problematic when comparing explanations:\n",
    "                # i.e. this can fail: assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt]\n",
    "                \n",
    "                # For this experiment, it is only important to separate the \"prompt\", which is the common part between the two documents, from the generated/cut parts\n",
    "                # Determining the bounds of the prompt has to be done AFTER tokenization:\n",
    "                # Strategy: change bounds until assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt] passes\n",
    "                lenght_promt = len(explainer.tokenize(prompt)) # this would suffice for SHAP, but not for lime (because it sometimes collapses punctuation chars into one token) \n",
    "                while explainer.tokenize(original)[0:lenght_promt] != explainer.tokenize(edited)[0:lenght_promt]:    \n",
    "                    lenght_promt -= 1 \n",
    "                lenght_promt = max(lenght_promt, 1) # if the first word is followed by a punctuation char, e.g., \"Example! Is a sentence.\" and the prompt is just \"Example\", the loop above would set lenght_promt=0. This happens as LIME tokenizes this to ['Example!', 'Is', 'a', 'sentence.']\n",
    "                assert explainer.tokenize(original)[1:lenght_promt] == explainer.tokenize(edited)[1:lenght_promt]\n",
    "                       \n",
    "                # get cut/edited parts\n",
    "                exp_original_cut_part = explainer.get_fi_scores(original, fill=True)[0][lenght_promt:] # TODO hard coded: \"machine\"\n",
    "                exp_edited_new_part = explainer.get_fi_scores(edited, fill=True)[0][lenght_promt:] # setting fill=True returns all features (not just the top_k) \n",
    "                fi_scores_exp_original_cut_part = np.array([fi_score for _, fi_score in exp_original_cut_part])\n",
    "                fi_scores_exp_edited_new_part =   np.array([fi_score for _, fi_score in exp_edited_new_part])\n",
    "                \n",
    "\n",
    "                # get common part\n",
    "                exp_original_common_part = explainer.get_fi_scores(original, fill=True)[0][0:lenght_promt]\n",
    "                exp_edited_common_part = explainer.get_fi_scores(edited, fill=True)[0][0:lenght_promt]\n",
    "                fi_scores_exp_original_common_part = np.array([fi_score for _, fi_score in exp_original_common_part])\n",
    "                fi_scores_exp_edited_common_part=    np.array([fi_score for _, fi_score in exp_edited_common_part])\n",
    "                \n",
    "                # build result row\n",
    "                if len(fi_scores_exp_original_cut_part) == 0 or(fi_scores_exp_edited_new_part.shape[0] == 0 or fi_scores_exp_edited_common_part.shape[0] == 0 ):\n",
    "                  #  print(cannonical_form.shape)\n",
    "                    print(lenght_promt)\n",
    "                    print(original)\n",
    "                    print(prompt)\n",
    "                    print(edited)\n",
    "                    print(fi_scores_exp_edited_new_part)\n",
    "                    print(fi_scores_exp_edited_common_part)\n",
    "               \n",
    "                max_fi_in_new_part = fi_scores_exp_edited_new_part.max() > fi_scores_exp_edited_common_part.max()\n",
    "\n",
    "\n",
    "                cannonical_form = np.vstack([fi_scores_exp_original_common_part, fi_scores_exp_edited_common_part])\n",
    "                \n",
    "                assert len(fi_scores_exp_original_cut_part) > 0\n",
    "                assert len(fi_scores_exp_edited_new_part) > 0\n",
    "                assert len(fi_scores_exp_original_common_part) > 0\n",
    "                assert len(fi_scores_exp_edited_common_part) > 0\n",
    "\n",
    "                k_alpha = None\n",
    "                if np.all(cannonical_form==0): # the krippendorff library requires items to not all be the same\n",
    "                    k_alpha = 1\n",
    "                else:\n",
    "                    k_alpha = krippendorff.alpha(cannonical_form, level_of_measurement=\"interval\")\n",
    "\n",
    "                cos_sim = cosine_similarity(fi_scores_exp_original_common_part.reshape(1, -1),fi_scores_exp_edited_common_part.reshape(1, -1))[0,0]\n",
    "                #columns = [\"Explainer\",                  \"Detector\",                 \"Mean FI Original Cut Part\",               \"Mean FI Edited New Part\",              \"Mean FI Original Common Part\",              \"Mean FI Edited Common Part\",             \"cos_sim Common Part\",  \"Krippendorff's Alpha Common Part\", \"Maximum FI for Machine in New Part\"                                                                                                             ]\n",
    "                row =     (explainer.__class__.__name__, detector.__class__.__name__, np.mean(fi_scores_exp_original_cut_part), np.mean(fi_scores_exp_edited_new_part), np.mean(fi_scores_exp_original_common_part), np.mean(fi_scores_exp_edited_common_part),  cos_sim,               k_alpha,                             max_fi_in_new_part                                                      )\n",
    "                results.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Explainer\",                  \"Detector\",                 \"Mean FI Original Cut Part\",               \"Mean FI Edited New Part\",              \"Mean FI Original Common Part\",              \"Mean FI Edited Common Part\",    \"cos_sim Common Part\", \"Krippendorff's Alpha Common Part\",\"Maximum FI for Machine in New Part\"]\n",
    "dff = pd.DataFrame(results, columns=columns)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\", \"Detector\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.groupby([\"Explainer\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Change original -> lf [%]\"] = ((df[\"E[original - prompt]\"] - df[\"E[lf - prompt]\"] ) / df[\"E[original - prompt]\"]) * 100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Explainer\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([fi_score for _, fi_score in exp_label_flip_minus_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_label_flip_minus_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
