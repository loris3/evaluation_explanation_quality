{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results/contrastivity/pairs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "\n",
    "documents = test[\"answer\"]\n",
    "\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "from detector_detectgpt import DetectorDetectGPT\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorRadford,DetectorGuo, DetectorDetectGPT]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Random_Explainer\n",
    "explainer_classes = [SHAP_Explainer,LIME_Explainer] + [Random_Explainer] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "# model used to generate perturbations\n",
    "base_model_name=\"facebook/opt-350m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir=\"./.cache\").to(DEVICE)\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name, cache_dir=\"./.cache\", padding_side='left',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate/Load documents\n",
    "Generate or load documents from `RESULTS_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\", \"f(Original)\", \"f(Edited)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(RESULTS_PATH):\n",
    "    df = pd.read_csv(RESULTS_PATH)\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(RESULTS_PATH, encoding=\"UTF-8\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Detector\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for document in tqdm(documents, desc=\"Generating perturbations\"): # only use those where f(x) = human\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        if df[df[\"Detector\"] == detector.__class__.__name__][\"Original\"].str.contains(document, regex=False).any(): # check if document is in csv, if yes, skip\n",
    "            continue\n",
    "        doc = nlp(document)\n",
    "        n_tokens_original = len(base_tokenizer(document, return_tensors=\"pt\", padding=True).to(DEVICE).input_ids[0])\n",
    "\n",
    "        substrings = [''.join(token.text_with_ws for token in doc[:-i]) for i in range(1,len(document)) ]\n",
    "        substrings = [substring for substring in substrings if substring != \"\"] # at least one token\n",
    "\n",
    "        n_generations_per_lenght = 5\n",
    "        substrings = list(itertools.chain.from_iterable(itertools.repeat(s, n_generations_per_lenght) for s in substrings))\n",
    "\n",
    "        batch_size = 20 # adjust so it fits in your GPU memory\n",
    "        prediction_original = detector.predict_label([document])[0]\n",
    "        row = (detector.__class__.__name__, document, None, None, prediction_original, None) # to mark the document in the csv cache if no pair is found\n",
    "        for batch in (sklearn.utils.gen_batches(len(substrings), batch_size)):\n",
    "            encoded = base_tokenizer(substrings[batch], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "\n",
    "            outputs = base_model.generate(**encoded, min_length=n_tokens_original-5, max_length=n_tokens_original+5, do_sample=True, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)\n",
    "            decoded = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            predictions = detector.predict_label(decoded)\n",
    "            \n",
    "            if any(predictions != prediction_original):\n",
    "                first_new_label = (predictions!=prediction_original).argmax(axis=0)\n",
    "                assert decoded[first_new_label] != document\n",
    "                assert predictions[first_new_label] != prediction_original\n",
    "                #                                   original  prompt                              first instance that flips label\n",
    "                row = (detector.__class__.__name__, document, substrings[batch][first_new_label], decoded[first_new_label], prediction_original, predictions[first_new_label])\n",
    "                break\n",
    "        pd.DataFrame([row], columns=columns).to_csv(RESULTS_PATH, mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Detector\", \"f(Edited)\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics of perturbed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tokens New Part\"] = df.apply(lambda row : len(nlp(row[\"Edited\"].replace(row[\"Prompt\"], \"\"))) , axis=1)\n",
    "df[\"Tokens Original Part\"] = df.apply(lambda row : len(nlp(row[\"Original\"].replace(row[\"Prompt\"], \"\"))) , axis=1)\n",
    "df[\"Tokens Prompt\"] = df.apply(lambda row : len(nlp(row[\"Prompt\"])) , axis=1)\n",
    "df[\"Tokens Document\"] = df.apply(lambda row : len(nlp(row[\"Original\"])) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tokens Edited\"] = (df[\"Tokens Document\"] - df[\"Tokens Prompt\"])\n",
    "df[\"Tokens Edited Proportion\"] = (df[\"Tokens Document\"] - df[\"Tokens Prompt\"]) / df[\"Tokens Document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pgf import FigureCanvasPgf\n",
    "matplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'sans-serif',\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "sns.set_palette(sns.color_palette(\"husl\", 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharey=True, figsize=(5,5))\n",
    "df[df[\"f(Original)\"] == 1].groupby([\"Detector\"])[\"Tokens Edited Proportion\"].plot.hist(alpha=0.5, bins=20, ax=ax1 )\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"$f(d_i)$ = human → machine = $f(d_i^\\Omega)$\")\n",
    "\n",
    "df[df[\"f(Original)\"] == 0].groupby([\"Detector\"])[\"Tokens Edited Proportion\"].plot.hist(alpha=0.5, bins=20, ax=ax2 )\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.set_title(\"$f(d_i)$ = machine → human = $f(d_i^\\Omega)$\")\n",
    "ax2.set_xlabel(\"\")\n",
    "f.tight_layout()\n",
    "plt.savefig('./figures/contrastivity-label-flip.pgf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Tokens Edited Proportion\"] <= 0.50].groupby(\"Detector\")[\"Tokens Edited\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Tokens Edited Proportion\"] <= 0.50].groupby(\"Detector\")[\"Tokens Edited Proportion\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment/Calculate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Tokens Edited Proportion\"] <= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Explainer\",\n",
    "          \"Detector\",\n",
    "          \"f(d) → f(m)\",\n",
    "          \"Mean FI Original Cut Part\",\n",
    "          \"Mean FI Edited New Part\",\n",
    "          \"Mean FI Original Common Part\",\n",
    "          \"Mean FI Edited Common Part\",\n",
    "          \"[Score 1] cos sim\",\n",
    "          \"[Score 1] Krippendorff\",\n",
    "          \"[Score 4]\",\n",
    "          \"[Score 2]\",\n",
    "          \"[Score 3]\",\n",
    "          \"Tokens Edited Proportion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    random_run = 1\n",
    "    for explainer_class in explainer_classes:\n",
    "            if explainer_class == Random_Explainer:\n",
    "                 explainer = explainer_class(detector, seed=random_run) # 10 random runs per detector (Random_Explainer is 10 times in explainer_classes)\n",
    "                 random_run += 1\n",
    "            else:\n",
    "                explainer = explainer_class(detector)\n",
    "            for idx, (_, original, prompt, edited, f_original, f_edited, _,_,_,_,_,tokens_edited_proportion) in tqdm(list(df[df[\"Detector\"] == detector.__class__.__name__].iterrows()), desc=\"Gathering results\"):\n",
    "                target_label = f_edited\n",
    "                # if not (explainer.is_cached(original)) or not(explainer.is_cached(edited)):\n",
    "                #      continue\n",
    "                # The generation strategy above uses spacy's tokenizer (where punctuation chars are end up in seperate tokens). \n",
    "                # The explanation methods have their own tokenizers and FI scores are reported irt to those tokens\n",
    "                # For LIME, multiple punctuation chars can end up in the same token e.g.: \"!)\" is one token, \"!\" too, \"#+#++..,++##\" as well. \n",
    "                # This is problematic when comparing explanations:\n",
    "                # i.e. this can fail: assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt]\n",
    "                \n",
    "                # For this experiment, it is only important to separate the \"prompt\", which is the common part between the two documents, from the generated/cut parts\n",
    "                # Determining the bounds of the prompt has to be done AFTER tokenization:\n",
    "                # Strategy: change bounds until assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt] passes\n",
    "                lenght_promt = len(explainer.tokenize(prompt)) # this would suffice for SHAP, but not for lime (because it sometimes collapses punctuation chars into one token) \n",
    "                while explainer.tokenize(original)[0:lenght_promt] != explainer.tokenize(edited)[0:lenght_promt] or len(explainer.tokenize(original)[lenght_promt:]) == 0 or len(explainer.tokenize(edited)[lenght_promt:]) == 0:    \n",
    "                    lenght_promt -= 1 \n",
    "                lenght_promt = max(lenght_promt, 1) # if the first word is followed by a punctuation char, e.g., \"Example! Is a sentence.\" and the prompt is just \"Example\", the loop above would set lenght_promt=0. This happens as LIME tokenizes this to ['Example!', 'Is', 'a', 'sentence.']\n",
    "                assert explainer.tokenize(original)[1:lenght_promt] == explainer.tokenize(edited)[1:lenght_promt]\n",
    "                       \n",
    "                # get cut/edited parts\n",
    "                exp_original_cut_part = explainer.get_fi_scores(original, fill=True)[target_label][lenght_promt:] # TODO hard coded: \"machine\"\n",
    "                exp_edited_new_part = explainer.get_fi_scores(edited, fill=True)[target_label][lenght_promt:] # setting fill=True returns all features (not just the top_k) \n",
    "                fi_scores_exp_original_cut_part = np.array([fi_score for _, fi_score in exp_original_cut_part])\n",
    "                fi_scores_exp_edited_new_part =   np.array([fi_score for _, fi_score in exp_edited_new_part])\n",
    "                \n",
    "\n",
    "                # get common part\n",
    "                exp_original_common_part = explainer.get_fi_scores(original, fill=True)[target_label][0:lenght_promt]\n",
    "                exp_edited_common_part = explainer.get_fi_scores(edited, fill=True)[target_label][0:lenght_promt]\n",
    "                fi_scores_exp_original_common_part = np.array([fi_score for _, fi_score in exp_original_common_part])\n",
    "                fi_scores_exp_edited_common_part=    np.array([fi_score for _, fi_score in exp_edited_common_part])\n",
    "                \n",
    "                # build result row\n",
    "                score_2_new_and_cut_parts_opposite = np.mean(fi_scores_exp_edited_new_part) > np.mean(fi_scores_exp_original_cut_part)\n",
    "                score_4_max_fi_in_new_part = fi_scores_exp_edited_new_part.max() > fi_scores_exp_edited_common_part.max()\n",
    "                score_3_new_average_higher_than_common = np.mean(fi_scores_exp_edited_new_part) > np.mean(fi_scores_exp_edited_common_part)\n",
    "\n",
    "                cannonical_form = np.vstack([fi_scores_exp_original_common_part, fi_scores_exp_edited_common_part])\n",
    "                \n",
    "                assert len(fi_scores_exp_original_cut_part) > 0\n",
    "                assert len(fi_scores_exp_edited_new_part) > 0\n",
    "                assert len(fi_scores_exp_original_common_part) > 0\n",
    "                assert len(fi_scores_exp_edited_common_part) > 0\n",
    "\n",
    "                score_1_k_alpha = None\n",
    "                if np.all(cannonical_form==0): # the krippendorff library requires items to not all be the same\n",
    "                    score_1_k_alpha = 1\n",
    "                else:\n",
    "                    score_1_k_alpha = krippendorff.alpha(cannonical_form, level_of_measurement=\"interval\")\n",
    "\n",
    "                score_1_cos_sim = cosine_similarity(fi_scores_exp_original_common_part.reshape(1, -1),fi_scores_exp_edited_common_part.reshape(1, -1))[0,0]\n",
    "                row =     (\n",
    "                     explainer.__class__.__name__, \n",
    "                     detector.__class__.__name__, \n",
    "                     \"m → h\" if target_label else \"h → m\",\n",
    "                     np.mean(fi_scores_exp_original_cut_part), \n",
    "                     np.mean(fi_scores_exp_edited_new_part), \n",
    "                     np.mean(fi_scores_exp_original_common_part), \n",
    "                     np.mean(fi_scores_exp_edited_common_part),  \n",
    "                     score_1_cos_sim,               \n",
    "                     score_1_k_alpha,                             \n",
    "                     score_4_max_fi_in_new_part, \n",
    "                     score_2_new_and_cut_parts_opposite,\n",
    "                     score_3_new_average_higher_than_common,\n",
    "                     tokens_edited_proportion\n",
    "                     )\n",
    "                results.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(results, columns=columns)\n",
    "dff[\"Explainer\"] = dff[\"Explainer\"].str.replace(\"_Explainer\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_cols = [\n",
    "          \"[Score 1] cos sim\",\n",
    "          \"[Score 1] Krippendorff\",          \n",
    "          \"[Score 2]\",\n",
    "          \"[Score 3]\",\n",
    "          \"[Score 4]\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(styled_df, caption=\"TODO\", label=\"TODO\", environment=\"table\"):\n",
    "    return styled_df.format(precision=3).to_latex(environment=environment, convert_css=True, clines=\"all;data\", hrules=True, caption=caption, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_bold = dff.set_index([\"Explainer\", \"Detector\",\"f(d) → f(m)\"])[export_cols].groupby([\"Detector\",\"Explainer\",  \"f(d) → f(m)\"]).mean().style.highlight_max(props=\"font-weight: bold;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_dff(dff, groupby):\n",
    "    p_results = dff.set_index([\"Explainer\", \"Detector\",\"f(d) → f(m)\"])[export_cols].groupby(groupby).agg(\n",
    "    {\n",
    "          \"[Score 1] cos sim\": [\"count\", \"mean\"],\n",
    "          \"[Score 1] Krippendorff\": \"mean\",          \n",
    "          \"[Score 2]\": \"mean\",\n",
    "          \"[Score 3]\": \"mean\",\n",
    "          \"[Score 4]\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    p_results[('n')] = p_results[('[Score 1] cos sim', 'count')]\n",
    "    p_results = p_results.drop([('[Score 1] cos sim', 'count')], axis=1)\n",
    "\n",
    "    p_results = p_results[[list(p_results.columns)[-1]] + list(p_results.columns)[:-1]]\n",
    "    p_results.columns = [a for a, _ in p_results.columns]\n",
    "    p_results = p_results.rename(columns={'[Score 1] cos sim': '(1) cosine', '[Score 1] Krippendorff': '(1) Krippendorff', '[Score 2]': '(2)', '[Score 3]': '(3)', })\n",
    "    p_results = p_results.sort_values(by=[\"(2)\"], ascending=False)\n",
    "    p_results = p_results.style#.apply(highlight_max, subset=p_results.columns[1:], axis=None)\n",
    "    p_results.hide(\"[Score 4]\", axis=1)\n",
    "    return p_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_results_detector_level = style_dff(dff, groupby=[\"Detector\",\"Explainer\",  \"f(d) → f(m)\"])\n",
    "display(p_results_detector_level)\n",
    "p_results_aggregate_level = style_dff(dff, groupby=[\"Explainer\"]).hide(subset=[\"n\",\"[Score 4]\", \"(1) cosine\"], axis=1)\n",
    "display(p_results_aggregate_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df_to_latex(p_results_detector_level, label=\"contrastivity-explainer-detector-direction\", caption=\"Scores per detector, explainer and direction of change. For scores 2 and 3, higher values are better\", environment=\"longtable\")\n",
    "with open(\"figures/tables_contrastivity_detector.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(out)\n",
    "out = df_to_latex(p_results_aggregate_level, label=\"contrastivity-aggregate\", caption=\"Average scores per method. For scores 2 and 3, higher values are better\")\n",
    "with open(\"figures/tables_contrastivity_explainer.tex\", \"w\", encoding=\"UTF-8\") as text_file:\n",
    "    text_file.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_results_aggregate_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
