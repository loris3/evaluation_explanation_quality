{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "N_DEBUG = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "from detector_dummy import DetectorDummy\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Anchor_Explainer\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import sklearn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_human) 152\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "\n",
    "test = test[test[\"author\"] == \"human_answers\"]\n",
    "print(\"len(test_human)\", len(test))\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\" # convention: 0: machine, 1: human, see detector.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "#from detectgpt.detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorRadford,DetectorGuo]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer\n",
    "explainer_classes = [LIME_Explainer,SHAP_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "base_model_name=\"facebook/opt-350m\"\n",
    "openai_model = False\n",
    "\n",
    "cache_dir=\"./.cache\"\n",
    "# mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "# mask_tokenizer = transformers.AutoTokenizer.from_pretrained(model, model_max_length=mask_model.config.n_positions, cache_dir=cache_dir)#.to(DEVICE)\n",
    "do_top_k= False\n",
    "do_top_p= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir=cache_dir).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir, padding_side='left',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112    I've heard of handyman type people making a li...\n",
       "24     No. Securities brokers/dealers in the United S...\n",
       "600    Hello and Welcome to ‘Ask A Doctor’ service.I ...\n",
       "874    Use VTIVX. The \"Target Retirement 2045\" and \"T...\n",
       "528    Predictive analytics encompasses a variety of ...\n",
       "                             ...                        \n",
       "728    Hi, dearI have gone through your question. I c...\n",
       "640    Hi,Dear thanks for your query.this is a chroni...\n",
       "552    Tensor Processing Unit (TPU) is an AI accelera...\n",
       "148    I believe that capital gains do affect AGI, bu...\n",
       "354    I know it may not last longer but i was able t...\n",
       "Name: answer, Length: 152, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate/Load documents\n",
    "Generate or load documents from `./contrastivity_label_flip_pairs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Detector\", \"Original\", \"Prompt\", \"Edited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Original</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Detector                                           Original  \\\n",
       "0      DetectorRadford  I've heard of handyman type people making a li...   \n",
       "1      DetectorRadford  No. Securities brokers/dealers in the United S...   \n",
       "2      DetectorRadford  Hello and Welcome to ‘Ask A Doctor’ service.I ...   \n",
       "3      DetectorRadford  Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "4      DetectorRadford  Predictive analytics encompasses a variety of ...   \n",
       "..                 ...                                                ...   \n",
       "362  DetectorDetectGPT  Welcome at HCM    I have gone through your que...   \n",
       "363  DetectorDetectGPT  On what basis did you do your initial allocati...   \n",
       "364  DetectorDetectGPT  Peter A. Wegner (August 20, 1932 – July 27, 20...   \n",
       "365  DetectorDetectGPT  Hi, dearI have gone through your question. I c...   \n",
       "366  DetectorDetectGPT  I know it may not last longer but i was able t...   \n",
       "\n",
       "                                                Prompt  \\\n",
       "0    I've heard of handyman type people making a li...   \n",
       "1    No. Securities brokers/dealers in the United S...   \n",
       "2    Hello and Welcome to ‘Ask A Doctor’ service.I ...   \n",
       "3    Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "4    Predictive analytics encompasses a variety of ...   \n",
       "..                                                 ...   \n",
       "362  Welcome at HCM    I have gone through your que...   \n",
       "363  On what basis did you do your initial allocati...   \n",
       "364  Peter A. Wegner (August 20, 1932 – July 27, 20...   \n",
       "365  Hi, dearI have gone through your question. I c...   \n",
       "366  I know it may not last longer but i was able t...   \n",
       "\n",
       "                                                Edited  \n",
       "0    I've heard of handyman type people making a li...  \n",
       "1    No. Securities brokers/dealers in the United S...  \n",
       "2    Hello and Welcome to ‘Ask A Doctor’ service.I ...  \n",
       "3    Use VTIVX. The \"Target Retirement 2045\" and \"T...  \n",
       "4    Predictive analytics encompasses a variety of ...  \n",
       "..                                                 ...  \n",
       "362  Welcome at HCM    I have gone through your que...  \n",
       "363  On what basis did you do your initial allocati...  \n",
       "364  Peter A. Wegner (August 20, 1932 – July 27, 20...  \n",
       "365  Hi, dearI have gone through your question. I c...  \n",
       "366  I know it may not last longer but i was able t...  \n",
       "\n",
       "[367 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(\"./contrastivity_label_flip_pairs.csv\"):\n",
    "    df = pd.read_csv(\"./contrastivity_label_flip_pairs.csv\")\n",
    "else: \n",
    "    df = pd.DataFrame([], columns=columns)\n",
    "    # write headers (mode != \"a\")\n",
    "    df.to_csv(\"./contrastivity_label_flip_pairs.csv\", encoding=\"UTF-8\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Edited</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DetectorDetectGPT</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>149</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Original  Prompt  Edited\n",
       "Detector                                   \n",
       "DetectorDetectGPT        76      76      76\n",
       "DetectorGuo             149      68      68\n",
       "DetectorRadford         142     142     142"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Detector\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for detector_class in detector_classes:\n",
    "#     detector = detector_class()\n",
    "#     for document in tqdm(documents[detector.predict_label(documents).astype(bool)], desc=\"Generating perturbations\"): # only use those where f(x) = human\n",
    "#         np.random.seed(42)\n",
    "#         torch.manual_seed(42)\n",
    "\n",
    "#         if df[df[\"Detector\"] == detector.__class__.__name__][\"Original\"].str.contains(document, regex=False).any(): # check if document is in csv, if yes, skip\n",
    "#             continue\n",
    "#         doc = nlp(document)\n",
    "#         n_tokens_original = len(base_tokenizer(document, return_tensors=\"pt\", padding=True).to(DEVICE).input_ids[0])\n",
    "\n",
    "#         substrings = [''.join(token.text_with_ws for token in doc[:-i]) for i in range(1,len(document)) ]\n",
    "#         substrings = [substring for substring in substrings if substring != \"\"] # at least one token\n",
    "\n",
    "#         n_generations_per_lenght = 5\n",
    "#         substrings = list(itertools.chain.from_iterable(itertools.repeat(s, n_generations_per_lenght) for s in substrings))\n",
    "\n",
    "#         batch_size = 20 # adjust so it fits in your GPU memory\n",
    "#         row = (detector.__class__.__name__, document, None, None) # to mark the document in the csv cache if no pair is found\n",
    "#         for batch in (sklearn.utils.gen_batches(len(substrings), batch_size)):\n",
    "#             encoded = base_tokenizer(substrings[batch], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "\n",
    "#             outputs = base_model.generate(**encoded, min_length=n_tokens_original-5, max_length=n_tokens_original+5, do_sample=True, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)\n",
    "#             decoded = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#             predictions = detector.predict_label(decoded)\n",
    "#             if any(predictions != 1):\n",
    "#                 first_new_label = (predictions!=1).argmax(axis=0)\n",
    "#                 assert decoded[first_new_label] != document\n",
    "#                 #                                   original  prompt                              first instance that flips label\n",
    "#                 row = (detector.__class__.__name__, document, substrings[batch][first_new_label], decoded[first_new_label])\n",
    "#                 break\n",
    "#         pd.DataFrame([row], columns=columns).to_csv(\"./contrastivity_label_flip_pairs.csv\", mode=\"a\", encoding=\"UTF-8\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./contrastivity_label_flip_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Original</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "      <td>I've heard of handyman type people making a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "      <td>No. Securities brokers/dealers in the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "      <td>Hello and Welcome to ‘Ask A Doctor’ service.I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "      <td>Use VTIVX. The \"Target Retirement 2045\" and \"T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "      <td>Welcome at HCM    I have gone through your que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "      <td>On what basis did you do your initial allocati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "      <td>Peter A. Wegner (August 20, 1932 – July 27, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "      <td>Hi, dearI have gone through your question. I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>DetectorDetectGPT</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "      <td>I know it may not last longer but i was able t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Detector                                           Original  \\\n",
       "0      DetectorRadford  I've heard of handyman type people making a li...   \n",
       "1      DetectorRadford  No. Securities brokers/dealers in the United S...   \n",
       "2      DetectorRadford  Hello and Welcome to ‘Ask A Doctor’ service.I ...   \n",
       "3      DetectorRadford  Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "4      DetectorRadford  Predictive analytics encompasses a variety of ...   \n",
       "..                 ...                                                ...   \n",
       "362  DetectorDetectGPT  Welcome at HCM    I have gone through your que...   \n",
       "363  DetectorDetectGPT  On what basis did you do your initial allocati...   \n",
       "364  DetectorDetectGPT  Peter A. Wegner (August 20, 1932 – July 27, 20...   \n",
       "365  DetectorDetectGPT  Hi, dearI have gone through your question. I c...   \n",
       "366  DetectorDetectGPT  I know it may not last longer but i was able t...   \n",
       "\n",
       "                                                Prompt  \\\n",
       "0    I've heard of handyman type people making a li...   \n",
       "1    No. Securities brokers/dealers in the United S...   \n",
       "2    Hello and Welcome to ‘Ask A Doctor’ service.I ...   \n",
       "3    Use VTIVX. The \"Target Retirement 2045\" and \"T...   \n",
       "4    Predictive analytics encompasses a variety of ...   \n",
       "..                                                 ...   \n",
       "362  Welcome at HCM    I have gone through your que...   \n",
       "363  On what basis did you do your initial allocati...   \n",
       "364  Peter A. Wegner (August 20, 1932 – July 27, 20...   \n",
       "365  Hi, dearI have gone through your question. I c...   \n",
       "366  I know it may not last longer but i was able t...   \n",
       "\n",
       "                                                Edited  \n",
       "0    I've heard of handyman type people making a li...  \n",
       "1    No. Securities brokers/dealers in the United S...  \n",
       "2    Hello and Welcome to ‘Ask A Doctor’ service.I ...  \n",
       "3    Use VTIVX. The \"Target Retirement 2045\" and \"T...  \n",
       "4    Predictive analytics encompasses a variety of ...  \n",
       "..                                                 ...  \n",
       "362  Welcome at HCM    I have gone through your que...  \n",
       "363  On what basis did you do your initial allocati...  \n",
       "364  Peter A. Wegner (August 20, 1932 – July 27, 20...  \n",
       "365  Hi, dearI have gone through your question. I c...  \n",
       "366  I know it may not last longer but i was able t...  \n",
       "\n",
       "[286 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detector</th>\n",
       "      <th>Original</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Detector, Original, Prompt, Edited]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Original\"] == df[\"Edited\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic idea: assert that exp(original)[original - prompt] <substantially different than> exp(label_flip_example)[label_flip_example - prompt]\n",
    "# i.e. the new/changed section is assigned the opposite label (TODO hard coded: \"machine\") more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering results: 142it [00:01, 110.33it/s]\n",
      "Gathering results: 142it [00:05, 25.58it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Gathering results: 68it [00:00, 148.47it/s]\n",
      "Gathering results: 68it [00:00, 78.16it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for detector_class in detector_classes:\n",
    "    detector = detector_class()\n",
    "    for explainer_class in explainer_classes:\n",
    "            explainer = explainer_class(detector)\n",
    "            for idx, (_, original, prompt, edited) in tqdm(df[df[\"Detector\"] == detector.__class__.__name__].iterrows(), desc=\"Gathering results\"):\n",
    "                \n",
    "                # if not (explainer.is_cached(original)) or not(explainer.is_cached(edited)):\n",
    "                #      continue\n",
    "                # The generation strategy above uses spacy's tokenizer (where punctuation chars are end up in seperate tokens). \n",
    "                # The explanation methods have their own tokenizers and FI scores are reported irt to those tokens\n",
    "                # For LIME, multiple punctuation chars can end up in the same token e.g.: \"!)\" is one token, \"!\" too, \"#+#++..,++##\" as well. \n",
    "                # This is problematic when comparing explanations:\n",
    "                # i.e. this can fail: assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt]\n",
    "                \n",
    "                # For this experiment, it is only important to separate the \"prompt\", which is the common part between the two documents, from the generated/cut parts\n",
    "                # Determining the bounds of the prompt has to be done AFTER tokenization:\n",
    "                # Strategy: change bounds until assert explainer.tokenize(original)[0:lenght_promt] == explainer.tokenize(edited)[0:lenght_promt] passes\n",
    "                lenght_promt = len(explainer.tokenize(prompt)) # this would suffice for SHAP, but not for lime (because it sometimes collapses punctuation chars into one token) \n",
    "                while explainer.tokenize(original)[0:lenght_promt] != explainer.tokenize(edited)[0:lenght_promt] or len(explainer.tokenize(original)[lenght_promt:]) == 0 or len(explainer.tokenize(edited)[lenght_promt:]) == 0:    \n",
    "                    lenght_promt -= 1 \n",
    "                lenght_promt = max(lenght_promt, 1) # if the first word is followed by a punctuation char, e.g., \"Example! Is a sentence.\" and the prompt is just \"Example\", the loop above would set lenght_promt=0. This happens as LIME tokenizes this to ['Example!', 'Is', 'a', 'sentence.']\n",
    "                assert explainer.tokenize(original)[1:lenght_promt] == explainer.tokenize(edited)[1:lenght_promt]\n",
    "                       \n",
    "                # get cut/edited parts\n",
    "                exp_original_cut_part = explainer.get_fi_scores(original, fill=True)[0][lenght_promt:] # TODO hard coded: \"machine\"\n",
    "                exp_edited_new_part = explainer.get_fi_scores(edited, fill=True)[0][lenght_promt:] # setting fill=True returns all features (not just the top_k) \n",
    "                fi_scores_exp_original_cut_part = np.array([fi_score for _, fi_score in exp_original_cut_part])\n",
    "                fi_scores_exp_edited_new_part =   np.array([fi_score for _, fi_score in exp_edited_new_part])\n",
    "                \n",
    "\n",
    "                # get common part\n",
    "                exp_original_common_part = explainer.get_fi_scores(original, fill=True)[0][0:lenght_promt]\n",
    "                exp_edited_common_part = explainer.get_fi_scores(edited, fill=True)[0][0:lenght_promt]\n",
    "                fi_scores_exp_original_common_part = np.array([fi_score for _, fi_score in exp_original_common_part])\n",
    "                fi_scores_exp_edited_common_part=    np.array([fi_score for _, fi_score in exp_edited_common_part])\n",
    "                \n",
    "                # build result row\n",
    "                if len(fi_scores_exp_original_cut_part) == 0 or(fi_scores_exp_edited_new_part.shape[0] == 0 or fi_scores_exp_edited_common_part.shape[0] == 0 ):\n",
    "                  #  print(cannonical_form.shape)\n",
    "                    print(lenght_promt)\n",
    "                    print(original)\n",
    "                    print(prompt)\n",
    "                    print(edited)\n",
    "                    print(fi_scores_exp_edited_new_part)\n",
    "                    print(fi_scores_exp_edited_common_part)\n",
    "                    print(fi_scores_exp_original_cut_part)\n",
    "               \n",
    "                max_fi_in_new_part = fi_scores_exp_edited_new_part.max() > fi_scores_exp_edited_common_part.max()\n",
    "\n",
    "\n",
    "                cannonical_form = np.vstack([fi_scores_exp_original_common_part, fi_scores_exp_edited_common_part])\n",
    "                \n",
    "                assert len(fi_scores_exp_original_cut_part) > 0\n",
    "                assert len(fi_scores_exp_edited_new_part) > 0\n",
    "                assert len(fi_scores_exp_original_common_part) > 0\n",
    "                assert len(fi_scores_exp_edited_common_part) > 0\n",
    "\n",
    "                k_alpha = None\n",
    "                if np.all(cannonical_form==0): # the krippendorff library requires items to not all be the same\n",
    "                    k_alpha = 1\n",
    "                else:\n",
    "                    k_alpha = krippendorff.alpha(cannonical_form, level_of_measurement=\"interval\")\n",
    "\n",
    "                cos_sim = cosine_similarity(fi_scores_exp_original_common_part.reshape(1, -1),fi_scores_exp_edited_common_part.reshape(1, -1))[0,0]\n",
    "                #columns = [\"Explainer\",                  \"Detector\",                 \"Mean FI Original Cut Part\",               \"Mean FI Edited New Part\",              \"Mean FI Original Common Part\",              \"Mean FI Edited Common Part\",             \"cos_sim Common Part\",  \"Krippendorff's Alpha Common Part\", \"Maximum FI for Machine in New Part\"                                                                                                             ]\n",
    "                row =     (explainer.__class__.__name__, detector.__class__.__name__, np.mean(fi_scores_exp_original_cut_part), np.mean(fi_scores_exp_edited_new_part), np.mean(fi_scores_exp_original_common_part), np.mean(fi_scores_exp_edited_common_part),  cos_sim,               k_alpha,                             max_fi_in_new_part                                                      )\n",
    "                results.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th>Mean FI Original Cut Part</th>\n",
       "      <th>Mean FI Edited New Part</th>\n",
       "      <th>Mean FI Original Common Part</th>\n",
       "      <th>Mean FI Edited Common Part</th>\n",
       "      <th>cos_sim Common Part</th>\n",
       "      <th>Krippendorff's Alpha Common Part</th>\n",
       "      <th>Maximum FI for Machine in New Part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>-0.022479</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.012822</td>\n",
       "      <td>0.640168</td>\n",
       "      <td>0.525773</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>-0.003730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.035247</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>-0.021161</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>-0.018898</td>\n",
       "      <td>-0.007048</td>\n",
       "      <td>0.690172</td>\n",
       "      <td>0.677160</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>0.138192</td>\n",
       "      <td>0.096623</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIME_Explainer</td>\n",
       "      <td>DetectorRadford</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>0.983648</td>\n",
       "      <td>-0.798943</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.006295</td>\n",
       "      <td>0.836267</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.002009</td>\n",
       "      <td>0.244874</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.010522</td>\n",
       "      <td>-0.015239</td>\n",
       "      <td>0.796072</td>\n",
       "      <td>0.392361</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>SHAP_Explainer</td>\n",
       "      <td>DetectorGuo</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>-0.991496</td>\n",
       "      <td>-0.666666</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Explainer         Detector  Mean FI Original Cut Part  \\\n",
       "0    LIME_Explainer  DetectorRadford                  -0.022479   \n",
       "1    LIME_Explainer  DetectorRadford                  -0.002890   \n",
       "2    LIME_Explainer  DetectorRadford                  -0.021161   \n",
       "3    LIME_Explainer  DetectorRadford                   0.000000   \n",
       "4    LIME_Explainer  DetectorRadford                   0.000000   \n",
       "..              ...              ...                        ...   \n",
       "415  SHAP_Explainer      DetectorGuo                   0.000003   \n",
       "416  SHAP_Explainer      DetectorGuo                   0.000274   \n",
       "417  SHAP_Explainer      DetectorGuo                  -0.000001   \n",
       "418  SHAP_Explainer      DetectorGuo                   0.005107   \n",
       "419  SHAP_Explainer      DetectorGuo                  -0.000002   \n",
       "\n",
       "     Mean FI Edited New Part  Mean FI Original Common Part  \\\n",
       "0                   0.001395                     -0.008005   \n",
       "1                   0.001969                     -0.001524   \n",
       "2                   0.006397                     -0.018898   \n",
       "3                   0.000000                     -0.002178   \n",
       "4                   0.000275                     -0.000805   \n",
       "..                       ...                           ...   \n",
       "415                 0.006129                     -0.000016   \n",
       "416                 0.026362                     -0.000253   \n",
       "417                 0.005787                     -0.000005   \n",
       "418                 0.014641                     -0.010522   \n",
       "419                 0.009503                     -0.000001   \n",
       "\n",
       "     Mean FI Edited Common Part  cos_sim Common Part  \\\n",
       "0                     -0.012822             0.640168   \n",
       "1                     -0.003730             0.000000   \n",
       "2                     -0.007048             0.690172   \n",
       "3                     -0.001524             0.138192   \n",
       "4                      0.000485             0.000000   \n",
       "..                          ...                  ...   \n",
       "415                   -0.007838             0.983648   \n",
       "416                   -0.006295             0.836267   \n",
       "417                   -0.002009             0.244874   \n",
       "418                   -0.015239             0.796072   \n",
       "419                    0.000241            -0.991496   \n",
       "\n",
       "     Krippendorff's Alpha Common Part  Maximum FI for Machine in New Part  \n",
       "0                            0.525773                                True  \n",
       "1                           -0.035247                                True  \n",
       "2                            0.677160                               False  \n",
       "3                            0.096623                               False  \n",
       "4                            0.002750                               False  \n",
       "..                                ...                                 ...  \n",
       "415                         -0.798943                                True  \n",
       "416                          0.005847                                True  \n",
       "417                          0.010832                                True  \n",
       "418                          0.392361                                True  \n",
       "419                         -0.666666                                True  \n",
       "\n",
       "[420 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"Explainer\",                  \"Detector\",                 \"Mean FI Original Cut Part\",               \"Mean FI Edited New Part\",              \"Mean FI Original Common Part\",              \"Mean FI Edited Common Part\",    \"cos_sim Common Part\", \"Krippendorff's Alpha Common Part\",\"Maximum FI for Machine in New Part\"]\n",
    "dff = pd.DataFrame(results, columns=columns)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean FI Original Cut Part</th>\n",
       "      <th>Mean FI Edited New Part</th>\n",
       "      <th>Mean FI Original Common Part</th>\n",
       "      <th>Mean FI Edited Common Part</th>\n",
       "      <th>cos_sim Common Part</th>\n",
       "      <th>Krippendorff's Alpha Common Part</th>\n",
       "      <th>Maximum FI for Machine in New Part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th>Detector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LIME_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.054476</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>-0.005011</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>-0.006190</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>0.285810</td>\n",
       "      <td>0.242693</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SHAP_Explainer</th>\n",
       "      <th>DetectorGuo</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.003422</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>-0.216423</td>\n",
       "      <td>0.955882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetectorRadford</th>\n",
       "      <td>-0.011269</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>-0.007162</td>\n",
       "      <td>0.755039</td>\n",
       "      <td>0.635299</td>\n",
       "      <td>0.542254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Mean FI Original Cut Part  \\\n",
       "Explainer      Detector                                     \n",
       "LIME_Explainer DetectorGuo                       0.000003   \n",
       "               DetectorRadford                  -0.005011   \n",
       "SHAP_Explainer DetectorGuo                       0.000406   \n",
       "               DetectorRadford                  -0.011269   \n",
       "\n",
       "                                Mean FI Edited New Part  \\\n",
       "Explainer      Detector                                   \n",
       "LIME_Explainer DetectorGuo                     0.000636   \n",
       "               DetectorRadford                 0.003827   \n",
       "SHAP_Explainer DetectorGuo                     0.010159   \n",
       "               DetectorRadford                 0.008824   \n",
       "\n",
       "                                Mean FI Original Common Part  \\\n",
       "Explainer      Detector                                        \n",
       "LIME_Explainer DetectorGuo                          0.000004   \n",
       "               DetectorRadford                     -0.006190   \n",
       "SHAP_Explainer DetectorGuo                         -0.000436   \n",
       "               DetectorRadford                     -0.008248   \n",
       "\n",
       "                                Mean FI Edited Common Part  \\\n",
       "Explainer      Detector                                      \n",
       "LIME_Explainer DetectorGuo                        0.000510   \n",
       "               DetectorRadford                   -0.005440   \n",
       "SHAP_Explainer DetectorGuo                       -0.003422   \n",
       "               DetectorRadford                   -0.007162   \n",
       "\n",
       "                                cos_sim Common Part  \\\n",
       "Explainer      Detector                               \n",
       "LIME_Explainer DetectorGuo                 0.054476   \n",
       "               DetectorRadford             0.285810   \n",
       "SHAP_Explainer DetectorGuo                 0.403014   \n",
       "               DetectorRadford             0.755039   \n",
       "\n",
       "                                Krippendorff's Alpha Common Part  \\\n",
       "Explainer      Detector                                            \n",
       "LIME_Explainer DetectorGuo                              0.006498   \n",
       "               DetectorRadford                          0.242693   \n",
       "SHAP_Explainer DetectorGuo                             -0.216423   \n",
       "               DetectorRadford                          0.635299   \n",
       "\n",
       "                                Maximum FI for Machine in New Part  \n",
       "Explainer      Detector                                             \n",
       "LIME_Explainer DetectorGuo                                0.779412  \n",
       "               DetectorRadford                            0.563380  \n",
       "SHAP_Explainer DetectorGuo                                0.955882  \n",
       "               DetectorRadford                            0.542254  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_detector_level = dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\", \"Detector\"]).mean()\n",
    "results_detector_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean FI Original Cut Part</th>\n",
       "      <th>Mean FI Edited New Part</th>\n",
       "      <th>Mean FI Original Common Part</th>\n",
       "      <th>Mean FI Edited Common Part</th>\n",
       "      <th>cos_sim Common Part</th>\n",
       "      <th>Krippendorff's Alpha Common Part</th>\n",
       "      <th>Maximum FI for Machine in New Part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Explainer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LIME_Explainer</th>\n",
       "      <td>-0.003387</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>0.210902</td>\n",
       "      <td>0.166211</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHAP_Explainer</th>\n",
       "      <td>-0.007489</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0.641050</td>\n",
       "      <td>0.359503</td>\n",
       "      <td>0.676190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Mean FI Original Cut Part  Mean FI Edited New Part  \\\n",
       "Explainer                                                            \n",
       "LIME_Explainer                  -0.003387                 0.002794   \n",
       "SHAP_Explainer                  -0.007489                 0.009256   \n",
       "\n",
       "                Mean FI Original Common Part  Mean FI Edited Common Part  \\\n",
       "Explainer                                                                  \n",
       "LIME_Explainer                     -0.004184                   -0.003513   \n",
       "SHAP_Explainer                     -0.005718                   -0.005951   \n",
       "\n",
       "                cos_sim Common Part  Krippendorff's Alpha Common Part  \\\n",
       "Explainer                                                               \n",
       "LIME_Explainer             0.210902                          0.166211   \n",
       "SHAP_Explainer             0.641050                          0.359503   \n",
       "\n",
       "                Maximum FI for Machine in New Part  \n",
       "Explainer                                           \n",
       "LIME_Explainer                            0.633333  \n",
       "SHAP_Explainer                            0.676190  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_explainer_level = dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\"]).mean()\n",
    "results_explainer_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_rel\n",
    "from scipy.stats.mstats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explainer\n",
       "LIME_Explainer       (-9.860397927464787, 4.4651274511593e-19)\n",
       "SHAP_Explainer    (-19.458669358910566, 8.295188895065286e-49)\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\"]).apply(lambda group: ttest_rel(group[\"Mean FI Original Cut Part\"], group[\"Mean FI Edited New Part\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explainer\n",
       "LIME_Explainer    (-2.317917477363351, 0.02142119917196867)\n",
       "SHAP_Explainer     (0.2997543385384911, 0.7646624508180425)\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\"]).apply(lambda group: ttest_rel(group[\"Mean FI Original Common Part\"], group[\"Mean FI Edited Common Part\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explainer\n",
       "LIME_Explainer    (3.9999999999999982, 8.789002398047055e-05)\n",
       "SHAP_Explainer    (5.443475073719952, 1.4574983832591005e-07)\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.set_index([\"Explainer\", \"Detector\"]).groupby([\"Explainer\"]).apply(lambda group: ttest_1samp(group[\"Maximum FI for Machine in New Part\"], popmean=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
