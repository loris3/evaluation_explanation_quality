{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates new train and test splits from Guo et al.'s Huggingface dataset.\n",
    "Their CSV file on Google Drive appears to mix up questions and answers (see [this Github issue](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection/issues/30)). \n",
    "While not directly relevant here (the questions won't be used), it can't be easily verified if other issues were introduced when generating those splits.\n",
    "\n",
    "Also, human answers in reddit_eli5 and open_qa appear to have artifacts in the form of spaces added before/after punctuation. For open_qa is already part of WikiQACorpus! E.g. whenever a word is a link. This effectively watermarks everything! \n",
    "\n",
    "Those two datasets are therefore excluded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by document length as human responses tend to be shorter in this dataset on average\n",
    "MAX_WORDS = 150\n",
    "MIN_WORDS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "h3_dataset_hf_raw = load_dataset(\"Hello-SimpleAI/HC3\",name=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_dataset_hf = pd.DataFrame(h3_dataset_hf_raw[\"train\"], columns=list(h3_dataset_hf_raw[\"train\"].info.features.keys()))\n",
    "h3_dataset_hf = h3_dataset_hf.explode(\"human_answers\").explode(\"chatgpt_answers\")\n",
    "h3_dataset_hf = pd.melt(h3_dataset_hf, id_vars=[\"question\", \"source\", \"id\"], value_vars=[\"human_answers\", \"chatgpt_answers\"], value_name=\"answer\", var_name=\"author\")\n",
    "#h3_dataset_hf[\"label\"] = h3_dataset_hf[\"author\"] == \"chatgpt_answers\"\n",
    "#h3_dataset_hf[\"label\"] = h3_dataset_hf[\"label\"].astype(int)\n",
    "h3_dataset_hf[\"id\"] = h3_dataset_hf[\"id\"].astype(int)\n",
    "h3_dataset_hf = h3_dataset_hf.dropna(subset=[\"answer\"])\n",
    "\n",
    "# h3_dataset_hf[\"answer\"] = h3_dataset_hf[\"answer\"].replace(r'\\n','', regex=True) # for comparision only: the csvs don't have nl\n",
    "# h3_dataset_hf[\"question\"] = h3_dataset_hf[\"question\"].replace(r'\\n','', regex=True)\n",
    "\n",
    "h3_dataset_hf = h3_dataset_hf[~(h3_dataset_hf[\"source\"].str.contains(\"open_qa\"))] # the original human dataset has artifacts form hyperlinks effectively watermarking human text \n",
    "h3_dataset_hf = h3_dataset_hf[~(h3_dataset_hf[\"source\"].str.contains(\"reddit_eli5\"))] # the human dataset has artifacts \n",
    "#h3_dataset_hf = h3_dataset_hf[~(h3_dataset_hf[\"source\"].str.contains(\"reddit_eli5\"))] # the human dataset has artifacts \n",
    "h3_dataset_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by document length as human responses tend to be shorter in this dataset on average\n",
    "\n",
    "doc_within_range = h3_dataset_hf[\"answer\"].str.split().str.len().apply(lambda l : (l <= MAX_WORDS and l >= MIN_WORDS))\n",
    "df_min_max_len = h3_dataset_hf[doc_within_range]\n",
    "df_min_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out responses that contain some of the \"indicating words\" provided by Guo et al.\n",
    "# note that only some are used: they also remove certain stock phrases like \"There are several ways\" for their filtered version\n",
    "\n",
    "\n",
    "indicating_words_chatgpt_en = [\n",
    "    \"AI assistant\",\n",
    "    \"AI language model\",\n",
    "    \"I'm sorry\", # e.g. ... but I am not a medical doctor; but I am an AI language; to hear about your husband's symptoms \n",
    "    \"It is difficult for me\",\n",
    "    \"Contents may violate our content\",\n",
    "    \"This content may violate our content policy\",\n",
    "    \"Can you please provide the statement\",\n",
    "    \"If you have any more questions, please don't hesitate to ask.\",\n",
    "    \"If you have any questions about\",\n",
    "    \"Let me know if you have any other questions\",\n",
    "    \"If you have any more questions, feel free to ask!\",\n",
    "    \"!\\rnetwork error\\r\\r\\r\\r\", # new\n",
    "    \"Free Research Preview.\", # new\n",
    "    \"Your feedback will help us improve.\", # new\n",
    "    ]\n",
    "\n",
    "remove = df_min_max_len[\"answer\"].str.contains('|'.join(indicating_words_chatgpt_en), regex=True)\n",
    "\n",
    "print(\"Removing {} documents, specifically:\".format(len(df_min_max_len[remove])))\n",
    "display(df_min_max_len[remove][\"author\"].value_counts())\n",
    "\n",
    "df = df_min_max_len[~remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"question\", \"answer\", \"author\"])\n",
    "\n",
    "# There are duplicated human answers, not chat\n",
    "df = df.drop_duplicates(subset=[ \"answer\", \"author\"])\n",
    "print(\"Dropped {} duplicates (was {})\".format(len_before - len(df),len_before))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=[\"answer\"])].sort_values(by=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equal_numbers_of_answers(group):\n",
    "    human_answers = group[group[\"author\"] == \"human_answers\"]\n",
    "    chatgpt_answers = group[group[\"author\"] == \"chatgpt_answers\"]\n",
    "    n = min(len(human_answers), len(chatgpt_answers))\n",
    "    return pd.concat([human_answers.sample(n, random_state=42), chatgpt_answers.sample(n, random_state=42)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a balanced dataset: sample pairs of answers to the same question\n",
    "\n",
    "df = df.groupby([\"question\"]).apply(get_equal_numbers_of_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} human answers, {} chat\".format(len(df[df[\"author\"] == \"human_answers\"]), len(df[df[\"author\"] == \"chatgpt_answers\"])))\n",
    "assert len(df[df[\"author\"] == \"human_answers\"]) == len(df[df[\"author\"] == \"chatgpt_answers\"]), \"Sampling not balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train\", len(train))\n",
    "print(\"test\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(\"./dataset_train.pkl\")\n",
    "train.to_csv(\"./dataset/train.csv\", index=False, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle(\"./dataset_test.pkl\")\n",
    "test.to_csv(\"./dataset/test.csv\", index=False, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the experiments where run on the .pkl files, providing .csv files for convenience\n",
    "from pandas.testing import assert_frame_equal\n",
    "assert_frame_equal(pd.read_pickle(\"./dataset_train.pkl\").reset_index(drop=True), pd.read_csv(\"./dataset/train.csv\").reset_index(drop=True),check_dtype=False)\n",
    "assert_frame_equal(pd.read_pickle(\"./dataset_test.pkl\").reset_index(drop=True), pd.read_csv(\"./dataset/test.csv\").reset_index(drop=True), check_dtype=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
