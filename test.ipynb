{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is server.py from Radford et al without the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectgpt.detector_detectgpt import DetectorDetectGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = DetectorDetectGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector.dataset import Corpus, EncodedDataset\n",
    "from gpt2outputdataset.detector.download import download\n",
    "from gpt2outputdataset.detector.train import load_datasets\n",
    "# load_datasets(\"./data\", real_dataset=\"webtext\", fake_dataset=\"small-117M-k40\",  tokenizer = RobertaTokenizer.from_pretrained('roberta-large'), batch_size=24,\n",
    "#                                                    max_sequence_length=128, random_sequence_length=False, epoch_size=None,\n",
    "#                                                    token_dropout=None, seed=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./h3/all.jsonl\", encoding=\"utf8\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human, machine = zip(*[(question[\"human_answers\"][0], question[\"chatgpt_answers\"][0]) for question in data if len(question[\"chatgpt_answers\"]) and len(question[\"human_answers\"])])\n",
    "human = list(human)\n",
    "machine = list(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human = Corpus(\"webtext\", data_dir=\"./data\").train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine = Corpus(\"small-117M-k40\", data_dir=\"./data\").test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_machine = detector.predict_proba_(machine[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_human = detector.predict_proba_(human[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(np.min(d_machine),np.min(d_human)), max(np.max(d_machine),np.max(d_human)), 100)\n",
    "\n",
    "pyplot.hist(d_human, bins, alpha=0.5, label='human')\n",
    "pyplot.hist(d_machine, bins, alpha=0.5, label='machine')\n",
    "#pyplot.hist(real_xl, bins, alpha=0.5, label='real_xl')\n",
    "#pyplot.hist(samples_xl, bins, alpha=0.5, label='samples_xl')\n",
    "pyplot.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = np.array([False] * len(d_human) + [True]* len(d_machine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tresholds =[]\n",
    "acc = []\n",
    "for treshold in np.arange(-5, 5, 0.01, dtype=float):\n",
    "    is_machine = np.hstack([np.array(d_human) > treshold, np.array(d_machine) > treshold])\n",
    "    tresholds.append(treshold)\n",
    "    acc.append(np.count_nonzero(gold_labels == is_machine)/(len(gold_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(tresholds, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(np.min(d_machine),np.min(d_human)), max(np.max(d_machine),np.max(d_human)), 100)\n",
    "\n",
    "pyplot.hist(d_human, bins, alpha=0.5, label='human')\n",
    "pyplot.hist(d_machine, bins, alpha=0.5, label='machine')\n",
    "#pyplot.hist(real_xl, bins, alpha=0.5, label='real_xl')\n",
    "#pyplot.hist(samples_xl, bins, alpha=0.5, label='samples_xl')\n",
    "pyplot.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector.dataset import Corpus, EncodedDataset\n",
    "from gpt2outputdataset.detector.download import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector.train import load_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_datasets(\"./data\", real_dataset=\"webtext\", fake_dataset=\"xl-1542M-nucleus\",  tokenizer = RobertaTokenizer.from_pretrained('roberta-large'), batch_size=24,\n",
    "                                                    max_sequence_length=128, random_sequence_length=False, epoch_size=None,\n",
    "                                                    token_dropout=None, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_corpus = Corpus(\"webtext\", data_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_corpus.train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_corpus = Corpus(\"xl-1542M-nucleus\", data_dir=\"./data\")\n",
    "\n",
    "real_train, real_valid = real_corpus.train, real_corpus.valid\n",
    "fake_train, fake_valid = fake_corpus.train, fake_corpus.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2outputdataset.detector_radford import DetectorRadford\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_human = u\"It often happens that while working on one project, you need to use another project from within it. Perhaps it’s a library that a third party developed or that you’re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.\"\n",
    "text_gpt2 = u\"Good news for The Logan Society members [kiss_off/Stephen] and their allies. There's a new news article cited above that says we can rest assured as of now that the project is not dead, but will instead be put on permanent hiatus for the foreseeable future. There's no official reason for the decision, though we can hazard some guesses which do have Hexpoint infringements tolement, and multiple individuals have already taken to Discord to say it isn't a good idea. Keep in mind that this article is authored by the person who actually contacted Hexpoint directly, and he contradicted the article's claims up and down in more subtle ways.\"\n",
    "lorem_ipsum = u\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "nyt = u'European Union policymakers agreed on Friday to a sweeping new law to regulate artificial intelligence, one of the world’s first comprehensive attempts to limit the use of a rapidly evolving technology that has wide-ranging societal and economic implications. The law, called the A.I. Act, sets a new global benchmark for countries seeking to harness the potential benefits of the technology, while trying to protect against its possible risks, like automating jobs, spreading misinformation online and endangering national security. The law still needs to go through a few final steps for approval, but the political agreement means its key outlines have been set.'\n",
    "emma = u'Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister’s marriage, been mistress of his house from a very early period. Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection. Sixteen years had Miss Taylor been in Mr. Woodhouse’s family, less as a governess than a friend, very fond of both daughters, but particularly of Emma. Between them it was more the intimacy of sisters. Even before Miss Taylor had ceased to hold the nominal office of governess, the mildness of her temper had hardly allowed her to impose any restraint; and the shadow of authority being now long passed away, they had been living together as friend and friend very mutually attached, and Emma doing just what she liked; highly esteeming Miss Taylor’s judgment, but directed chiefly by her own.'\n",
    "human_h3 = u\"\"\"Basically there are many categories of \" Best Seller \" . Replace \" Best Seller \" by something like \" Oscars \" and every \" best seller \" book is basically an \" oscar - winning \" book . May not have won the \" Best film \" , but even if you won the best director or best script , you 're still an \" oscar - winning \" film . Same thing for best sellers . Also , IIRC the rankings change every week or something like that . Some you might not be best seller one week , but you may be the next week . I guess even if you do n't stay there for long , you still achieved the status . Hence , # 1 best seller .\"\"\"\n",
    "machine_h3 = u\"\"\"There are many different best seller lists that are published by various organizations, and the New York Times is just one of them. The New York Times best seller list is a weekly list that ranks the best-selling books in the United States based on sales data from a number of different retailers. The list is published in the New York Times newspaper and is widely considered to be one of the most influential best seller lists in the book industry.  It's important to note that the New York Times best seller list is not the only best seller list out there, and there are many other lists that rank the top-selling books in different categories or in different countries. So it's possible that a book could be a best seller on one list but not on another. Additionally, the term \"best seller\" is often used more broadly to refer to any book that is selling well, regardless of whether it is on a specific best seller list or not. So it's possible that you may hear about a book being a \"best seller\" even if it is not specifically ranked as a number one best seller on the New York Times list or any other list.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = DetectorDetectGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector.detect(text_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(0,1000)):\n",
    "#     detector.detect(text_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "\n",
    "c = make_pipeline(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# aa = detector.predict_proba_([machine[i]])[0]\n",
    "# while aa <= 0:\n",
    "#     print(machine[i])\n",
    "#     print(aa)\n",
    "#     i+=1\n",
    "#     aa = detector.predict_proba_([machine[i]])[0]\n",
    "# print(machine[i])\n",
    "# print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.predict_proba([human_h3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=(\"machine\", \"human\"), bow=False, split_expression= r\"\\s\",mask_string=\"<mask>\")#detector.tokenizer.mask_token\") # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.base_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(emma, c.predict_proba, num_features=10, num_samples=100)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion experiment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def deletion_experiment(top_tokens_human):\n",
    "    t = emma\n",
    "    for word in top_tokens_human:\n",
    "        t = re.sub( r\"(\\W)(%s)(\\W)\" % word, \"\\\\1<mask>\\\\3\" , t, 0, re.MULTILINE)\n",
    "    p_human_del = c.predict_proba([t])[0,1]\n",
    "    p_human_original = c.predict_proba([emma])[0,1]\n",
    "\n",
    "    print(\"p human del %f\" % p_human_del)\n",
    "    print(\"p human og  %f\" % p_human_original)\n",
    "\n",
    "    assert p_human_del < p_human_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletion_experiment(top_tokens_human = [word for (word, p) in exp.as_list() if p > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ") # example from https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer, mask_token=detector.get_mask_token())#detector.tokenizer.mask_token)\n",
    "# explainer = shap.Explainer(detector.predict_proba_machine, masker=masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING MASK MODEL TO GPU...DONE (0.00s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying perturbations: 0it [00:00, ?it/s]\n",
      "Applying perturbations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 10 texts have no fills. Trying again [attempt 1].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 2].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 3].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 4].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 5].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 6].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 7].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 8].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 9].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 10].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 11].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 12].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 13].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 14].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 15].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 16].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 17].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 18].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 19].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 20].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 21].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 22].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 23].\n",
      "WARNING: 10 texts have no fills. Trying again [attempt 24].\n"
     ]
    }
   ],
   "source": [
    "# build an explainer using a token masker\n",
    "explainer = shap.Explainer(detector.predict_proba_machine, tokenizer=tokenizer, masker=masker)\n",
    "\n",
    "\n",
    "shap_values = explainer([machine_h3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.argpartition(shap_values.values[0,:], -105)\n",
    "# idx = idx[np.argsort(shap_values.values[0,:][idx])][0:105]\n",
    "# deletion_experiment(shap_values.data[0][idx]) # TODO only for performance test as this is wrong, shap does not rank this way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_text\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['machine', 'human',], use_unk_distribution=True, mask_string=detector.tokenizer.mask_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.predict_label(['the', text_gpt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(emma, detector.predict_label, \n",
    "                                    threshold=0.95, # i.e. confidence, i.e. probability decision will be the same\n",
    "                                    delta=0.1, # default 0.1, 0.05 was used in the paper\n",
    "                                    tau=0.35, # i.e. epsilon, increase to increase tolerance\n",
    "                                    batch_size=10, \n",
    "                                   # only applies to BERT, onepass=True, # default false\n",
    "                                    use_proba=True, \n",
    "                                    beam_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "76j54n7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(nyt, detector.predict_label, \n",
    "                                    threshold=0.95, # i.e. confidence, i.e. probability decision will be the same\n",
    "                                    delta=0.1, # default 0.1, 0.05 was used in the paper\n",
    "                                    tau=0.35, # i.e. epsilon, increase to increase tolerance\n",
    "                                    batch_size=10, \n",
    "                                   # only applies to BERT, onepass=True, # default false\n",
    "                                    use_proba=True, \n",
    "                                    beam_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(text_gpt2, detector.predict_label, \n",
    "                                    threshold=0.95, # i.e. confidence, i.e. probability decision will be the same\n",
    "                                    delta=0.1, # default 0.1, 0.05 was used in the paper\n",
    "                                    tau=0.35, # i.e. epsilon, increase to increase tolerance\n",
    "                                    batch_size=10, \n",
    "                                   # only applies to BERT, onepass=True, # default false\n",
    "                                    use_proba=True, \n",
    "                                    beam_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "764u5wserty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import spacy\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from anchor import anchor_text\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_polarity(path='C:/Users/loris/Downloads/rt-polaritydata/rt-polaritydata'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    f_names = ['rt-polarity.neg', 'rt-polarity.pos']\n",
    "    for (l, f) in enumerate(f_names):\n",
    "        for line in open(os.path.join(path, f), 'rb'):\n",
    "            try:\n",
    "                line.decode('utf8')\n",
    "            except:\n",
    "                continue\n",
    "            data.append(line.strip())\n",
    "            labels.append(l)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_polarity()\n",
    "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)\n",
    "train_vectors = vectorizer.transform(train)\n",
    "test_vectors = vectorizer.transform(test)\n",
    "val_vectors = vectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sklearn.linear_model.LogisticRegression()\n",
    "# c = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=10)\n",
    "c.fit(train_vectors, train_labels)\n",
    "preds = c.predict(val_vectors)\n",
    "print('Val accuracy', sklearn.metrics.accuracy_score(val_labels, preds))\n",
    "def predict_lr(texts):\n",
    "    return c.predict(vectorizer.transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "text = 'This is a good bad good book .'\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "exp = explainer.explain_instance(text, predict_lr, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lr([\"Test good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
