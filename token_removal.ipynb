{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHE = True\n",
    "RESULTS_PATH = \"./results/token_removal\" # path to cache results to so that plots can be adjusted without re-running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./dataset_test.pkl\")\n",
    "\n",
    "\n",
    "documents = test[\"answer\"]\n",
    "gold_labels = test[\"author\"] == \"human_answers\"\n",
    "\n",
    "from detector_radford import DetectorRadford\n",
    "from detector_detectgpt import DetectorDetectGPT\n",
    "from detector_guo import DetectorGuo\n",
    "detector_classes = [DetectorRadford, DetectorGuo, DetectorDetectGPT]\n",
    "\n",
    "from explainer_wrappers import LIME_Explainer, SHAP_Explainer, Random_Explainer, Anchor_Explainer\n",
    "explainer_classes = [Anchor_Explainer, LIME_Explainer, SHAP_Explainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Experiment(ABC):\n",
    "    @abstractmethod \n",
    "    def __init__(self, explainer, document, gt):\n",
    "        self.document = document\n",
    "        self.gt = gt\n",
    "        self.explainer = explainer\n",
    "        self.fi_scores_machine  = self.explainer.get_fi_scores(document, fill=True)[0]\n",
    "        self.fi_scores_human = self.explainer.get_fi_scores(document, fill=True)[1]\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def modified_document(self):\n",
    "        pass\n",
    "    # as in Arras et al. 2016: \"The target class is the true document class,[...]\"\n",
    "    def get_fi_scores_target(self):\n",
    "        if self.gt:\n",
    "            return self.fi_scores_human\n",
    "        else:\n",
    "            return self.fi_scores_machine\n",
    "    def remove_features(self, id_fi_tuples_list, mask=True):\n",
    "        tokenized_modified_document = self.explainer.tokenize(self.document)\n",
    "       \n",
    "        ### assert right words are removed\n",
    "        top_words = [word for word, fi in self.explainer.as_list(self.explainer.get_explanation_cached(self.document), label=self.gt) if fi > 0]\n",
    "        for a, b in zip(tokenized_modified_document, self.explainer.tokenize(self.document)):\n",
    "            if a == self.explainer.detector.get_pad_token():\n",
    "                assert b in top_words, \"Masking strategy faulty\"\n",
    "        ###\n",
    "       \n",
    "        ids_tokens_to_remove = []\n",
    "        # don't remove anything if there are no best/worst features (e.g. only positive or only negative fi scores)\n",
    "        if len(id_fi_tuples_list):\n",
    "            ids_tokens_to_remove, _ = zip(*id_fi_tuples_list) \n",
    "        # replace with pad token (all detectors support partial input)\n",
    "        if mask:\n",
    "            for t in ids_tokens_to_remove:\n",
    "                tokenized_modified_document[t] = self.explainer.detector.get_pad_token()\n",
    "        else:\n",
    "            tokenized_modified_document = [t for i, t in enumerate(tokenized_modified_document) if i not in ids_tokens_to_remove]\n",
    "        return self.explainer.untokenize(tokenized_modified_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment_Delete_n_Highest(Experiment):\n",
    "    def __init__(self, explainer, document, gt, include_zero_scores=False, n=10, mask=True):\n",
    "        super().__init__(explainer, document, gt)\n",
    "        self.n = n\n",
    "        self.include_zero_scores = include_zero_scores\n",
    "        self.mask = mask\n",
    "    @property\n",
    "    def modified_document(self):\n",
    "        if self.n == 0:\n",
    "            return self.document\n",
    "        highest_fi_scores_target = sorted(self.get_fi_scores_target(), key=lambda x: x[1], reverse=True)[0:self.n]\n",
    "        # remove 0 scores if !include_zero_scores\n",
    "        highest_fi_scores_target = [(idx, fi) for idx,fi in highest_fi_scores_target if (self.include_zero_scores or (fi != 0)) and (fi >= 0)]\n",
    "        if len(highest_fi_scores_target) == 0:\n",
    "            return None\n",
    "        return self.remove_features(highest_fi_scores_target, mask=self.mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction cache, useful as original document is the same in all experiments\n",
    "prediction_cache = {}\n",
    "def prediction_cached(detector, document):\n",
    "    id = (detector.__class__.__name__,document)\n",
    "    if id not in prediction_cache:\n",
    "        prediction_cache[id] = detector.predict_proba([document])[0]\n",
    "    return prediction_cache[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_row(document, gt,  explainer, experiment_class, detector, n, mask):\n",
    "    \"\"\"Runs the experiment at the specified n for one detector and explanation method. Used to create a pandas df\n",
    "\n",
    "    Args:\n",
    "        document: The original document\n",
    "        gt: Ground truth of the original document\n",
    "        explainer: Instance of the explanation method to use\n",
    "        experiment_class: A class that extends Experiment\n",
    "        detector: Instance of the detector to use\n",
    "        n: How many tokens to remove\n",
    "        mask: If False, tokens are deleted instead of masked with the tokenizer's mask/pad token \n",
    "\n",
    "    Returns:\n",
    "        A row of the df\n",
    "    \"\"\"\n",
    "    experiment = experiment_class(explainer, document, gt, n=n, mask=mask)\n",
    "    if experiment.modified_document is None: # modified_document is None when len(fi_scores) < n. skip to speed up calculation\n",
    "        return [    explainer.__class__.__name__, \n",
    "                    explainer.detector.__class__.__name__, \n",
    "                    n,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None]\n",
    "    p_machine_original, p_human_original  = prediction_cached(detector, experiment.document)\n",
    "    y_original = p_human_original >= p_machine_original\n",
    "\n",
    "    p_machine_modified = None\n",
    "    p_human_modified = None\n",
    "\n",
    "    p_machine_modified, p_human_modified = prediction_cached(detector, experiment.modified_document)\n",
    "    y_modified = p_human_modified >= p_machine_modified\n",
    "    return [\n",
    "                    explainer.__class__.__name__, \n",
    "                    explainer.detector.__class__.__name__, \n",
    "                    n,\n",
    "                    p_machine_original,\n",
    "                    p_human_original,\n",
    "                    y_original,\n",
    "                    p_machine_modified,\n",
    "                    p_human_modified,\n",
    "                    y_modified,\n",
    "                    gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(experiment_class, detector, n=10, mask=True):\n",
    "    \"\"\"Collects results for all explanation methods as a pandas df\n",
    "\n",
    "    Args:\n",
    "        experiment_class: A class that extends Experiment\n",
    "        detector:  Instance of the detector to use\n",
    "        n: How many tokens to remove. Defaults to 10.\n",
    "        mask: If False, tokens are deleted instead of masked with the tokenizer's mask/pad token. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A pandas df with results built with get_results_row\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    columns=[\"Explainer\", \"Detector\", \"n\", \"p_machine_original\", \"p_human_original\", \"y_original\", \"p_machine_modified\", \"p_human_modified\", \"y_modified\", \"gt\"]\n",
    "    \n",
    "    for explainer_class in explainer_classes:\n",
    "        explainer = explainer_class(detector)\n",
    "        if isinstance(explainer, LIME_Explainer) and n > explainer.num_features:\n",
    "            print(\"skip lime\",n, explainer.num_features)\n",
    "            continue\n",
    "\n",
    "        for document, gt in zip(documents, gold_labels):\n",
    "            results.append(get_results_row(document, gt, explainer, experiment_class, detector, n, mask))\n",
    "\n",
    "    # random baseline (with new seeds at each run)\n",
    "    for i in range(0,5):\n",
    "        random_explainer = Random_Explainer(detector, seed=42-i)\n",
    "        for document, gt in zip(documents, gold_labels):\n",
    "            results_row = get_results_row(document, gt, random_explainer, experiment_class, detector, n, mask)\n",
    "            results_row[0] = \"Random Run \"+str(i)\n",
    "            results.append(results_row)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    df[\"p_target_original\"] = df.apply(lambda x: x[\"p_human_original\"] if x[\"gt\"] else x[\"p_machine_original\"], axis=1)\n",
    "    df[\"p_target_modified\"] = df.apply(lambda x: x[\"p_human_modified\"] if x[\"gt\"] else x[\"p_machine_modified\"], axis=1)    \n",
    "    df[\"drop_target\"] = df[\"p_target_original\"] - df[\"p_target_modified\"] \n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_by_initial_prediction_right_wrong(df, n, mask):\n",
    "    \"\"\"Calculates accuracy scores for the two cases \"initially wrong\" and \"initially right\"\n",
    "\n",
    "    Args:\n",
    "        df: Output from run_experiment\n",
    "        n: How many tokens to remove\n",
    "        mask: If False, tokens are deleted instead of masked with the tokenizer's mask/pad token \n",
    "\n",
    "    Returns:\n",
    "        A df with two accurary_scores\n",
    "    \"\"\"\n",
    "    accs = []\n",
    "    for explainer_name, group in df.groupby(\"Explainer\"):\n",
    "        group_initial_right = group[group[\"y_original\"] == group[\"gt\"]]\n",
    "        group_initial_wrong = group[group[\"y_original\"] != group[\"gt\"]]\n",
    "\n",
    "        acc_initial_right = accuracy_score(group_initial_right[\"gt\"].astype(int), group_initial_right[\"y_modified\"].astype(int)) if len(group_initial_right) > 0 else None\n",
    "        acc_inital_wrong = accuracy_score(group_initial_wrong[\"gt\"].astype(int), group_initial_wrong[\"y_modified\"].astype(int)) if len(group_initial_wrong) > 0 else None \n",
    "        accs.append((explainer_name,acc_initial_right, acc_inital_wrong,n, mask))\n",
    "    return pd.DataFrame(accs, columns=[\"Explainer\", \"acc_initial_right\", \"acc_initial_wrong\", \"n\", \"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_deleted_words = list(range(0,10+1)) + [20,30,40,50]\n",
    "\n",
    "\n",
    "for detector_class in detector_classes:\n",
    "    \n",
    "    detector = detector_class()\n",
    "    path = os.path.join(RESULTS_PATH, detector.__class__.__name__+\".csv\")\n",
    "    if USE_CACHE and os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    dfs_at_n_mask = [(run_experiment(Experiment_Delete_n_Highest,detector, n=n, mask=True), n, True) for n in tqdm(n_deleted_words, desc=\"Running mask experiment for \"+detector.__class__.__name__)]\n",
    "    dfs_at_n_delete = [(run_experiment(Experiment_Delete_n_Highest,detector, mask=False, n=n), n, False) for n in tqdm(n_deleted_words, desc=\"Running delete experiment for \"+detector.__class__.__name__)]\n",
    "    dfs_at_n = dfs_at_n_mask + dfs_at_n_delete\n",
    "    dfs_at_n = [(df.dropna(),n,mask) for df,n, mask in dfs_at_n]\n",
    "    df_accuracy_scores = pd.concat([get_acc_by_initial_prediction_right_wrong(df,n, mask) for df, n, mask in dfs_at_n])\n",
    "    \n",
    "    df_accuracy_scores.loc[df_accuracy_scores[\"Explainer\"].str.startswith(\"Random\"), \"Explainer\"] = \"Random\"\n",
    "    # df_accuracy_scores.groupby([\"Explainer\", \"n\"]).mean().reset_index()\n",
    "\n",
    "    df_accuracy_scores[\"Explainer\"] = df_accuracy_scores[\"Explainer\"].str.replace(\"_Explainer\",\"\")\n",
    "    df_accuracy_scores[\"Detector\"] = detector.__class__.__name__\n",
    "    \n",
    "    df_accuracy_scores.to_csv(path, encoding=\"UTF-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "Like in Arras et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(detector_name, df_accuracy_scores, mask):\n",
    "    df_accuracy_scores = df_accuracy_scores[(df_accuracy_scores[\"n\"] <= 10) | (df_accuracy_scores[\"Explainer\"] == \"SHAP\") | (df_accuracy_scores[\"Explainer\"] == \"Random\") ]\n",
    "    f, (ax_right, ax_wrong) = plt.subplots(1,2, sharey=True, figsize=(18,4))\n",
    "    ax_right.axhline(y=0.5,linestyle='--', lw=0.5, color=\"red\")\n",
    "    ax_wrong.axhline(y=0.5,linestyle='--', lw=0.5, color=\"red\")\n",
    "\n",
    "    sns.lineplot(data=df_accuracy_scores[df_accuracy_scores[\"mask\"]== mask], x=\"n\", y=\"acc_initial_right\", hue=\"Explainer\", ax=ax_right)\n",
    "    sns.lineplot(data=df_accuracy_scores[df_accuracy_scores[\"mask\"]== mask], x=\"n\", y=\"acc_initial_wrong\", hue=\"Explainer\", ax=ax_wrong)\n",
    "    ax_wrong.set_xlim(0,30)\n",
    "    ax_right.set_xlim(0,30)\n",
    "    plt.suptitle((\"Mask\" if mask else \"Delete\") + \" \" + detector_name)\n",
    "    f.tight_layout()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(RESULTS_PATH):\n",
    "    detector_name = f.split(\".\")[0]\n",
    "    df_accuracy_scores = pd.read_csv(os.path.join(RESULTS_PATH, f))\n",
    "    plot_acc(detector_name, df_accuracy_scores, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(RESULTS_PATH):\n",
    "    detector_name = f.split(\".\")[0]\n",
    "    df_accuracy_scores = pd.read_csv(os.path.join(RESULTS_PATH, f))\n",
    "    plot_acc(detector_name, df_accuracy_scores, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy_scores = pd.concat([pd.read_csv(os.path.join(RESULTS_PATH, f)) for f in os.listdir(RESULTS_PATH)]).reset_index(drop=True)\n",
    "df_accuracy_scores = df_accuracy_scores[(df_accuracy_scores[\"n\"] <= 10) | (df_accuracy_scores[\"Explainer\"] == \"SHAP\") | (df_accuracy_scores[\"Explainer\"] == \"Random\") ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.backends.backend_pgf import FigureCanvasPgf\n",
    "matplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'sans-serif',\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in [False, True]:\n",
    "    f, (ax_right, ax_wrong) = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "    sns.lineplot(data=df_accuracy_scores[df_accuracy_scores[\"mask\"]== mask], x=\"n\", y=\"acc_initial_right\", hue=\"Explainer\", ax=ax_right, markers=True, marker='o', errorbar=\"sd\")\n",
    "    sns.lineplot(data=df_accuracy_scores[df_accuracy_scores[\"mask\"]== mask], x=\"n\", y=\"acc_initial_wrong\", hue=\"Explainer\", ax=ax_wrong, markers=True, marker='o',errorbar=\"sd\")\n",
    "    ax_right.axhline(y=0.5,linestyle='--', lw=0.5, color=\"black\")\n",
    "    ax_wrong.axhline(y=0.5,linestyle='--', lw=0.5, color=\"black\")\n",
    "    ax_right.set_ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/token_removal_mask.pgf' if mask else './figures/token_removal_delete.pgf')\n",
    "    plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
